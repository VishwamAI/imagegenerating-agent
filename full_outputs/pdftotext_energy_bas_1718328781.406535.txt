Syntax Warning: Mismatch between font type and embedded font file
arXiv:2403.10961v1 [cs.LG] 16 Mar 2024

Foundations and Trends® in Signal Processing

Energy-Based Models with
Applications to Speech and
Language Processing
Suggested Citation: Zhijian Ou (2022), “Energy-Based Models with Applications to
Speech and Language Processing”, Foundations and Trends® in Signal Processing: Vol.
xx, No. xx, pp 1–200. DOI: 10.1561/XXXXXXXXX.

Zhijian Ou
Tsinghua University, Beijing, China
ozj@tsinghua.edu.cn

This article may be used only for the purpose of research, teaching,
and/or private study. Commercial use or systematic downloading (by
robots or other automatic processes) is prohibited without explicit
Publisher approval.

Boston — Delft


Contents

1 Introduction
1.1 The probabilistic approach . . . . . . . . . . . . . . . . .
1.1.1 Generative models and discriminative models . . .
1.1.2 Conditional models . . . . . . . . . . . . . . . . .
1.2 Features of EBMs . . . . . . . . . . . . . . . . . . . . . .
1.3 Organization of this monograph . . . . . . . . . . . . . .

3
3
6
7
8
10

2 Basics for EBMs
2.1 Probabilistic graphical models (PGMs) . . . . . . . . . . .
2.1.1 Directed graphical models . . . . . . . . . . . . . .
2.1.2 Undirected graphical models . . . . . . . . . . . .
2.2 EBM model examples . . . . . . . . . . . . . . . . . . . .
2.2.1 Ising model in statistical physics . . . . . . . . . .
2.2.2 Restricted Boltzmann Machines (RBMs) . . . . . .
2.2.3 EBMs parameterized by neural networks . . . . . .
2.3 Learning EBMs by maximum likelihood . . . . . . . . . . .
2.3.1 Markov Chain Monte Carlo (MCMC) . . . . . . . .
2.3.2 Importance sampling . . . . . . . . . . . . . . . .
2.3.3 Stochastic approximation methods . . . . . . . . .
2.3.4 Variational methods with auxiliary models . . . . .
2.3.5 Non-MLE methods for learning EBMs . . . . . . .

13
13
15
19
24
24
26
29
32
35
42
43
49
60


2.4
2.5

Learning EBMs by noise-contrastive estimation (NCE) . .
2.4.1 Dynamic noise-contrastive estimation (DNCE) . . .
Generation from EBMs . . . . . . . . . . . . . . . . . . .

61
64
66

3 EBMs for sequential data with applications in language modeling
71
3.1 Autoregressive language model (ALM) . . . . . . . . . . . 71
3.2 Energy-based language model (ELM) . . . . . . . . . . . . 73
3.2.1 Globally-normalized ELM (GN-ELM) . . . . . . . . 73
3.2.2 Trans-dimensional random field (TRF) LMs . . . . 74
3.2.3 Comparison between GN-ELM and TRF-LM . . . . 77
3.3 ELMs for speech recognition . . . . . . . . . . . . . . . . 78
3.3.1 Architectures of energy functions . . . . . . . . . . 78
3.3.2 Integrating discrete and neural features . . . . . . 86
3.3.3 Residual ELMs . . . . . . . . . . . . . . . . . . . . 88
3.3.4 Training methods . . . . . . . . . . . . . . . . . . 89
3.4 Energy-based cloze models for representation learning over
text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4 Conditional EBMs with applications
94
4.1 CRFs as conditional EBMs . . . . . . . . . . . . . . . . . 94
4.1.1 Linear-chain CRFs . . . . . . . . . . . . . . . . . . 95
4.1.2 Label bias and exposure bias . . . . . . . . . . . . 97
4.1.3 Training of CRFs . . . . . . . . . . . . . . . . . . 104
4.2 CRFs for speech recognition . . . . . . . . . . . . . . . . . 108
4.2.1 Connectionist Temporal Classification (CTC) . . . 108
4.2.2 CRF-based acoustic modeling with CTC topology . 115
4.3 CRFs for sequence labeling in NLP . . . . . . . . . . . . . 123
4.3.1 RNN-Transducer (RNN-T) . . . . . . . . . . . . . 124
4.3.2 From RNN-T to CRF transducer . . . . . . . . . . 125
4.4 EBMs for conditional text generation . . . . . . . . . . . . 130
4.4.1 Residual energy-based models . . . . . . . . . . . . 130
4.4.2 Controlled text generation from pre-trained language
models . . . . . . . . . . . . . . . . . . . . . . . . 135


5 Joint EBMs with applications
142
5.1 Basics for semi-supervised learning . . . . . . . . . . . . . 142
5.1.1 Discriminative SSL . . . . . . . . . . . . . . . . . 143
5.1.2 Generative SSL . . . . . . . . . . . . . . . . . . . 144
5.2 Upgrading EBMs to Joint EBMs (JEMs) for fixed-dimensional
data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
5.3 Upgrading CRFs to Joint random fields (JRFs) for sequential
data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
5.4 JEMs and JRFs for semi-supervised learning . . . . . . . . 156
5.4.1 Pre-training via EBMs for SSL . . . . . . . . . . . 157
5.4.2 Joint-training via EBMs for SSL . . . . . . . . . . 158
5.4.3 Comparison of joint-training and pre-training . . . 160
5.5 JRFs for calibrated natural language understanding . . . . 164
6 Conclusion
169
6.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
6.2 Future challenges and directions . . . . . . . . . . . . . . 170
Acknowledgements

173

Appendices

174

A Notations and definitions
175
A.1 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . 175
A.2 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . 176
B Background material
178
B.1 Maximum entropy models . . . . . . . . . . . . . . . . . . 178
B.2 Fisher equality . . . . . . . . . . . . . . . . . . . . . . . . 179
C Open-source toolkits related to EBMs

181

References

182

Index

200


List of Algorithms

1
2
3
4
5
6
7
8
9
10

Metropolis-Hastings Algorithm . . . . . . . . . . . . . . 36
Gibbs sampler . . . . . . . . . . . . . . . . . . . . . . . . 38
A naive algorithm of learning EBMs by Monte Carlo
methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
The general stochastic approximation (SA) algorithm . . 45
SA with multiple moves . . . . . . . . . . . . . . . . . . 47
Stochastic maximum likelihood for fitting an EBM . . . 48
The inclusive-NRF algorithm for learning EBMs for continuous data with latent-variable auxiliary models . . . 56
Sampling in the augmented space defined by pθ (x)qϕ (h|x) 59
NCE for fitting an unnormalized model . . . . . . . . . 63
Top-k sampling for the residual EBM . . . . . . . . . . . 133

1


List of Figures

1.1 The probabilistic approach . . . . . . . . . . . . . . . . .
1.2 Outline of this monograph . . . . . . . . . . . . . . . . .
2.1 (a) A simple directed graphical model with four variables (x1 , x2 , x3 , x4 ). (b) A simple undirected graphical
model with four variables (x1 , x2 , x3 , x4 ). For both types
of graphs, V denotes the set of nodes and E the set of
edges. If both ordered pairs (α, β) and (β, α) belong to E,
we say that we have an undirected edge between α and
β. A nice introduction of graph theory in the context of
graphical models could be found in Chapter 4 of Cowell
et al., 1999. . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Graphical model representation of a hidden Markov
model (HMM). . . . . . . . . . . . . . . . . . . . . . . .
2.3 Neural network based classifier. (a) GM representation;
(b) Computational graph representation. . . . . . . . . .
2.4 Illustration of the global Markov property in UGMs. . .
2.5 Ising model: (a) The undirected graph representation,
(b) A sample. . . . . . . . . . . . . . . . . . . . . . . . .
2.6 Sample states of square Ising models with J = 1, H =
0, kB = 1, N = 4096 at a sequence of temperatures
T = 5, 2.5, 2.4, 2.3, 2. (MacKay, 2003) . . . . . . . . . . .
2

4
11

14
17
18
21
24

26


LIST OF FIGURES
2.7

3

Restricted Boltzmann Machine. The top layer represents
a vector of stochastic binary hidden variables h and the
bottom layer represents a vector of stochastic binary
visible variables v. (Salakhutdinov, 2009) . . . . . . . . .

27

2.8 (a) Restricted Boltzmann machine (RBM), (b) Sigmoid
belief network (SBN). . . . . . . . . . . . . . . . . . . .

28

2.9

Potential functions in EBMs can be flexibly parameterized by neural networks for images, natural languages
and so on. . . . . . . . . . . . . . . . . . . . . . . . . .

30

2.10 Overview of the inclusive-variational approach for learning EBMs for continuous data. Two neural networks are
used to define the EBM’s potential function Uθ (x) and
the auxiliary generator gϕ (h) respectively. The parameters of both networks, θ and ϕ, are updated by using
the revised samples (x, h) in the augmented space, which
are obtained by revising the samples (x′ , h′ ) proposed by
the auxiliary generator, according to the stochastic gradients defined by both the target EBM and the auxiliary
generator. (Song and Ou, 2018) . . . . . . . . . . . . . .

54

3.1

Example of discrete features. . . . . . . . . . . . . . . .

80

3.2

Hidden2Scalar: a deep CNN architecture used to define
the potential function Uθ (x). Shadow areas denote the
padded zeros. (Wang and Ou, 2017) . . . . . . . . . . .

82

Hidden2Scalar: a bidirectional LSTM on top of CNN
used to define the potential function Uθ (x). (Wang and
Ou, 2018b) . . . . . . . . . . . . . . . . . . . . . . . . .

83

SumInnerProduct: a bidirectional LSTM used to define
the potential function Uθ (x). (Wang and Ou, 2018a) . .

83

3.5 The WER curves of the three TRF-LMs during the first
100 training epochs are plotted. (Gao et al., 2020) . . .

87

3.3

3.4


4

LIST OF FIGURES
3.6

4.1
4.2

Comparison of BERT and Electric. Both model the conditional probability of a token given its surrounding context.
BERT produces normalized conditional distribution for
masked positions, while Electric calculates unnormalized
conditional probabilities for all input tokens. (Clark et al.,
2020b) . . . . . . . . . . . . . . . . . . . . . . . . . . . .

90

Graphical model representation of a conditional random
field (CRF). . . . . . . . . . . . . . . . . . . . . . . . . .

95

State transitions resulting from estimating an autoregressive language model from training data - “Tom likes tea”,
“John likes tea”, and “Alice like tea”. For some transitions
not appeared in the training data, the transition probabilities are smoothed to take small values ϵ. We pad the
beginning and the end of a sentence with special tokens,
⟨s⟩ and ⟨/s⟩, respectively (Chen and Goodman, 1999). .

99

4.3

Estimating a globally-normalized energy-based language
model (ELM) from training data - “Tom likes tea”, “John
likes tea”, and “Alice like tea”. The bi-gram features used
by the ELM are similar to those used in the bigram ALM,
and so can also be illustrated by a graph. The estimated
parameters are shown over the edges, which represent
the corresponding bi-gram features. . . . . . . . . . . . . 100

4.4

Illustration of exposure bias. y: real, ŷ: predicted. . . . . 103

4.5

Different units of labels can be used in speech recognition.109

4.6

State transitions in HMMs for speech recognition are
constrained by a number of knowledge sources. . . . . . 110

4.7

Overview of CTC architecture. . . . . . . . . . . . . . . 112

4.8

Illustration of the lattice, which contains all the possible
alignments between the acoustic sequence and the label
sequence ‘CAT’. Also illustration of the forward-backward
algorithm. Black circles represent ordinary labels, and
white circles represent blanks. Arrows signify allowed
transitions. (Graves et al., 2006) . . . . . . . . . . . . . 113


LIST OF FIGURES

5

4.9

Graphical model representation of the CTC model (a)
and the CTC-CTF model (b). Note that the edge potential does not involve exactly n consecutive nodes for a
n-gram LM of labels, as detailed in the text of Section
4.2.2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
4.10 Graphical model representations of different ASR models:
(a) HMM, defined in Eq. (2.2), (b) CTC, defined in Eq.
(4.17), (c) RNN-T, (d) AED, (e) CTC-CRF, defined in
Eq. (4.21). . . . . . . . . . . . . . . . . . . . . . . . . . . 121
4.11 Graphical model representations of (a) a linear-chain
CRF, (b) a RNN-T for the aligned setting, and (c) a
CRF transducer. Notably, the graphical representation
of the RNN-T for the aligned setting, as defined in Eq.
(4.28), is different from that of the usual RNN-T as shown
in Figure 4.10(c). . . . . . . . . . . . . . . . . . . . . . . 126
4.12 The architecture of a CRF transducer. . . . . . . . . . . 127
4.13 Overview of mix-and-match LM. The Lego pieces show
different experts that can be used to form the energy
LM and help control different features in the generated
text. The right side shows the i-th step in the the Gibbs
sampling chain, where a proposal is made by the MLM,
and then it is accepted/rejected based on the energy
score. (Mireshghallah et al., 2022) . . . . . . . . . . . . 137
5.1 An overview of SSL and a general categorization of generative SSL methods. Examples are mainly chosen from
NLP. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
5.2 Overview of the JRF model. The node and edge potentials define a JRF (a joint distribution over xl and
y l ). Inducing the conditional and the marginal from the
joint yields a CRF and a TRF respectively. A JRF can
be trained from labeled data (acting like a CRF) and
also from unlabeled data (acting like a TRF). In practice, the node potentials are calculated from the logits
oi , i = 1, · · · , l, from the NN, and the edge potential
follows a linear-chain definition. . . . . . . . . . . . . . 153


6

LIST OF FIGURES
5.3

Illustration of EBM based semi-supervised image classification. (a) Pre-training, (b) Fine-tuning, (c) Joint-training.157
5.4 Illustration of EBM based sequence labeling. (a) Pretraining, (b) Fine-tuning, (c) Joint-training. . . . . . . . 159
5.5 Comparison of the scalar and the hidden variants of
energy functions. The modules introduced for EBM are
shaded in green. (He et al., 2021) . . . . . . . . . . . . . 165
5.6 The entropy of the posterior (pθ (·|x)) versus energy value
Êθ (x) for SST-2 test-set samples. (He et al., 2021) . . . 168


List of Tables

2.1 A survey of different sampling methods used in generating text from EBMs. The target model is the EBM,
while a proposal is required for both MCMC and IS. Different proposals are used in different applications. Shorthands: ALMs (autoregressive language model), MLM
(masked language model), SNIS (self-normalized importance sampling), ASR (automatic speech recognition),
CTG (controlled text generation), CTGAP (conditional
text generation after prefix). . . . . . . . . . . . . . . . .

68

3.1 The development of TRF-LMs. . . . . . . . . . . . . . .

76

3.2

Feature definition in TRF LMs (Wang et al., 2018) . . .

81

4.1 A general classification of sequence models, with some
common examples. . . . . . . . . . . . . . . . . . . . . .

98

7


8

LIST OF TABLES
4.2

4.3

Comparison of different models for ASR. HMM topology denotes that labels (including silence) are modeled
by multiple states with left-to-right transitions, possible
self-loops and skips. CTC topology denotes the special
state transitions used in CTC (including blank). Locally/globally normalized denotes the formulation of the
model distribution. In defining the joint distribution of a
model, locally normalized models use conditional probability functions, while globally normalized models use unnormalized potential functions. SS-LF-MMI is classified
as globally normalized, though it is cast as MMI-based
discriminative training of a pseudo HMM and the HMM
model is locally normalized. AED does not use states to
align label sequence y and observation sequence x. . . . 120
Model comparison and connection. . . . . . . . . . . . . 125

5.1 Applications of EBMs across different domains: comparison and connection (See text for details). . . . . . . . . 156
5.2 SSL for image classification over CIFAR-10 with 4,000
labels for a full training set of 50K images. The upper/lower blocks show the generative/discriminative SSL
methods respectively. The means and standard deviations
are calculated over ten independent runs with randomly
sampled labels. . . . . . . . . . . . . . . . . . . . . . . . 161
5.3 Relative improvements by joint-training EBMs compared
to the supervised baseline (abbreviated as sup.) and
the pre-training+fine-tuning EBMs (abbreviated as pre.)
respectively. The evaluation metric is accuracy for POS
and F1 for chunking and NER. “Labeled” denotes the
amount of labels in terms of the proportions w.r.t. the
full set of labels. “U/L” denotes the ratio between the
amount of unlabeled and labeled data. . . . . . . . . . . 163


Energy-Based Models with
Applications to Speech and
Language Processing
Zhijian Ou1
1 Tsinghua University, Beijing, China; ozj@tsinghua.edu.cn

ABSTRACT
Energy-Based Models (EBMs) are an important class of
probabilistic models, also known as random fields and undirected graphical models. EBMs are un-normalized and thus
radically different from other popular self-normalized probabilistic models such as hidden Markov models (HMMs),
autoregressive models, generative adversarial nets (GANs)
and variational auto-encoders (VAEs). During these years,
EBMs have attracted increasing interests not only from core
machine learning but also from application domains such
as speech, vision, natural language processing (NLP) and
so on, with significant theoretical and algorithmic progress.
To the best of our knowledge, there are no review papers
about EBMs with applications to speech and language processing. The sequential nature of speech and language also
presents special challenges and needs treatment different
from processing fix-dimensional data (e.g., images).
The purpose of this monograph is to present a systematic
introduction to energy-based models, including both algorithmic progress and applications in speech and language
processing, which is organized into four chapters. First, we
Zhijian Ou (2022), “Energy-Based Models with Applications to Speech and Language
Processing”, Foundations and Trends® in Signal Processing: Vol. xx, No. xx, pp
1–200. DOI: 10.1561/XXXXXXXXX.
©2024 Zhijian Ou


2

LIST OF TABLES
will introduce basics for EBMs, including classic models,
recent models parameterized by neural networks, sampling
methods, and various learning methods from the classic
learning algorithms to the most advanced ones. The next
three chapters will present how to apply EBMs in three
different scenarios, i.e., for modeling marginal, conditional
and joint distributions, respectively. 1) EBMs for sequential data with applications in language modeling, where we
are mainly concerned with the marginal distribution of a
sequence itself; 2) EBMs for modeling conditional distributions of target sequences given observation sequences, with
applications in speech recognition, sequence labeling and
text generation; 3) EBMs for modeling joint distributions of
both sequences of observations and targets, and their applications in semi-supervised learning and calibrated natural
language understanding. In addition, we will introduce some
open-source toolkits to help the readers to get familiar with
the techniques for developing and applying energy-based
models.


1
Introduction

1.1

The probabilistic approach

As a community we seem to have embraced the fact that dealing with
uncertainty is crucial for machine intelligence tasks such as speech recognition and understanding, speech synthesis, natural language labeling,
machine translation, text generation, computer vision, signal denoising,
decision making, and so on. Uncertainty arises because of limitations
in our ability to observe the world, limitations in our ability to model
it, and possibly even because of innate nondeterminism (Koller and
Friedman, 2009). In face of such uncertainty, we use probabilistic models
to describe the random phenomena. Indeed, many tasks in intelligent
signal processing and machine learning are solved in the probabilistic
approach, which generally involves probabilistic modeling, inference and
learning, as shown in Figure 1.1. Such probabilistic approach has been
introduced in textbooks with sufficient details (Koller and Friedman,
2009; Murphy, 2012; Bishop, 2006; Hastie et al., 2009), and thus in this
paper we only give a brief overview as the background material.
A probabilistic model is, in mathematical terms, a distribution over a
set of random variables, which are assumed to characterise the random
phenomena in the specific task. The set of variables can generally
3


4

Introduction

Figure 1.1: The probabilistic approach

be divided into observations x and (optionally) hidden variables h,
according to their roles in the task. Hidden variables, or called latent
variables, are variables that are part of the model, but which we do
not observe, and are therefore not part of the data. Remarkably, the
observability of some variables may change, depending on what phase
(training or testing) the model is used. A most common example is the
target variable in prediction tasks, such as the class label in classification
or the response variable in regression, which is observed in training
but becomes unknown in testing. To avoid clutter in this paper, such
variable is viewed as part of the hidden variables and usually denoted
by y.
We will typically denote a variable by a lower case letter such as
x, h and y. Whether x denotes the value that the variable takes or
represents the variable itself would be clear from the context. Further,
for notational simplicity, we also use lower case letter (e.g., x) to denote
a set of random variables, i.e., flattened and concatenated such that the
set is represented as a single vector. So if x is a vector or a sequence,
its components can be accessed by subscripts xi . Here, we are using
the terminology distribution or density loosely, typically denoted by
p. Our notation p should be understood as a mass function (density
with respect to counting measure) in the discrete case, and a density
function with respect to Lebesgue measure in the continuous case. See
Section A.1 for more on notations.
Given the form of the probabilistic model, namely the distribution


1.1. THE PROBABILISTIC APPROACH

5

pθ (x, h) with parameters θ, there are two crucial problems that must
be solved in applying the model in real-world tasks:
• Inference: how to reason in the presence of uncertainty;
• Learning: how to learn from experience.
The former problem is often referred to probabilistic inference with
a fully-specified model, or inference for short; and the later problem
sometimes referred to statistical inference (or more often to say, learning
in machine learning terminology) for model parameters (Neal, 1993).
Put in a more straightforward way, learning is to find the most
appropriate model with parameters, using both data and human knowledge. Human knowledge is implicitly employed to specify the family of
parametric distributions, and data are used to estimate the parameters.
Given a fully-specified model, i.e., fully-determined with fixed parameters, inference is to infer the unknown from the observation x. There
are several typical classes of inference problems:
• Computing conditional probabilities, e.g., pθ (h|x). This amounts
to computing the posterior probability of some variables given the
values of other variables (i.e., given evidence on others).
• Computing marginal probabilities, including the likelihood pθ (x).
• Computing modes, e.g., arg maxh pθ (h|x).
• Sampling from the model (Neal, 1993; Liu, 2001).
We provide two more points for readers to appreciate the importance of the inference problems. First, the inference problems themselves
are often taken as the means to use the model. For example, speech
recognition is generally to find the mode of the posterior distribution
on state sequences given observed speech. Second, learning algorithms
often make use of some inference problem as a subroutine. For example, algorithms that maximize likelihood for learning latent variable
models, e.g., the expectation-maximization (EM) algorithm (Dempster
et al., 1977), call the calculation of pθ (h|x) as a subroutine. Seeking
computational efficient algorithms to solve these inference problems for


6

Introduction

increasingly complex models has been an enduring challenge for our
research community.
1.1.1

Generative models and discriminative models

One major division in the probabilistic approach is generative versus
discriminative modeling. In generative modeling, one aims to learn
the joint distribution pθ (x, h) over all the variables. In discriminative
modeling, one only models the conditional distribution pθ (h|x) over the
target variable (denoted by h for convenience) given the observation x.
In discriminative modeling, the observation and the target variable are
also called the input and output, respectively.
The generative-discriminative distinction has received much attention in machine learning (Ng and Jordan, 2001; Liang and Jordan, 2008).
When a discriminative model follows the induced form of the conditional distribution pθ (h|x) from a generative model pθ (x, h), the two
models are called a generative-discriminative pair (i.e., under the same
parametric family of models) (Ng and Jordan, 2001). For example, naive
Bayes classifier and logistic regression, hidden Markov model (HMM)
(Rabiner, 1989) and conditional random field (CRF) (Lafferty et al.,
2001; Sutton, McCallum, et al., 2012), form Generative-Discriminative
pairs, respectively. To compare generative and discriminative learning,
it seems natural to focus on such pairs. Basically, there are different
regimes of performance as the training set size in increased. Taking naive
Bayes and logistic regression as a case study, it is shown in (Ng and
Jordan, 2001) that “while discriminative learning has lower asymptotic
error, a generative classifier may also approach its (higher) asymptotic
error much faster”. The comparison of HMM and CRF is further studied
in (Liang and Jordan, 2008), and it is found that generative modeling
(modeling more of the data) tends to reduce asymptotic variance, but
at the cost of being more sensitive to model misspecification. These
previous results, including (Ng and Jordan, 2001; Liang and Jordan,
2008), to name a few, strengthen our basic intuitions about generativediscriminative distinction.
Given that the generative and discriminative estimators are complementary, one natural question is how to interpolate between the two


1.1. THE PROBABILISTIC APPROACH

7

to get the benefits of both. There have studies for hybrid generativediscriminative methods (see Bouchard, 2007 and the references therein).
Notably, those hybrid models have been applied for semi-supervised
learning (SSL), where one may have few labeled examples and many
more unlabeled examples, but mostly based on traditional generative
models like naive Bayes.
In recent years, generative modeling techniques have been greatly
advanced by inventing new models with new learning algorithms under
the umbrella of deep generative models (DGMs), which are characterized by using multiple layers of stochastic or deterministic variables in
modeling and are much more expressive than classic generative models such as naive Bayes and HMM. See (Ou, 2018) for a systematic
introduction to DGMs from perspective of graphical modeling. The
generative-discriminative discussion continues with new points, when
more types of generative models have constantly emerged and become
studied. Here we provide two examples with the new points.
• A type of DGMs, variational autoencoders (VAEs) (Kingma,
Welling, et al., 2019), has been successfully applied in the setting
of semi-supervised learning.
• It is concurrently shown in (Song and Ou, 2018; Grathwohl et al.,
2020) that a standard discriminative classifier pθ (y|x) can be used
to directly define an energy-based model (EBM) for the joint distribution pθ (x, y). It is shown in (Song and Ou, 2018) that energybased semi-supervised training of the joint distribution produces
strong classification results on par with state-of-art DGM-based
semi-supervised methods. It is demonstrated in (Grathwohl et al.,
2020) that energy based training of the joint distribution improves
calibration, robustness, and out-of-distribution detection while
also generating samples rivaling the quality of recent generative
adversarial network (GAN) (Goodfellow et al., 2014) approaches.
1.1.2

Conditional models

Discriminative models are a kind of conditional models for discriminative tasks. However, conditional modeling is a more general modeling


8

Introduction

concept than discriminative modeling. Basically, a conditional model is,
in probability terms, a conditional distribution of a random variable of
interest, when another variable c is known to take a particular value.
In this case, c is often called the input of the model. The variable of
interest generally can still consist of observable and (optionally) hidden
components, denoted by x and h respectively. Thus, a conditional model
can generally be denoted by pθ (x, h|c).
Many real-world applications are solved by conditional modeling.
Some examples from discriminative tasks are as follows.
• First, by abuse of notation, discriminative modeling of image
classification involves the conditional model pθ (y|x), where x is
the input image and y is the images’s class.
• A more complicated example is the recurrent neural network
transducer (RNN-T) model (Graves, 2012) for speech recognition.
Let x denote the input speech, y the label sequence (e.g., word
transcription), and π the hidden state sequence (or say, a path)
which realizes the alignment of x and y. Then the RNN-T model
involves the conditional model pθ (y, π|x). See Section 4.3.1 for
more details on RNN-T.
Apart from discriminative tasks, conditional models can also be
used for conditional generation tasks. One example is the reverse of the
image classification problem: prediction of a distribution over images,
conditioned on the class label.
Importantly, one should keep in mind that the learning and inference
methods introduced in unconditional modeling are in theory equally
applicable to conditional models. So the basics introduced in Chapter
2 lay the foundation for both (unconditional) EBMs in Chapter 3 and
conditional EBMs in Chapter 4. On the other hand, the unconditional
and conditional settings have their own characteristics, and thus needs
different treatments, as we will detail in Chapter 3 and 4 respectively.
1.2

Features of EBMs

In the probabilistic approach, the family of models chosen in real-world
applications clearly plays a crucial role. In terms of graphical modeling


1.2. FEATURES OF EBMS

9

terminology (Koller and Friedman, 2009), probabilistic models can be
broadly classified into two classes - directed and undirected.
• In directed graphical models (DGMs), also known as (a.k.a.)Bayesian
networks (BNs) or called locally-normalized models, the distribution is factorized into a product of local conditional density
functions.
• In contrast, in undirected graphical models (UGMs), also known
as Markov random fields (MRFs) or energy-based models (EBMs)
or called globally-normalized models, the distribution is defined to
be proportional to the product of local potential functions. The
three terms, UGMs, MRFs and EBMs, are exchangeable in this
monograph.
Simply speaking, an easy way to tell an undirected model from a directed
model is that an undirected model is un-normalized and involves the
normalizing constant (also called the partition function in physics),
while the directed model is self-normalized.
In general, directed models and undirected models make different
assertions of conditional independence. Thus, there are families of probability distributions that are captured by a directed model and are
not captured by any undirected model, and vice versa (Pearl, 1988).
Therefore, undirected models, though less explored, provide an important complementary choice to directed models for various real-world
applications.
During these years, EBMs have attracted increasing interests not
only from core machine learning but also from application domains such
as speech, vision, natural language processing and so on, with significant
theoretical and algorithmic progress. There have emerged a dedicated
workshop at ICLR 2021, which is a broad forum about EBM researches,
and a tutorial at CVPR 2021, which focuses on computer vision tasks.
• ICLR2021 Workshop - Energy Based Models: Current Perspectives, Challenges, and Opportunities, https://sites.google.com/
view/ebm-workshop-iclr2021
• CVPR 2021 Tutorial: Theory and Application of Energy-Based
Generative Models, https://energy-based-models.github.io/


10

Introduction

To the best of our knowledge, there are no review papers about EBMs
with applications to speech and language processing. The sequential
nature of speech and language also presents special challenges and needs
treatment different from processing fix-dimensional images that was
described in the CVPR 2021 tutorial. The aim of this monograph is
to present a systematic introduction to energy-based models, including
both algorithmic progress and applications in speech and language
processing. We hope it will also be of general interests to the artificial
intelligence and signal processing communities.
Before delving into the specific content, we first point out five key
features of EBMs, which may motivate you to pursue the study and
application of EBMs.
• Flexibility in modeling. Compared to modeling a self-normalized
density function, learning EBMs relaxes the normalization constraint and thus allows much greater flexibility in the parameterization of the energy function. Moreover, undirected modeling is
more natural for certain domains, where fixing the directions of
edges is awkward in a graphical model.
• Computation efficiency in likelihood evaluation, since the negative
log likelihood of an EBM (by ignoring an additive constant)
can be easily evaluated, without incurring any calculation for
normalization.
• Naturally overcoming label bias and exposure bias suffered by
locally-normalized models (Section 4.1.2).
• Superiority for hybrid generative-discriminative and semi-supervised
learning (Section 5).
• Challenge in model training. Both computation of the exact likelihood and exact sampling from EBMs are generally intractable,
which makes training especially difficult.
1.3

Organization of this monograph

The rest of the monograph is organized as follows.


1.3. ORGANIZATION OF THIS MONOGRAPH

11

Figure 1.2: Outline of this monograph

In Chapter 2, we present basics for EBMs. We start with a brief
introduction to probabilistic graphical models (PGMs), because basically
we introduce EBMs as undirected graphical models (UGMs). Then, we
present EBM model examples, including both classic ones (such as
Ising model and restricted Boltzmann machines) and modern ones
parameterized by neural networks. Next, basic algorithms for learning
EBMs are described, which covers the two most widely used classes of
methods - Monte Carlo based maximum likelihood methods and noisecontrastive estimation (NCE) methods. Finally, we present a dedicated
section to introduce how to sample/generate from EBMs, since sampling
is not only a critical step in maximum likelihood learning of EBMs, but
also itself forms as an important class of applications in speech and
language processing.
The basics for inference and learning with EBMs are general for both
discrete and continuous data modeling. Remarkably, most applications
covered in this monograph is discrete data modeling (text in natural
language processing, discrete labels in speech recognition), but in some
places, we also present examples and applications in images. For example,
Ising model is introduced for readers to get the abstract concepts
conveyed by EBMs. EBM based joint-training for semi-supervised image
classification is a fixed-dimensional counterpart of the more complicated
sequence setting, which is for semi-supervised natural language labeling.
The next three chapters are devoted to introduce how to develop


12

Introduction

EBMs in three different scenarios respectively.
• Note that the sequential nature of speech and language presents
special challenges and needs treatment different from processing
fix-dimensional data (e.g., images). In Chapter 3, we introduce
EBMs for sequential data with applications in language modeling. In this scenario, we are mainly concerned with learning the
(marginal) distribution of an observation sequence x itself, e.g., a
natural language sentence as in language modeling.
• In Chapter 4, we introduce EBMs for modeling conditional distributions of target sequences given observation sequences. Conditional EBMs have been successfully applied in speech recognition,
sequence labeling in natural language processing (NLP), and various forms of conditional text generation (e.g., controlled text
generation, factual error correction).
• In Chapter 5, we introduce EBMs for modeling joint distributions
of both sequences of observations and targets. We first introduce
the fixed-dimensional case, then move on to the sequential case,
and finally present the applications in semi-supervised natural
language labeling and calibrated natural language understanding.
Finally, conclusions are given in Chapter 6 to summarize the monograph and to discuss future challenges and directions.
We visualize the content of this monograph in Figure 1.2. At the
center is the basic knowledge for EBM modeling and learning. The
basic theory can be applied to model different types of distributions –
the distribution of the observation itself, the conditional distribution,
and the joint distribution. In different applications or scenarios, we are
concerned with different types of distributions. In Chapter 3, 4, and
5, we in fact show how to develop EBMs for the three different types
of distributions in three different scenarios, respectively, as described
above.
This monograph contains the material expanded from the tutorial
that the author gave at ICASSP 2022 in May 2022. Substantial updates
have been made to incorporate more recent work and cover wider areas
of research activities.


2
Basics for EBMs

Basically we introduce EBMs as undirected graphical models. We begin
with background on probabilistic graphical models, which would be
beneficial for readers to intuitively appreciate the differences between
directed graphical models (DGMs) and undirected graphical models
(UGMs).
2.1

Probabilistic graphical models (PGMs)

Probabilistic graphical models provide a general framework for describing and applying probabilistic models in the probabilistic approach.
Many ideas developed in the probabilistic approach can be understood,
unified, and generalized within the formalism of graphical models.
“A graphical model is a family of probability distributions
defined in terms of a directed or undirected graph. The nodes
in the graph are identified with random variables, and joint
probability distributions are defined by taking products over
functions defined on connected subsets of nodes.” (Jordan,
2004)
Consider a graph G = (V, E) where V is a set of vertices (also
13


14

Basics for EBMs

Figure 2.1: (a) A simple directed graphical model with four variables (x1 , x2 , x3 , x4 ).
(b) A simple undirected graphical model with four variables (x1 , x2 , x3 , x4 ). For both
types of graphs, V denotes the set of nodes and E the set of edges. If both ordered
pairs (α, β) and (β, α) belong to E, we say that we have an undirected edge between
α and β. A nice introduction of graph theory in the context of graphical models
could be found in Chapter 4 of Cowell et al., 1999.

called nodes) and the set of edges E is a subset of the set V × V .
See Figure 2.1 for an illustration of these concepts from graph theory.
Let1 xV ≜ {xv : v ∈ V } be a collection of random variables indexed
by the nodes of the graph. A graphical model in terms of G describes
a family of probability distributions p(xV ) over the variables xV . A
variable can either be scalar- or vector-valued, where in the latter case
the vector variable implicitly corresponds to a sub-graphical model over
the elements of the vector.
The edges E specifies the connections between the nodes and, according to the graph semantics (see below), plays a crucial role in
defining the graphical model distribution. One view is that the edges E,
depending on the graph semantics, determines a particular factorized
form of the distribution. Another view is that the edges E, of course
still depending on the graph semantics, determines a particular set of
conditional independence (CI) assumptions over the random variables.
In both views, the properties, either the factorized form or the CI
properties, implied by the graphical model are true for all members of
1

As described in Section A.1, we allow sets of indices to appear wherever a single
index appears.


2.1. PROBABILISTIC GRAPHICAL MODELS (PGMS)

15

its associated distribution family. As we will see in Section 2.1.1 and
Section 2.1.2, the two views are, in a strong sense, equivalent.
Graphical models can be defined over different types of graphs,
directed, undirected or mixed, each with differing semantics. The semantics specifies what a graphical model means and tells how the family
of distributions is defined (Russell and Norvig, 2010; Koller and Friedman, 2009). The set of CI properties specified by a particular graphical
model, and therefore the family of probability distributions it represents,
will be different depending on the type of graphical model currently
being considered.
The two most common forms of graphical model (GM) are directed
graphical models (DGMs) and undirected graphical models (UGMs),
based on directed acylic graphs and undirected graphs, respectively. In
general, directed graphs and undirected graphs make different assertions
of conditional independence. Thus, there are families of probability distributions that are captured by a directed graph and are not captured by
any undirected graph, and vice versa (Pearl, 1988; Koller and Friedman,
2009).
2.1.1

Directed graphical models

Let us begin with the directed case. Continuing with the notations in
Section 2.1, let G = (V, E) be a directed acyclic graph (DAG), and xV
be a collection of random variables indexed by the nodes of G. For each
node v ∈ V , let pa(v) denote the subset of indices of its parents; thus
xpa(v) denotes the vector of random variables indexed by the parents of
v.
Definition 2.1 (DGM). A directed graphical model in terms of G consists
of a family of distributions that factorize in the following way:
p(xV ) =

Y

p(xv |xpa(v) )

(2.1)

v∈V

We then also say that p(xV ) has the directed factorization property
(DF) according to G, or simply, p(xV ) factorizes according to G.
Remarkably, the notation in Eq. (2.1) is self-consistent, because it
can be verified that the joint distribution p(xV ) defined by the factor-


16

Basics for EBMs

ization Eq. (2.1) has {p(xv |xpa(v) )} as its conditionals. For the simple
directed graphical model shown in Figure 2.1(a), the joint distribution
that it describes is:
p(x1 , x2 , x3 , x4 ) = p(x1 )p(x2 |x1 )p(x3 |x2 )p(x4 |x1 , x3 )
General speaking, the nodes in a graphical model correspond to
the random variables, and the edges indicate some direct probabilistic
interactions between the nodes. In directed graphical models, every edges
is directed and, intuitively, correspond to direct influence of the parent
node on the child node. Thus, DGMs are suitable for modeling clear
influence relationships between random variables, which are expressed
through conditional distributions.
Factorization and Markov properties in directed graphical models
The factorization in Eq. (2.1) implies a set of conditional independence
statements among the variables xV . In an opposite way, we could define
a set of conditional independence statements in terms of G, which is
often referred to as a Markov property over G. A range of Markov
properties could be defined, relative to G, such as the directed global
Markov property (DG), the directed local Markov property (DL), the
directed ordered Markov property (DO) (See Section 5.3 of Cowell et al.,
1999). It can be shown that the three Markov properties, (DG), (DL)
and (DO), are equivalent, and further, they are equivalent to the (DF)
property. These properties collectively characterize a graphical model
(i.e., a family of distributions defined in terms of a graph). The Markov
properties of a distribution are precisely what allow it to be expressed
compactly in a factorized form. Conversely, a particular factorization of
a distribution guarantees that certain independencies hold.
DGM example - HMM
Many classic probabilistic models in speech and language processing
can be easily understood in terms of graphical models. Figure 2.2 shows
the graphical model representation of an hidden Markov model (HMM)
(Rabiner, 1989), which has been widely used in speech recognition and
various natural language processing tasks.


2.1. PROBABILISTIC GRAPHICAL MODELS (PGMS)

17

Figure 2.2: Graphical model representation of a hidden Markov model (HMM).

In an HMM, there is an underlying hidden Markov chain, corresponding to the state sequence π1:T ≜ π1 · · · πT . At each time frame,
depending on the state πt , the model probabilistically emit an output
xt , which can be observed. The joint distribution is given by
p(π1:T , x1:T ) = p(π1 )

TY
−1

p(πt+1 |πt )

t=1

T
Y

p(xt |πt )

(2.2)

t=1

where p(πt+1 |πt ) and p(xt |πt ) are often called state-transition distribution and state-output distribution, respectively. It is easy to verify
that the graphical model as shown in Figure 2.2 exactly describes the
joint distribution Eq. (2.2), by following the DGM semantics - the joint
distribution is defined as the product of local conditionals of each variable
given its parents. Through this example, readers can appreciate the
naturalness of the graphical model approach in formulating probabilistic
models of complex phenomena.
DGM example - Neural network based classifier
Traditionally, each conditional probability distribution p(xv |xpa(v) ) is
parameterized as a lookup table or a linear model (Koller and Friedman,
2009). A more flexible way to parameterize such conditional distributions
is with neural networks. In this case, a neural networks takes as input the
parents of a variable in a directed graph, and produces the distributional
parameters over the variable.
η = NeuralNet(xpa(v) )
p(xv |xpa(v) ) = PDF(xv |η)


18

Basics for EBMs

Figure 2.3: Neural network based classifier. (a) GM representation; (b) Computational graph representation.

where we use NeuralNet(·) and PDF(·|η) to generally denote a neural
network (NN) function and a probability density function (PDF) parameterized by η, respectively. For example, if xv is a continuous variable,
η could denote the mean and variance parameters; if xv is a discrete
variable, η could denote the logits (as explained below).
Basically, we can employ the above concept to build arbitrary directed graphical models, which is not the main focus of this monograph.
For illustration, let us examine the widely used NN based classifier,
which could be cast as a simple two-node directed model for observation
x ∈ RD and class label y ∈ {1, · · · , K}, as shown in Figure 2.3. It is
important to differentiate the GM representation and computational
graph representation.
The classic multi-class logistic regression (Bishop, 2006; Murphy,
2012) basically is to use a single linear layer to obtain the logits zk ’s,
which are then fed to a softmax layer to calculate the class posterior:
exp(zk )
p(y = k|x) = PK
≜ softmax(z1:K )k
j=1 exp(zj )

(2.3)

zk = wkT x + bk , k = 1, · · · , K

(2.4)

where
are often called the logits 2 , and wk ∈ RD , bk ∈ R denote the weight
vector and bias of the linear layer. A simple notation to describe the
2

Presumably because the argument of the sigmoid function is often called the logit,
so analogously, the argument of the softmax function (as a multi-class generalization
of the sigmoid) is also called the logit.


2.1. PROBABILISTIC GRAPHICAL MODELS (PGMS)

19

linear layer Eq. (2.4) is
zk = Linear(x|wk , bk )
or denoted as zk = Linear(x) when the parameters are suppressed.
A recent advance in deep learning is that we can use a multi-layer
neural network, often referred to as a deep neural network (DNN), to
calculate the logits, and then still use the softmax function to obtain
the probability vector from the logits. In this way, the multi-layer
NN could be viewed as a non-linear feature extractor, which hopefully
can be trained to extract features, more discriminative than the raw
observation x. Here in describing an NN based classifier, we show
a directed model which has shallow stochastic connections but use
deep deterministic layers in implementing the conditional distributions.
Variational autoencoder (VAE) (Kingma, Welling, et al., 2019) is also
such a model. Further, directed models with deep stochastic connections
have also been examined such as such as Sigmoid belief Networks (SBNs)
(Neal, 1992; Saul et al., 1996), Helmholtz machines (HMs) (Hinton et al.,
1995; Dayan et al., 1995).
2.1.2

Undirected graphical models

Let us now consider the undirected case. Given an undirected graph
G = (V, E), we again let xV be a collection of random variables indexed
by the nodes of the graph and let C denote the set of cliques3 of the
graph. Associated with each clique C ∈ C, let ϕC (xC ) denote a potential
function, which is a non-negative function of its arguments.
Definition 2.2 (UGM). With the above notation, an undirected graphical
model in terms of G consists of a family of distributions that factorize
as:
1 Y
p(xV ) =
ϕC (xC )
(2.5)
Z C∈C
where Z is the normalizing constant (also known as the partition function) given by
X Y
Z=
ϕC (xC )
(2.6)
xV C∈C
3

A subset of nodes C is called a clique, if every pair of nodes in C is joined.


20

Basics for EBMs

We then also say that p(xV ) has the factorization property (F) according
to G, or simply, p(xV ) factorizes according to G.
For the simple undirected graphical model shown in Figure 2.1(b),
the joint distribution that it describes is4 :
p(x1 , x2 , x3 , x4 ) =

1
ϕ(x1 , x2 )ϕ(x2 , x3 )ϕ(x3 , x4 )ϕ(x1 , x4 )
Z

Remarkably, the potentials do not need to be self-normalized, only
required to be non-negative functions. In contrast, the conditionals
in directed models are required to be normalized. Thus, undirected
models generally offer more flexibility in modeling than directed models.
Moreover, undirected models do not require us to specify edge orientations, and are well suited to be used in problems in which there is little
directional structure to guide the construction of a directed graph.
Factorization and Markov properties in undirected graphical models
We have discussed the equivalence of factorization and conditional
independence in directed models in Section 2.1.1. Similarly, a range of
Markov properties could be defined, relative to a undirected graph G,
such as the global Markov property (G), the pairwise Markov property
(P), the local Markov property (L) (See Section 5.2 of Cowell et al.,
1999). However, in contrast to the discussion in the case of directed
graphical models, the four properties, (F), (G), (L) and (P), are different
in general. In general, we have (F)⇒(G)⇒(L)⇒(P). In the case where
p(xV ) has a positive density (i.e., never zero or negative for any value of
xV ), it can be shown that (P) implies (F), and thus the four properties
become equivalent. This result is known as the Hammersley-Clifford
theorem.
When we use the term undirected graphical model without further
qualification, we shall always mean one that factorizes, hence satisfies all
of the properties. In the following, we will detail the (G) property, which
4

Note that factorization over the set of cliques can be easily shown to be equivalent
to factorization over the set of maximal cliques, i.e., the set of all cliques that are not
properly contained within any other clique. Therefore, the joint distribution in this
example can be written as the product of 4 potentials over the 4 maximum cliques,
divided by the normalizing constant.


2.1. PROBABILISTIC GRAPHICAL MODELS (PGMS)

21

Figure 2.4: Illustration of the global Markov property in UGMs.

is important because it enable us to easily decide when two groups of
variables A and B are conditionally independent give a third group of
variables S in an undirected model.
A probability distribution p(xV ) is said to obey the global Markov
property (G), relative to a undirected graph G , if for any triple (A, B, S)
of disjoint subsets of V such that S separates A from B 5 , we have
xA ⊥ xB |xS . Simply put, the (G) property means: separation between
nodes in the graph implies CI between variables in the distribution, as
illustrated in Figure 2.4.
Energy-based models and Gibbs distributions
Definition 2.3 (EBM). When we are restricted to potential functions
which are strictly positive, it is convenient to express them as exponentials, so that
ϕC (xC ) = exp[−EC (xC )]
where EC (xC ) is called an energy function. Hence, negative log-potential
is often called energy, and high probability states correspond to low
energy configurations. Distributions of this exponential form are called
energy-based models (EBMs), also known as the Gibbs (or Boltzmann)
distributions, originating from statistical physics:
X
1
p(xV ) = exp
−EC (xC )
Z
C∈C
"

#

(2.7)

A benefit of the form of EBMs is that unlike the potential functions,
the log-potential functions (or after negating, the energy functions) are
5

We say S separates A from B if all trails from A to B intersect S.


22

Basics for EBMs

not constrained to be non-negative and can be very flexibly parameterized.
Log-linear models and maximum entropy models
Definition 2.4 (Log-linear model). A classic approach to implement
log-potentials is to define them as a linear function of the parameters:
T
log ϕC (xC ) = θC
fC (xC )

where fC (xC ) is a feature vector derived from (the values of) the variable
xC , θC is the associated feature weight vector. The resulting distribution
has the form
#
"
X
1
T
p(xV ) =
θC fC (xC )
(2.8)
exp
Z(θ)
C
where θ = {θC | C ∈ C} collectively denotes the model parameters and
we explicit the dependence of the normalizing constant on θ. This is
known as a log-linear model or an exponential-family model (Wainwright,
Jordan, et al., 2008; Murphy, 2012).
Example 2.1 (Word morphology). As an illustrative example, suppose
we are interested in making a probabilistic model of English spelling.
This is known as the word morphology problem, which aims to model
English words as letter sequences, x1 , x2 , · · · , by assigning probabilities
(Wang et al., 2018; Pietra et al., 1997; Murphy, 2012). Since certain
letter combinations occur together quite frequently (e.g., “ing”), we will
need higher order cliques to capture this. Suppose we limit ourselves
to letter trigrams. Since the variables in the sequence here are discrete,
we can represent the potential functions as tables of (non-negative)
numbers, which are often called tabular potentials. A tabular potential
has 263 = 17, 576 parameters in it. However, most of these triples will
never occur. An alternative approach is to define indicator functions
(as feature functions) that look for certain special triples, such as “ing”,
“qu-”, etc. Then we can define the potential at position t as follows:
!

ϕt (xt−1 , xt , xt+1 ) = exp

X
k

θk fk (xt−1 , xt , xt+1 )


2.1. PROBABILISTIC GRAPHICAL MODELS (PGMS)

23

where k indexes the different features, corresponding to “ing”, “qu-”,
etc., and fk is the corresponding binary feature function6 . By tying the
parameters across positions, we can define the probability of a word of
any length T using
p(x1:T |θ) = exp

T X
X

!

θk fk (xt−1 , xt , xt+1 )

(2.9)

t=1 k

It has been shown in (Wang et al., 2018) that a (trans-dimensional)
log-linear model outperforms a traditional n-gram model, using the
same set of features.
Modeling with Eq. (2.9) raises the question of where these feature
functions come from. Traditionally, the features fC are mainly handcrafted to reflect domain knowledge, and people take efforts for feature
engineering. In fact, log-linear models with careful feature engineering
were mainstream in NLP before the revival of neural approaches. Recently, neural networks have been successfully used to implement more
general form of EBMs, where, unlike in classic log-linear models, the
log-potentials are defined by NN-based non-linear functions, as we will
show later in Section 2.2.3. Notably, if NN-based log-potential uses a
linear final layer, these NN-based EBMs could in some sense still be
viewed as log-linear models, but on top of the learned features extracted
by the trainable neural networks.
Interestingly, log-linear models are closely connected to maximum
entropy (maxent) models. A classic conclusion is that the maxent distribution (i.e., the distribution with the maximum entropy subject to
the constraints that empirical expectation of features equal to model
expectation of features) is the same as the maximum likelihood distribution from the closure of the set of log-linear distributions (MacKay,
2003; Pietra et al., 1997) (see Appendix B.1 for details). Hence EBMs,
which could be viewed as log-linear models as we describe above, also
enjoy such a connection to maxent models. Remarkably, the EBM distribution p(x) is only known up to a normalizing constant Z and the
potentials are not probabilities, it may be hard for us to understand
6

For example, f“ing” (xt−1 , xt , xt+1 ) equals to 1, if xt−1 = “i”, xt = “n”, xt+1 =
“g”, and equals to 0, otherwise.


24

Basics for EBMs

EBM models. Such maximum entropy property of EBM models allows
us to gain intuitive understanding of EBM models.
2.2

EBM model examples

In this section, we first introduce classic EBM model examples including
Ising model and restricted Boltzmann machine, to familiarize readers
with basic concepts. Then, we focus on modern ones parameterized by
neural networks.
2.2.1

Ising model in statistical physics

(a)

(b)

Figure 2.5: Ising model: (a) The undirected graph representation, (b) A sample.

A well-known example of EBMs is the Ising model, which arose from
statistical physics, and has been a classic probabilistic model for binary
images. It was originally used for modeling the behavior of magnets.
In particular, the spin of an atom can either be spin down or up. The
spins are arranged in a lattice, allowing each spin to interact with its
neighbors. Neighboring spins that agree have a lower energy than those
that disagree; the system tends to the lowest energy but heat disturbs
this tendency, thus creating the possibility of different structural phases.
The two-dimensional square-lattice Ising model is one of the simplest
statistical models to show a phase transition.
Consider a lattice of variables x1:N , each component xi taking values
−1/+1 to model a spin down/up or a pixel black/white, as shown in


2.2. EBM MODEL EXAMPLES

25

√
√
Figure 2.5. This N × N lattice, as an undirected graph, defines an
Ising model p(x1:N ) ∝ exp[−E(x1:N )] with the energy function


X
X
E(x1:N ) = −β  Jxi xj +
Hxi 
i∼j

(2.10)

i

where i ∼ j denotes that two spins i and j are neighbours.
The energy for an Ising model includes two contributions: the interaction between neighboring spins and the effect of an applied external
magnetic field on each individual spin. Consider the case of ferromagnetism. The interaction between neighboring spins tends to induce
parallel alignment of the neighbors, so it should be favorable (negative
energy) when the neighbors are both +1 or both −1, and unfavorable
(positive energy) when the neighbors are +1 next to −1. Hence, for
each pair of neighbors i and j, the interaction energy can be written as
−Jxi xj , where J is a positive coefficient giving the interaction strength.
If the applied magnetic field is pointing up, it favors each spin pointing
up; if the field is pointing down, it favors each spin pointing down.
Hence, for each site i, the field energy can be written as −Hxi , where H
denotes the magnetic moment of the field. Putting these pieces together,
the total energy for the system becomes Eq. (2.10).
It is usual to include in Eq. (2.10) the inverse temperature parameter
β = kB1T , where kB is Boltzmann’s constant, and T the temperature. β
measures how much neighboring spins take identical values is favored.
The larger β (equivalently the lower temperature T ) is, the more favorable of neighboring spins to take identical values. Figure 2.6 shows a
sequence of typical samples from the simulation of N = 4096 spins at a
sequence of decreasing temperatures. At infinite temperature (β = 0),
each spin is completely independent of any other, and if typical states
at infinite temperature are plotted, they look like television snow. For
high, but not infinite temperature, there are small correlations between
neighboring positions, the snow tends to clump a little bit, but the
screen stays randomly looking. When the temperature decreases (β
increases), it is more favored for neighboring spins to take identical
values, so large patches of black or white become to appear.
Through this example, we could get some sense of the characteristics
of EBM modeling - EBMs are natural for modeling interactions (mutual


26

Basics for EBMs

Figure 2.6: Sample states of square Ising models with J = 1, H = 0, kB = 1, N =
4096 at a sequence of temperatures T = 5, 2.5, 2.4, 2.3, 2. (MacKay, 2003)

influences), where the directions of edges cannot be clearly defined. For
example, here it is hard to say one pixel determines another pixel, even
in a probabilistic sense. It is better to use undirected edges to model
interactions between pixels through energy functions. Particularly in
this example, for each pair of nodes connected by an edge in the lattice,
there is an clique potential, which is implemented as follow:
(

ϕij (xi , xj ) =

eβ ,
xi = xj = ±1
−β
e , xi = −xj = ±1

where we set J = 1, H = 0, kB = 1. So when the two neighboring pixels
take the same value, it will contribute eβ to the un-normalized density;
otherwise, contribute e−β .
2.2.2

Restricted Boltzmann Machines (RBMs)

Restricted Boltzmann machines (RBMs) are main building blocks of
deep belief networks (DBNs) (Hinton et al., 2006), which ignite deep
learning. A RBM is a classic undirected graphical model with hidden
variables. It is defined over a bipartite graph, in which the visible,
binary stochastic variables v ∈ {0, 1}D are connected to hidden binary
stochastic variables h ∈ {0, 1}H , as shown in Figure 2.7. The energy of
the state {v, h} is defined over cliques7 :
Eθ (v, h) = −v T W h − bT v − aT h
=−

D X
H
X
i=1 j=1

7

vi Wij hj −

D
X
i=1

bi vi −

H
X

aj hj

j=1

Each node is a clique, and for each edge connecting vi and hj , there is a clique.


2.2. EBM MODEL EXAMPLES

27

Figure 2.7: Restricted Boltzmann Machine. The top layer represents a vector of
stochastic binary hidden variables h and the bottom layer represents a vector of
stochastic binary visible variables v. (Salakhutdinov, 2009)

where θ = {W, b, a} are the model parameters: Wij represents the
symmetric interaction term between visible unit i and hidden unit j; bi
and aj are bias terms. The joint distribution is:
pθ (v, h) =

1
exp [−Eθ (v, h)]
Z(θ)

Z(θ) = −

XX
v

exp [−Eθ (v, h)]

h

where Z(θ) is the normalizing constant.
Due to the special bipartite structure of RBMs, given v, different
hidden units hj ’s are separated and thus are conditionally independent,
according to the Markov property of UGMs. Therefore, the conditional
distribution of h given v is factored:
pθ (h|v) =

Y

pθ (hj |v)

j

!

pθ (hj |v) ∝ exp

X

vi Wij hj + aj hj

i

from which we could easily obtain the conditional probability of a single
unit hj , expressed by the sigmoid function σ(·):
!

pθ (hj = 1|v) = σ

X

Wij vi + aj

(2.11)

i

Similarly, the conditional distribution of v given h is also factored


28

Basics for EBMs

and given by:
pθ (v|h) =

Y

pθ (vi |h)

i



X
pθ (vi |h) ∝ exp  vi Wij hj + bi vi 
j



X
pθ (vi = 1|h) = σ  Wij hj + bi 

(2.12)

j

Remarkably, it can be seen from Eq. (2.11) that an RBM is related to
a stochastic version of a neural network, also known as a sigmoid belief
network (SBN) (Neal, 1992; Saul et al., 1996). To see this8 , imagine
that the nodes v1:D and the edges of an RBM as shown in Figure
2.7 are viewed as the input layer and the synaptic connections of a
two-layer SBN; the output layer at h1:H fires, hj taking 0 or 1, j =
1, · · · , H, stochastically from a sigmoid activation function. Therefore,
the conditional distribution pθ (h|v) induced from an RBM can be viewed
as implementing a two-layer SBN, and a two-layer SBN is very similar to
an ordinary two-layer feedfoward neural network, except it stochastically
fires instead of outputting activations to the next layer (see Figure 2.8).
This resemblance between RBMs and NNs is the underlying intuition
that a stack of RBMs can be trained as pre-training for a multi-layer
neural network (Salakhutdinov, 2009).

(a)

(b)

Figure 2.8: (a) Restricted Boltzmann machine (RBM), (b) Sigmoid belief network
(SBN).

8

Such relationship could also seen from Eq. (2.12) from an opposite direction.


2.2. EBM MODEL EXAMPLES
2.2.3

29

EBMs parameterized by neural networks

Classic EBM models employ simple energy functions, e.g., the energy
functions in both the Ising model and the RBM model are bilinear.
Recently, beyond the classic EBM models, there have emerged a bundle
of deep EBM models (deep undirected generative models), which are
characterized by using multiple layers of stochastic or deterministic
variables.
Those deep EBM models with multiple stochastic hidden layers such
as deep belief networks (DBNs) (Hinton et al., 2006) and deep Boltzmann machines (DBMs) (Salakhutdinov and Hinton, 2009) involve very
difficult inference and learning, which severely limits their applications
beyond of the form of pre-training. Another type of deep EBM models,
which appear to be more successfully applied, is to directly define the
energy function through a multi-layer neural network. In this case, the
layers of the network do not represent latent variables but rather are
deterministic transformations of input observations.
For simplicity, we will first introduce unconditional models in the
following. It is relatively straightforward to extend to models with
conditioning variables, which will be detailed in Chapter 4.
Definition 2.5 (EBMs parameterized by neural networks). Generally,
consider an EBM to define a probability distribution for a collection of
random variables x ∈ X with parameter θ in the form:
1
exp [Uθ (x)]
(2.13)
pθ (x) =
Z(θ)
where X denotes the space of all possible values of x, and Z(θ) denotes
the normalizing constant:
Z(θ) =

Z

exp [Uθ (x)] dx

(2.14)

Uθ (x) : X → R denotes the (log) potential function9 which assigns a
scalar value to each configuration of x in X and can be very flexibly
9

In the literature, log potential function is sometimes also referred to as potential
function. Whether taking log or not should be clear from the context, although
with abuse of nomenclature. Moreover, reversing the potential function will obtain
the energy function, and vise versa. So an EBM could be equivalently defined by an
energy function or a potential function.


30

Basics for EBMs

Figure 2.9: Potential functions in EBMs can be flexibly parameterized by neural
networks for images, natural languages and so on.

parameterized through neural networks of different architectures. For
different applications, X could be discrete or continuous, and x could be
fix-dimensional or trans-dimensional (i.e., sequences of varying lengths).
For example, images are fix-dimensional continuous data (i.e., X =
RD ), and natural languages are sequences taking discrete tokens (i.e.,
S
X = l Vl where V is the vocabulary of tokens). The general idea is
to parameterize Uθ (x) by a neural network, taking multi-variate x as
input and outputting scalar Uθ (x), so that we can take advantage of
the representation power of neural networks, as shown in Figure 2.9.
Historically, this type of EBMs has been studied several times in
different contexts. They are once called deep energy models (DEMs) in
(Ngiam et al., 2011; Kim and Bengio, 2016), generative ConvNet in (Xie
et al., 2016), descriptive models in (Guo et al., 2003; Xie et al., 2018),
neural trans-dimensional random field language models in (Wang and
Ou, 2017; Wang and Ou, 2018b; Wang and Ou, 2018a; Gao et al., 2020),
neural random fields (NRFs) in (Song and Ou, 2018). There are some
specific differences between implementation (or say, parameterization)
of the potential functions in these deep EBM models.
• The potential function in (Ngiam et al., 2011; Kim and Bengio,
2016) uses the form of a product of experts (Hinton, 2002) and is
composed of linear and squared terms and the aggregated (i.e.,


2.2. EBM MODEL EXAMPLES

31

the sum of) logistic regression responses of a set of weak classifier
(“expert”) for images x:
Uθ (x) = bT x −

X
1 T
T
x
x
+
log(1 + ewi fθ (x)+ci )
σ2
i

(2.15)

The first two terms capture the mean and the global variance, and
the last term comes from a set of experts over the feature data
space fθ (x). fθ (x) is the output of a feedforward neural network.
One can allow activations other than logitic sigmoid as shown in
Eq. (2.15).
• In (Wang and Ou, 2017; Wang and Ou, 2018b; Wang and Ou,
2018a; Gao et al., 2020; Deng et al., 2020), deep EBM models are
defined over sequences (natural language sentences).
• In (Xie et al., 2016; Guo et al., 2003; Xie et al., 2018; Wang and
Ou, 2017; Deng et al., 2020), the EBM is defined in the form of
exponential tilting of a reference distribution q(x):
pθ (x) =

1
q(x) exp [Uθ (x)]
Z(θ)

(2.16)

In (Xie et al., 2016; Guo et al., 2003; Xie et al., 2018) for image
modeling, a Gaussian white noise distribution is used as q(x). In
(Wang and Ou, 2017; Deng et al., 2020) for language modeling,
autoregressive language models based on LSTM or Transformer
networks are often used as q(x).
Despite the different parameterizations of the potential function
in various deep EBM models, these differences would not affect much
when presenting learning algorithms, as we will introduce immediately
in the next section.
Comparison between classic undirected graphical models and modern
EBM models parameterized by neural networks
As we introduced before, classic undirected graphical models such as the
Ising model and the RBM model employ simple potential functions like
linear or bilinear, while modern EBM models utilize neural networks to


32

Basics for EBMs

parameterize potential functions. From this apparent difference, there
is a remarkable implication, which will be described below.
In graphical modeling terminology, without loss of generality, let
each component of x indexed by a node in an undirected graph. The
EBM distribution pθ (x) is defined to be decomposed over cliques, as
shown in Eq. (2.7). Such decomposition reduces the complexity of model
representation but maybe at the sacrifice of model expressive capacity.
In an EBM model parameterized by a neural network as introduced
above, the model essentially becomes defined over a fully-connected
undirected graph and captures interactions in x to the largest order,
since the neural potential function U (x) involves all the components in
x. In this manner, hopefully we can take advantage of the representation
power of neural networks for modeling. As shown above, we can define
very flexible potential functions Uθ (x), by utilizing neural networks
of various architectures to define the densities Eq. (2.13). For this
reason, it is often assumed in theoretical analysis that pθ (x) has infinite
capacity (sometime called in the non-parametric setting). In practice, the
performances of the models largely depend on how they are optimized
in model learning.
2.3

Learning EBMs by maximum likelihood

The de facto standard for learning probabilistic models from IID (independent and identically distributed) data is maximum likelihood
estimation (MLE) . Let pθ (x), as defined in Eq. (2.13), be an EBM
P
model parameterized by θ, and pemp (x) ≜ N1 N
i=1 δ(x − xi ) denote the
empirical distribution for a training dataset consisting of N IID data
points {x1 , · · · , xN }. We can fit pθ (x) to data by maximizing the scaled
log-likelihood of the data, defined by
N
N
1 X
1 X
L(θ) ≜
log pθ (xi ) =
Uθ (xi ) − log Zθ
N i=1
N i=1

"

#

(2.17)

as a function of θ.
Maximizing likelihood is equivalent to minimizing the (inclusive)


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

33

KL divergence between pemp (x) and pθ (x), because
KL[pemp (x)||pθ (x)] = Ex∼pemp (x) [log pemp (x)] − Ex∼pemp (x) [log pθ (x)]
= constant − L(θ),
where the second equality holds because Ex∼pemp (x) [log pemp (x)] does
not depend on θ.
Taking the derivative of log-likelihood with respect to (w.r.t.) θ, the
first term of the gradient is a sum over data points and can be written
as an expectation under the empirical distribution:
N
1X
∇θ Uθ (xi ) = Epemp (x) [∇θ Uθ (x)]
n i=1

The second term involves taking the derivative of the log normalizing
constant, which, as shown below, can also be written as an expectation
but under model distribution pθ (x):
1
1
∇θ Zθ =
∇θ exp [Uθ (x)] dx
Zθ
Zθ
Z
1
exp [Uθ (x)] ∇θ Uθ (x)dx
=
Zθ
Z
exp [Uθ (x)]
=
∇θ Uθ (x)dx
Zθ
Z

∇θ log Zθ =

=

Z

pθ (x)∇θ Uθ (x)dx

= Epθ (x) [∇θ Uθ (x)]

(2.18)

Combining the two terms, we obtain the core formula in learning EBMs:
∇θ L(θ) = Epemp (x) [∇θ Uθ (x)] − Epθ (x) [∇θ Uθ (x)]

(2.19)

The maximum likelihood estimate of θ is obtained as a solution to
∇θ L(θ) = 0. Obviously, the challenge is in calculating the expectation
under model distribution, which is often intractable to compute exactly
and approximated by Monte Carlo averaging.
Suppose that we can draw random samples from the EBM pθ (x),
denoted as x(1) , · · · , x(M ) ∼ pθ (x), then we can obtain an unbiased


34

Basics for EBMs

estimate of the second term in the log-likelihood gradient10 :
Epθ (x) [∇θ Uθ (x)] ≈

M
1 X
∇θ Uθ (x(j) )
M j=1

(2.20)

Additionally, note that the computational cost of Eq. (2.19) is linear in
N (the number of training data points). So when N is large, we often
apply minibatching as follows. Through random drawing a minibatch
of κ1 , · · · , κB from {1, · · · , N }, we can obtain an unbiased estimate of
the first term in the log-likelihood gradient:
B
1 X
Epemp (x) [∇θ Uθ (x)] ≈
∇θ Uθ (xκj )
B j=1

(2.21)

Combining Eq. (2.20) and Eq. (2.21) allows us to optimize the parameters with stochastic gradient ascent. See further introduction in Section
2.3.3.
We have shown above the basic idea of applying Monte Carlo methods in maximum likelihood learning of EBMs. As long as we can draw
random samples from the model pθ (x), we have access to an unbiased
estimate of the log-likelihood gradient, allowing us to optimize the
parameters with stochastic gradient ascent. So a critical step in
learning EBMs by Monte Carlo methods is the simulation (sampling) from the EBM distribution pθ (x), as defined in Eq. (2.13) in
general.
In some applications, directly generating independent samples from
an distribution is not feasible. There are two broad classes of strategies
for sampling from high-dimensional distributions. The first is the MCMC
strategy, which produces statistically dependent samples based on the
theory of Markov chains. The second is the importance sampling (IS)
strategy, in which independent samples are generated from a trial
distribution (a.k.a. a proposal distribution) and then weighted according
to the importance weight. The two methods will be introduced as follows
in Section 2.3.1 and Section 2.3.2 respectively. Further introduction can
10

Here we suppose that the samples are direct samples from pθ (x). As will
be described in Section 2.3.3, such assumption can be relaxed in the stochastic
approximation methodology, which allows us to use Markov chain samples.


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

35

be found in monographs (Neal, 1993; Liu, 2001) or general textbooks on
machine learning (Koller and Friedman, 2009; Murphy, 2012; Bishop,
2006).
2.3.1

Markov Chain Monte Carlo (MCMC)

Let pθ (x) be the target distribution under investigation. The basic
idea of Markov Chain Monte Carlo (MCMC) is to construct a Markov
chain in the state space of x, denoted by X , so that the limiting (or
say, stationary or equilibrium) distribution of this chain is the target
distribution pθ . Roughly speaking, this means that the fraction of time
spent in each state along the chain in a long run is equal to pθ .
Metropolis–Hastings algorithm
A classic MCMC method is the Metropolis-Hastings (MH) algorithm,
which is described in Algorithm 1. Starting with any configuration x(0) ,
the MH algorithm proceeds by iterating “propose” and “accept/reject”,
as shown in Line 3 and Line 4 respectively. A single run of “propose”
and “accept/reject” is often called a MH transition, which defines a
particular Markov chain.
• First, we propose to move from the previous state x(t−1) to a
new state x′ with probability q(x′ |x(t−1) ), where q is called the
proposal distribution.
• Having proposed a move to x′ , we then decide whether to accept
this proposal or not according to some formula, which ensures that
the limiting distribution of this chain is the target distribution pθ .
If the proposal is accepted, the new state is x′ , otherwise the new
state is the same as the previous state, x(t−1) (i.e., we repeat the
sample).
At the end of the iterations, we obtain a realization (or say, a single
run) of the Markov chain, x(1) , · · · , x(T ) . It can be shown that this
particular Markov chain leave pθ invariant. Note that theoretically, only
the limiting distribution of the chain follows pθ . So it is necessary to
discard a few initial samples until the Markov chain has burned in,


36

Basics for EBMs

Algorithm 1 Metropolis-Hastings Algorithm
Input: A target distribution pθ (x), a proposal distribution q(x′ |x)
1: Randomly initialize x(0) ;
2: for t = 1 to T do
3:
Generate x′ from the proposal q(x′ |x(t−1)n);
o
′ )q(x(t−1) |x′ )
4:
Accept x(t) = x′ with probability min 1, p p(xθ (x
(t−1) )q(x′ |x(t−1) ) ,
otherwise set x(t) = x(t−1) ;
5: end for
6: Return: {x(1) , · · · , x(T ) }

θ

or entered its stationary distribution. The remained samples can then
be used for Monte Carlo averaging such as in Eq. (2.20) to estimate
expectations.
In practice, the accept/reject step is taken by drawing U ∼ Uni[0, 1],
calculate the acceptance probability
pθ (x′ )q(x(t−1) |x′ )
r = min 1,
pθ (x(t−1) )q(x′ |x(t−1) )
(

)

(2.22)

and update
(
(t)

x

=

x′ ,
if U ≤ r
x(t−1) , otherwise.

Critically, the MH algorithm only needs to know the target distribution up to a normalization constant. In particular, consider the EBM
distribution pθ (x) = Z1θ exp [Uθ (x)], then MH ratio
1
′
(t−1) |x′ )
pθ (x′ )q(x(t−1) |x′ )
Zθ exp [Uθ (x )] q(x


= 1
(t−1) ) q(x′ |x(t−1) )
pθ (x(t−1) )q(x′ |x(t−1) )
Zθ exp Uθ (x

in which the Zθ ’s cancel. Hence, we can sample from pθ (x) even if the
normalizing constant Zθ is unknown.
Remarkably, the user is free to use any kind of proposal they want,
subject to some theoretical conditions. This makes MH quite a flexible
method. We introduce two special algorithms that are instances of the
general MH algorithm.


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

37

The Metropolis algorithm. If the proposal transition function is
symmetric, so q(x′ |x) = q(x|x′ ), the acceptance probability is given by
the following formula:
pθ (x′ )
r = min 1,
pθ (x(t−1) )




We see that if x′ is more probable than x, we definitely move there
′)
′
(since p p(xθ (x
(t−1) ) > 1), but if x is less probable, we may still move there
θ
anyway, depending on the relative probabilities. So instead of greedily
moving to only more probable states, we occasionally allow “downhill”
moves to less probable states.
The Metropolis independence sampler (MIS). Another special choice
of the transition function is in the form of q(x′ ) = q(x′ |x); that is, the
proposed move x′ is generated independent of the previous state x(t−1) .
In MIS, the acceptance probability becomes:


r = min 1,

w(x′ )
w(x(t−1) )



θ (x)
is the usual importance weight.
where w(x) = pq(x)

Gibbs sampling
The Gibbs sampler is conceptually the simplest of the Markov chain
sampling method, and as we introduce below, could be viewed as the
MCMC analog of coordinate descent.
Suppose we wish to sample from the joint distribution for x =
(x1 , · · · , xn ) given by p(x1 , · · · , xn ), where the range of the xi may be
either continuous or discrete. The Gibbs sampler does this by repeatedly
replacing each component, say xi , with a value picked from the full
conditional for variable xi , i.e., the distribution of xi conditional on the
current values of all other components (x1 , · · · , xi−1 , xi+1 , · · · , xn ) ≜ x\i .
This process can be seen as generating a realization of a Markov chain
that is built from a set of base transition probabilities Bi , for i = 1, · · · , n.
Bi leaves all the components except xi unchanged, and draws a new xi
from its full conditional, which is assumed to be a feasible operation.


38

Basics for EBMs

Algorithm 2 Gibbs sampler
Input: A target distribution p(x) for x = (x1 , · · · , xn )
(0)
(0)
1: Randomly initialize x(0) = (x1 , · · · , xn );
2: for t = 1 to T do
(t)
3:
Pick
x1
from
the
distribution
for
x1
given
(t−1) (t−1)
(t−1)
x2
, x3
, · · · , xn ;
(t)
(t) (t−1)
(t−1)
4:
Pick x2 from the distribution for x2 given x1 , x3
, · · · , xn ;
..
.
(t)
Pick
xi
from
the
distribution
(t)
(t)
(t−1)
(t−1)
x1 , · · · , xi−1 , xi+1 , · · · , xn ;
..
.

5:
6:

for

xi

given

7:
(t)
(t) (t)
(t)
8:
Pick xn from the distribution for xn given x1 , x2 , · · · , xn−1 ;
9: end for
10: Return: {x(1) , · · · , x(T ) }

The Gibbs sampling algorithm can be described as simulating a
homogeneous Markov chain, x(0) , x(1) , x(2) , · · · , with transition matrix
P = B1 × B2 × · · · × Bn , as shown in Algorithm 2. Generating x(t) from
x(t−1) , i.e., from Line 3 to Line 8, is called a sweep. Note that the new
value for xi−1 is used immediately when picking the new value for xi .
Starting from the Gibbs sampler, we provide three useful points.
• Constructing a Markov chain from base transitions. In
sampling application, our goal is to find an ergodic Markov chain
that converges to the target invariant distribution p(x), at as fast
a rate as possible. The Gibbs sampling embodies a useful, general
method to construct such a Markov chain, as described below.
Consider to construct the transition probabilities for such a chain
from a set of base transition probabilities, given by B1 , · · · , Bs 11 ,
each of which leaves the target distribution invariant. It can be
shown that when the base transitions are applied in sequence, if
11

Generally, the number of base transitions s is not necessarily equal to n, the
dimensionaliy of x.


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

39

a distribution is invariant with respect to all the base transitions,
then it is also invariant with respect to P = B1 × B2 × · · · × Bs
(Neal, 1993). We show in the next point that each base transition
in Gibbs sampler leaves the target distribution invariant, so that
we can understand why the Gibbs sampler works.
• Gibbs sampler is a special case of MH, and thus leaves
the target distribution invariant. Each base transition in Gibbs
sampler is equivalent to using MH with a proposal of the form
q(x′ |x) = p(x′i |x\i )1(x′\i = x\i )
That is, we move to a new state where xi is sampled from its
full conditional, but x\i is left unchanged. It turns out that the
acceptance rate of such proposal is 1, because the MH ratio
p(x′i |x′\i )p(x′\i )p(xi |x′\i )
p(x′ )q(x|x′ )
=
p(x)q(x′ |x)
p(xi |x\i )p(x\i )p(x′i |x\i )
=

p(x′i |x\i )p(x\i )p(xi |x\i )
=1
p(xi |x\i )p(x\i )p(x′i |x\i )

where we exploited that fact that x′\i = x\i , and that q(x′ |x) =
p(x′i |x\i ). So every time the Gibbs sampler draws a new value
from the full conditional of a component and always accepts it.
• MH within Gibbs sampling. Gibbs sampling assumes that
drawing from the full conditional of each component is tractable.
When sampling from the full conditionals of a certain component
is intractable, we can replace the exact sampling of this component by a MH sampling step, i.e., a single run of “propose” and
“accept/reject”. The resulting algorithm is thus called MH within
Gibbs sampling.
Gradient guided MCMC
For continuous distribution, MCMC samplers leveraging continuous dynamics (namely continuous-time Markov processes described by stochastic differential equations), such as Langevin dynamics (LD) and Hamiltonian Monte Carlo (HMC) (Neal, 2011), are known to be efficient in


40

Basics for EBMs

exploring the continuous state space. Simulating the continuous dynamics leads to the target distribution as the stationary distribution.
In practice, a discretization of the continuous-time system is needed
necessitating a Metropolis-Hastings (MH) correction, though still with
high acceptance probability. Recently, stochastic gradient variants of
continuous-dynamic samplers have emerged, showing that adding the
“right amount” of noise to stochastic gradient ascent iterates leads to
samples from the target posterior as the step size is annealed (Welling
and Teh, 2011; Chen et al., 2014). In either manners, the Markov transition kernel defined by the continuous dynamical system usually involves
using the gradients of the target distribution w.r.t the data vector x.
Remarkably, the gradient of log-density of an EBM model w.r.t. the
data vector x is easy to calculate:
∇x log pθ (x) = ∇x Uθ (x) − ∇x log Z(θ) = ∇x Uθ (x)
|

{z

=0

}

which does not require the calculation of the normalizing constant.
In the following, we mainly introduce LD and SGLD. For HMC and
stochastic gradient Hamiltonian Monte Carlo (SGHMC), readers can
refer to (Neal, 2011; Chen et al., 2014; Ma et al., 2015).
Langevin dynamics (LD) sampler. Given current sample x0 , a new
observation is proposed as
σ2
∇x Uθ (x0 ) + σε,
(2.23)
2
2
where ε ∼ N (0, I) is a Gaussian noise, and σ is a step size. The next
sample x1 may directly be the proposal x 1 , in which case the Markov
2
transition from x0 to x1 does not strictly leave pθ invariant, but the
sampling bias may usually be small for σ ≈ 0. To allow large σ, a
correction can be achieved by accepting or rejecting the proposal x 1 ,
2
i.e., setting x1 = x 1 or x0 , with the Metropolis-Hastings probability.
2
Langevin sampling with rejection is known as the Metropolis-Adjusted
Langevin Algorithm (MALA) (Besag, 1994; Roberts and Tweedie, 1996).
x 1 = x0 +

Stochastic gradient Langevin dynamics (SGLD). Recently, stochastic gradient samplers have emerged in simulating posterior samples


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

41

in large-scale Bayesian inference, such as SGLD (stochastic gradient
Langevin dynamics) (Welling and Teh, 2011) and SGHMC (Stochastic Gradient Hamiltonian Monte Carlo) (Chen et al., 2014). To illustrate, consider the posterior p(θ|D) of model parameters θ given
the observed dataset D, with abuse of notation. We have p(θ|D) ∝
P
exp [ x∈D log pθ (x) + log p(θ)], which is taken as the target distribution.
∂
log p(θ|D), which needs a sweep
Instead of using full-data gradients ∂θ
over the entire dataset, these stochastic gradient
samplers subsample the

∂
dataset and use stochastic gradients ∂θ

|D̃| P
x∈D̃ log pθ (x) + log p(θ)
|D|

in the dynamic simulation, where D̃ ⊂ D is a subsampled data subset.
In this manner, the computation cost is significantly reduced in each
iteration and such Bayesian inference methods scale to large datasets.
In practice, sampling is based on a discretization of the continuous
dynamics. Despite the discretization error and the noise introduced by
the stochastic gradients, it can be shown that simulating the discretized
dynamics with stochastic gradients also leads to the target distribution
as the stationary distribution, when the step sizes are annealed to zero
at a certain rate12 . The convergence of SGLD and SGHMC can be
obtained from (Sato and Nakagawa, 2014; Chen et al., 2014; Ma et al.,
2015). We summarize in Theorem 2.1 for SGLD.
Theorem 2.1. Denote the target density as p(z; λ) with given λ. Assume
that one can compute a noisy, unbiased estimate ∆(z; λ) (a stochastic
∂
gradient) to the gradient ∂z
log p(z; λ). For a sequence of asymptotically
P
P∞ 2
vanishing time-steps {δl , l ≥ 1} (satisfying ∞
l=1 δl = ∞ and
l=1 δl <
(0)
∞), the SGLD algorithm iterates as follows, starting from z :
z (l) =z (l−1) + δl ∆(z (l−1) ; λ) +

2δl η (l) ,

p

η (l) ∼ N (0, I), l = 1, · · ·

(2.24)

The iterations of Eq. (2.24) lead to the target distribution p(z; λ) as
the stationary distribution.

12

A Metropolis-Hastings (MH) correction can be applied, when it is hard to check
the annealing condition is satisfied or not.


42
2.3.2

Basics for EBMs
Importance sampling

One of the principal reasons for wishing to sample from complicated
distributions is to be able to estimate expectations of the form Eq.
(2.20). The technique of importance sampling (IS) provides a framework
for approximating expectations directly.
Suppose, generally, one is interested in estimating
Epθ (x) [g(x)] =

Z

pθ (x)g(x)dx

(2.25)

Importance sampling is based on the use of a proposal distribution q(x)
from which it is easy to draw samples, say, x(1) , · · · , x(M ) ∼ q(x). We
can then express the expectation by Monte Carlo averaging, i.e., in the
form of a finite sum over samples {x(j) } drawn from q(x):
Epθ (x) [g(x)] =
≈

Z

q(x)

pθ (x)
g(x)dx
q(x)

M
1 X
pθ (x(j) )
g(x(j) )
M j=1 q(x(j) )

(2.26)

which is an unbiased estimate of the expectation in Eq. (2.25). The
(j) )
θ (x
quantities w(j) = pq(x
(j) ) are known as importance weights, and they
correct the bias that {x(j) } are drawn from the proposal distribution
rather than from the target distribution.
It will often be the case that the distribution pθ (x) can only be
evaluated up to a normalization constant (e.g., in EBMs), so that
pθ (x) = p̃θ (x)/Zp where p̃θ (x) can be evaluated easily, whereas Zp
denotes the unknown normalizing constant. Generally, we may wish to
use a proposal q(x) = q̃(x)/Zq , which is also in the un-normalized form.
We then have
Zq
Epθ (x) [g(x)] =
Zp
≈

Z

q(x)

p̃θ (x)
g(x)dx
q̃(x)

M
Zq 1 X
p̃θ (x(j) )
g(x(j) )
Zp M j=1 q̃(x(j) )

(2.27)


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

43

Z

We can use the same samples to evaluate the ratio Zpq with the result:
1
Zp
=
Zq
Zq
≈

Z

p̃θ (x)dx =

Z

q(x)

p̃θ (x)
dx
q̃(x)

M
1 X
p̃θ (x(j) )
M j=1 q̃(x(j) )

(2.28)

Z

which is an unbiased estimate of Zpq . When q̃(x) is self-normalized (i.e.,
Zq = 1), Eq. (2.28) shows a way of using importance sampling to
estimate the normalizing constant Zp .
(j)

)
θ (x
Combining Eq. (2.27) and Eq. (2.28) and letting w̃(j) = p̃q̃(x
(j) ) , we
can approximate the expectation by

Epθ (x) [g(x)] ≈

w̃(1) g(x(1) ) + · · · + w̃(M ) g(x(M ) )
w̃(1) + · · · + w̃(M )

(2.29)

which turns to be a biased estimate of the expectation in Eq. (2.25).
(j)
Further, by defining normalized importance weights ω (j) = PMw̃ (j) ,
Eq. (2.29) can be re-written in a simpler form:
Epθ (x) [g(x)] ≈

M
X

ω (j) g(x(j) )

j=1

w̃

(2.30)

j=1

which is often referred to as self-normalized importance sampling (SNIS),
for example, in (Parshakova et al., 2019). A major advantage of using
the biased estimate Eq. (2.30) instead of the unbiased estimate Eq.
(2.27) is that in using the former (although biased), we need only to
θ (x)
know the ratio pq(x)
up to a multiplicative constant; whereas in the
latter, the ratio needs to be known exactly.
Remarkably, the success of the importance sampling approach depends crucially on how well the proposal distribution q(x) matches the
target distribution pθ (x).
2.3.3

Stochastic approximation methods

In the above, we introduce the basics of some classic Monte Carlo
methods and the general idea of applying them in maximum likelihood


44

Basics for EBMs

Algorithm 3 A naive algorithm of learning EBMs by Monte Carlo
methods
Input: A target EBM distribution pθ (x)
for each minibatch of size B do
Obtain empirical expectations by Eq. (2.21);
for j = 1 to M do
Draw x(j) with pθ (x) as the target distribution;
end for
Obtain model expectations by Eq. (2.20);
Update parameter θ by gradient Eq. (2.19);
end for
learning of EBMs. We show in Eq. (2.19) that the log-likelihood gradient for learning EBMs is equal to the difference between empirical
expectation and model expectation, and in Eq. (2.20), that the model
expectation is approximated by Monte Carlo sampling from EBM distribution pθ (x). Combining Eq. (2.19), Eq. (2.20) and Eq. (2.21), we could
obtain a naive algorithm of learning EBMs by Monte Carlo methods,
as shown in Algorithm 3.
Typically we use MCMC to generate the samples x(1) , · · · , x(M ) ,
for each minibatch. For EBM distribution pθ (x), which can only be
evaluated up to a normalization constant, using the unbiased IS estimate
Eq. (2.27) is intractable. Using the biased IS estimate Eq. (2.30) will
produce biased gradient estimates, which were used in some prior studies
(Parshakova et al., 2019).
In learning EBMs by Monte Carlo methods, at first thought (as
shown in Algorithm 3), there are two loops. The outer loop iterates
over minibatches of training data. The inner loop iterates to generate
samples via MCMC (e.g., MH), but running MCMC sufficiently long
(with large M ) to approaching convergence at the inner loop would
be extremely slow. Fortunately, it was shown by (Younes, 1989) that
we can start the MCMC chain at its previous value from the outer
loop, and just take a few Markov moves in the inner loop (i.e., using
small M ). So in this way, the Markov chain evolves persistently across
outer loops. This algorithm, called stochastic maximum likelihood (SML)


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

45

Algorithm 4 The general stochastic approximation (SA) algorithm
for t = 1, 2, · · · do
Monte Carlo sampling: Draw a sample z (t) with a Markov transition kernel Kλ(t−1) (z (t−1) , ·), which starts with z (t−1) and admits
pλ(t−1) (·) as the invariant distribution.
SA updating: Set λ(t) = λ(t−1) + γt Fλ(t−1) (z (t) ), where γt is the
learning rate.
end for
(Younes, 1989), along with its variants appeared in the literature, turn
out to be application of the more general stochastic approximation (SA)
methodolgy to learning EBMs. See further introduction in Section 2.3.3.
Note. The above cognition of the EBM learning by Monte Carlo
is very important. Many people may think that learning with Monte
Carlo methods is very slow. But since we do not need to run MCMC
to convergence at the inner loop, but just a few steps. Learning with
MCMC is not so expensive as people might think.
Introduction to stochastic approximation (SA) methodology
Stochastic approximation methods are an important family of iterative
stochastic optimization algorithms, introduced in (Robbins and Monro,
1951) and extensively studied (Benveniste et al., 1990; Chen, 2002).
Basically, stochastic approximation provides a mathematical framework
for stochastically solving a root finding problem, which has the form of
expectations being equal to zeros. Suppose that the objective is to find
the solution λ∗ of f (λ) = 0 with
f (λ) = Ez∼pλ (·) [Fλ (z)],

(2.31)

where λ is a d-dimensional parameter vector in Λ ⊂ Rd , and z is an observation from a probability distribution pλ (·) depending on λ. Fλ (z) ∈ Rd
is a function of z, providing d-dimensional stochastic measurements of
the so-called mean-field function f (λ). Intuitively, we solve a system of
simultaneous equations, f (λ) = 0, which consists of d constraints, for
determining d-dimensional λ.


46

Basics for EBMs

Given some initialization λ(0) and z (0) , a general SA algorithm
iterates Monte Carlo sampling and parameter updating, as shown in
Algorithm 4. The convergence of SA has been established under conditions (Benveniste et al., 1990; Andrieu et al., 2005; Song et al., 2014),
including a few technical requirements for the mean-field function f (λ),
the Markov transition kernel Kλ(t−1) (z (t−1) , ·) and the learning rates.
Particularly, when f (λ) corresponds to the gradient of some objective
function, then λ(t) will converge to local optimum, driven by stochastic
gradients Fλ (z). For completeness, we provide a short summary on the
convergence of {λt , t ≥ 1} in Algorithm 4, based on Theorem 1 in (Song
et al., 2014).
Theorem 2.2. Let {γt } be a monotone non-increasing sequence of
P
P∞ 2
positive numbers such that13 ∞
t=1 γt = ∞ and
t=1 γt < ∞. Assume
that Λ is compact and the Lyapunov condition on f (λ) and the drift
condition on the transition kernel Kλ (·|·) hold. Then we have: d(λt , L) →
0 almost surely as t → ∞, where L = {λ : f (λ) = 0} and d(λ, L) =
inf λ′ ∈L ||λ − λ′ ||.
Remarkably, Algorithm 4 shows stochastic approximation with
Markovian perturbations (Benveniste et al., 1990). It is more general than the non-Markovian SA which requires exact sampling z (t) ∼
pλ(t−1) (·) at each iteration and in some tasks can hardly be realized. In
non-Markovian SA, we check that Fλ (z) is unbiased estimates of f (λ),
while in SA with Markovian perturbations, we check the ergodicity
property of the Markov transition kernel.
To speed up convergence, during each SA iteration, it is possible
to generate a set of multiple observations z by performing the Markov
transition repeatedly and then use the average of the corresponding
values of Fλ (z) for updating λ, which is known as SA with multiple
moves (Wang et al., 2018), as shown in Algorithm 5.
Note I. Perhaps the most familiar application of SA in machine learning literature is the stochastic gradient descent (SGD) technique, particularly the minibatching technique. When the objective (and therefore its
13

In practice, we can set a large learning rate at the early stage of learning and
decrease to 1/t for convergence.


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

47

Algorithm 5 SA with multiple moves
for t = 1, 2, · · · do
1.
Monte Carlo sampling: Set z (t,0) = z (t−1,K) . For k from
1 to K, generate z (t,k) ∼ Kλ(t−1) (z (t,k−1) , ·), where
Kλ(t−1) (z (t,k−1) , ·) is a Markov transition kernel that admits
pλ(t−1) (·) as the invariant distribution.
2.

SA updating: Set λ(t) = λ(t−1) + γt { K1
where B (t) = {z (t,k) |k = 1, · · · , K}.
end for

P

z∈B (t) Fλ(t−1) (z)},

gradient) is a sum of many terms that can be computed independently,
SGD samples one term at a time and follows one noisy estimate of
the gradient with a decreasing step size. Furthermore, it can be easily
seen that SGD training with minibatches is an application of SA with
multiple moves.
Note II. Generally, SA represents an iterative methodology to find the
root of an expectation. Each iteration consists of a sampling step and a
parameter updating step. Basically, we use MCMC to simulate the noisy
measurements to approximate the expectation. A keypoint is that we do
not need to wait the chain to converge, but use a decaying learning rate
to guarantee the convergence. Intuitively, as the learning rate becomes
sufficiently small compared to the mixing rate of the Markov chain, the
chain will stay close to the stationary distribution, even if it only runs
for one Markov move per parameter update.
Application of SA to learning EBMs
It can be easily seen that the EBM gradients ∇θ L(θ) in Eq. (2.19)
exactly follows the form of Eq. (2.31), as summarized in Theorem 2.3.
So the problem of maximum likelihood estimate of EBM parameters
can then be solved by setting the gradients to zeros and applying the SA
algorithm to finding the root for the resulting system of simultaneous
equations.


48

Basics for EBMs

Algorithm 6 Stochastic maximum likelihood for fitting an EBM
for t = 1, 2, · · · do
Sampling: Draw xκ from training data, and simulate a sample x(t)
with a Markov transition kernel Kθ(t−1) (x(t−1) , ·), which starts with
x(t−1) and admits pθ(t−1) (·) as the invariant distribution.
Updating: Update θ by gradient ascent as:
θ(t) = θ(t−1) + γt {∇θ Uθ (xκ ) − ∇θ Uθ (x(t) )}|θ=θ(t−1)

(2.32)

end for
Theorem 2.3. Consider an EBM distribution pθ (x) parameterized with
θ as shown in Eq. (2.13), and a training dataset consisting of IID data
points {x1 , · · · , xN }. Introduce an index variable κ which is uniformly
distributed over {1, · · · , N }. The log-likelihood gradients w.r.t. θ as
shown in Eq. (2.19) can be recast in the expectation form of Eq. (2.31)
(i.e. as expectation of stochastic gradients), by letting λ ≜ θ, z ≜ (κ, x)T ,
pλ (z) ≜ N1 pθ (x), f (λ) ≜ ∇θ L(θ), and
Fλ (z) ≜ ∇θ Uθ (xκ ) − ∇θ Uθ (x)
Proof. This can be readily seen by rewriting Eq. (2.19) as:
∇θ L(θ) = Eκ∼Uni[1,N ],x∼pθ (x) [∇θ Uθ (xκ ) − ∇θ Uθ (x)]
and applying the independence between κ and x.

■

Combining Theorem 2.3 and general SA (Algorithm 4), the particular resulting pseudocode for learning EBMs is shown in Algorithm 6,
which is often known as stochastic maximum likelihood (SML) (Younes,
1989). Algorithm 6 corresponds to SA with single move. Further, by
applying SA with multiple moves (Algorithm 5), at each iteration, we
can draw a minibatch from training data (minibatching), say drawing κ1 , · · · , κB from {1, · · · , N }. At each iteration, we could directly
draw x(t,1) , · · · , x(t,M ) ∼ pθ(t−1) (x) when it is tractable, or run multiple
steps of a single chain, or multiple parallel chains, or a combination
of both, to draw multiple samples, say obtaining x(t,1) , · · · , x(t,M ) that
admit pθ(t−1) (x) as the invariant distribution. Then, at each iteration,


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

49

parameter updating in Eq. (2.32) can be replaced by:

B
1 X



M

1 X
θ(t) = θ(t−1) + γt
∇θ Uθ (xκj ) −
∇θ Uθ (x(t,j) )
B

M j=1
j=1

θ=θ(t−1)

Historical comments. The general stochastic approximation methodology was originally proposed in (Robbins and Monro, 1951). The
stochastic maximum likelihood method, proposed in (Younes, 1989),
turns out to be application of the more general SA methodology to
learning EBMs. The same idea was applied to training RBMs in (Tieleman, 2008), which is called persistent contrastive divergence (PCD)
to emphasize that the Markov chain is not reset between parameter
updates. The regular contrastive divergence (CD) method, proposed in
(Hinton, 2002), restarts the Markov chain at the training data rather
than at the previous state. This will not converge to MLE. As commented in (Salakhutdinov, 2009), “Clearly, the widely used practice
of CD1 learning is a rather poor “substitute” for maximum likelihood
learning.”

2.3.4

Variational methods with auxiliary models

As introduced before, Monte Carlo sampler is a crucial component
which affects maximum likelihood learning of EBMs. A recent progress
as studied in (Kim and Bengio, 2016; Wang and Ou, 2017; Kuleshov
and Ermon, 2017; Xie et al., 2018) is to pair the target EBM pθ (x) with
an auxiliary directed generative model (often called generator) qϕ (x)
parameterized by ϕ, which approximates sampling from the target EBM.
Learning is performed by maximizing the log-likelihood of training data
under pθ or some bound of the log-likelihood, and simultaneously minimizing some divergence between the target EBM pθ and the auxiliary


50

Basics for EBMs

generator qϕ 14 :
(

Maximize over θ the log-likelihood itself or some bound
Minimize over ϕ some divergence between pθ and qϕ .

Different learning methods mainly differ in the objective functions used
in the joint training of pθ and qϕ , and thus have different computational
and statistical properties. There are also other factors that distinguish
different studies in learning EBMs with auxiliary models, e.g. modeling
discrete or continuous data, different model choices of the target EBM
and the auxiliary generator.
Many methods in learning EBMs with auxiliary models are related to
variational methods. Variational methods provide an optimizationbased principle to inference and learning (Jordan et al., 1999; Frey
and Jojic, 2005). A classic application of variational methods is in
Bayesian inference, called variational inference (VI). VI posits a family
of approximating distributions q and then finds the member of that
family that is closest to the target posterior distribution p, mostly by
minimizing the exclusive-divergence KL(q||p). Variational methods have
also been widely used in the context of maximum likelihood parameter
estimation, which is often called variational learning. In particular, (Neal
and Hinton, 1998) shows a link between variational bound and maximum
likelihood parameter estimation via the Expectation-Maximization (EM)
algorithm. Variational learning in early days is called variational EM
(Frey and Jojic, 2005).
Remarkably, classic variational methods mostly optimize the exclusive divergence, and hence could be classified as the exclusive-variational
approach. Recently, there have emerged some variational methods that
optimize the inclusive divergence KL(p||q), which has good statistical
properties that makes it more appropriate for certain inference and
learning problems. These studies include joint stochastic approximation (JSA) (Xu and Ou, 2016; Ou and Song, 2020), Markovian score
climbing (MSC) (Naesseth et al., 2020), parallel Markov chain score
14
Such optimization using two objectives has also been employed in training other
types of models apart from learning EBMs, such as learning GAN with logD trick
(Goodfellow et al., 2014), the wake-sleep algorithm (Hinton et al., 1995) for learning
Helmholtz Machines.


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

51

ascent (pMCSA) (Kim et al., 2022), and transport score climbing (TSC)
(Zhang et al., 2022b) for learning latent-variable models (belonging to
directed graphical models), AugSA plus JSA (Wang and Ou, 2017) and
inclusive-NRF (Song and Ou, 2018) for learning EBMs (belonging to
undirected graphical models). We could refer these studies collectively as
the inclusive-variational approach. See (Ou, 2018) for more introduction
on variational methods and the two approaches.
In practice, the performance of learning EBMs with auxiliary models
often performs better than without auxiliary models. In the following,
we first follow (Song and Ou, 2018) to give a short literature review of
this class of studies, and then mainly detail the inclusive-variational
approach for learning EBMs.
Related work in MLE of EBMs with auxiliary models
Let pθ (x) denote the target EBM as defined in Eq. (2.13), qϕ (x) the
auxiliary generator which allows efficient sampling and approximates
sampling from the target EBM, and pemp (x) the empirical distribution
for training data.
• It is shown in (Song and Ou, 2018) that we have the following
evidence upper bound (EUBO) w.r.t. θ for EBMs:
EUBO(x; θ, ϕ) = log pθ (x) + KL(qϕ (x)||pθ (x))
= Uθ (x) − log Z(θ) − (Eqϕ (x) [pθ (x)] + H[qϕ (x)])
= Uθ (x) − (Eqϕ (x) [Uθ (x)] + H[qϕ (x)])
≥ log pθ (x)
It is further shown in (Song and Ou, 2018) that learning in (Kim
and Bengio, 2016) amounts to maximizing the EUBO bound w.r.t.
θ, while simultaneously minimizing the gap, i.e., the exclusivedivergence KL[qϕ ||pθ ] w.r.t. ϕ:


 max Ex∼pemp (x) EUBO(x; θ, ϕ)
θ


 min KL [qϕ (x)||pθ (x)]
ϕ

(2.33)


52

Basics for EBMs
Remarkably, the EUBO bound involves the intractable entropy
term H [qϕ ] and tends to enforce the generator to seek modes,
yielding missing modes. In Eq. (2.33), we optimize the exclusivedivergence w.r.t. an auxiliary distribution to approximate a target
distribution pθ (x). Hence we classify Eq. (2.33) as the exclusivevariational approach, which is called exclusive-NRF in (Song and
Ou, 2018).
• Learning in (Wang and Ou, 2017; Song and Ou, 2018) minimizes
the inclusive-divergence KL[pθ ||qϕ ] w.r.t. ϕ, which could be classified as the inclusive-variational approach for learning EBMs.
The main idea is to perform maximum likelihood learning of pθ
and simultaneously minimize the inclusive-divergence between the
target EBM pθ and the auxiliary generator qϕ by


 min KL [pemp (x)||pθ (x)]
θ


 min KL [pθ (x)||qϕ (x)]

(2.34)

ϕ

The first line of Eq. (2.34) is equivalent to maximum likelihood
fitting of the target EBM pθ under the empirical distribution pemp ,
which requires sampling from pθ . Simultaneously, the second line
optimizes the generator qϕ to be close to pθ so that qϕ becomes a
good proposal for sampling from pθ .
Compared to the exclusive-variational approach, the inclusivevariational approach shown in Eq. (2.34) has several advantages.
First, minimizing inclusive-divergence avoids the annoying entropy
term, which is suffered by minimizing the exclusive-divergence. Second, inclusive-divergence minimization tends to drive the auxiliary
generator, acting like an adaptive proposal in adaptive MCMC
(Andrieu and Thoms, 2008; Roberts and Rosenthal, 2009), to
cover modes of the target density pθ . Mode-covering is a desirable
property for proposal design in MCMC. In contrast, minimizing
exclusive-divergence leads to variational approximations that seek
modes and underestimate uncertainty. The auxiliary model qϕ (x)
and the sampler for pθ (x) can be very flexibly designed, depending
on the nature of data x, discrete or continuous.


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

53

– (Wang and Ou, 2017) mainly studies neural random field language models, using LSTM generators (autoregressive with
no latent variables) and employing Metropolis independence
sampler (MIS) - applicable for discrete data (natural sentences). The learning algorithm proposed in (Wang and Ou,
2017), called AugSA plus JSA, is an instance of the inclusivevariational approach for learning EBMs over discrete data.
– (Song and Ou, 2018) mainly designs neural random field
models (NRFs) for continuous data (e.g., images), choosing
latent-variable generators and developing SGLD (stochastic
gradient Langevin dynamics)/SGHMC (stochastic gradient
Hamiltonian Monte Carlo) samplers to exploit noisy gradients
in the continuous space. The learning algorithm proposed in
(Song and Ou, 2018), called inclusive-NRF , is an instance
of the inclusive-variational approach for learning EBMs over
continuous data.
• In (Xie et al., 2018) (CoopNet), motivated by interweaving maximum likelihood training of the EBM pθ (x) and the latent-variable
generator qϕ (h, x), a joint training method is introduced to train
EBMs. There are clear differences that distinguish the inclusivevariational approach. First, CoopNet uses LD (Langevin dynamics)
sampling to generate samples, but two LD sampling steps are
∂
intuitively interleaved according to ∂x
log pθ (x) (with Lx steps)
∂
and ∂h log qϕ (h, x) (with Lh steps) separately, not aiming to draw
samples from pθ (x)qϕ (h|x). This is different from the stochastic
gradient sampler in the augmented space in inclusive-NRF, which
moves (x, h) jointly. Second, according to theoretical understanding in (Xie et al., 2018), Coopnet considers the following joint
optimization problem:


 min {KL [pemp (x)||pθ (x)] − KL [r(h, x)||pθ (x)]}
θ


 min KL [r(h, x)||qϕ (h, x)]
ϕ

where r(h, x) denotes the distribution of (x(Lx ) , h(Lh ) ), resulting
from the CoopNet sampler. This objective is also clearly differ-


54

Basics for EBMs

Figure 2.10: Overview of the inclusive-variational approach for learning EBMs
for continuous data. Two neural networks are used to define the EBM’s potential
function Uθ (x) and the auxiliary generator gϕ (h) respectively. The parameters of
both networks, θ and ϕ, are updated by using the revised samples (x, h) in the
augmented space, which are obtained by revising the samples (x′ , h′ ) proposed by
the auxiliary generator, according to the stochastic gradients defined by both the
target EBM and the auxiliary generator. (Song and Ou, 2018)

ent from inclusive-NRF, which aims to minimize the inclusivedivergence KL[pθ ||qϕ ] w.r.t. ϕ. It is found in (Song and Ou, 2018)
that inclusive-NRF with SGLD outperforms CoopNet in image
generation.
• Learning in (Kuleshov and Ermon, 2017) minimizes the χ2 -divergence
R (pθ −qϕ )2
χ2 [qϕ ||pθ ] ≜
w.r.t. ϕ, which also tends to drive the genqϕ
erator to cover modes. But this approach is severely limited by
the high variance of the gradient estimator w.r.t. ϕ, and is only
tested on the simpler MNIST and Omniglot.
• Learning in (Han et al., 2019) further extends CoopNet and
introduces an inference model, apart from the target EBM and
the latent-variable generator, and jointly optimizes the three
models under a divergence triangle.
The inclusive-variational approach for learning EBMs
The basic idea of using the inclusive-variational approach for learning
EBMs is described in Eq. (2.34). The auxiliary model qϕ (x) and the


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

55

sampler for pθ (x) can be very flexibly designed, depending on the
nature of data x, discrete or continuous. In the following, we mainly
introduce the inclusive-variational approach for learning EBMs for
continuous data, which is called inclusive-NRF in (Song and Ou, 2018)
and illustrated in Figure 2.10. The important features of inclusiveNRF is that the auxiliary model qϕ (x) is a latent-variable model, and
stochastic gradient guided samplers (SGLG/SGHMC) are developed,
which is particularly useful for sampling from a continuous distribution
pθ (x). For the inclusive-variational approach for learning EBMs for
discrete data, readers could see (Wang and Ou, 2017).
The NRF model. EBMs parameterized by neural network, as defined
in Eq. (2.13), are called Neural random fields (NRFs) in (Song and Ou,
2018), which are usually denoted by pθ (x). The potential Uθ (x) : Rdx →
R is realized by a neural network, which takes the multi-dimensional
x ∈ Rdx as input and outputting the scalar uθ (x) ∈ R.
An inclusive-divergence minimized auxiliary generator qϕ (x) is introduced to approximate sampling from the target EBM, particularly
for fixed-dimensional continuous observations x ∈ Rdx (e.g. images).
We use a directed generative model, qϕ (x, h) ≜ q(h)qϕ (x|h), for the
auxiliary generator, which is defined as follows15 :
h ∼ N (0, Ih ),
x = gϕ (h) + ϵ, ϵ ∼ N (0, σ 2 Iϵ ).

(2.35)

Here gϕ (h) : Rdh → Rdx is implemented as a neural network with
parameter ϕ, which maps the latent code h to the observation space.
Ih and Iϵ denote the identity matrices, with dimensionality implied by
h and ϵ respectively. Drawing samples from the generator qϕ (x, h) is
simple as it is just ancestral sampling (Murphy, 2012) from a 2-variable
directed graphical model.
By using Fisher equality (Appendix B.2), we have the following
gradients for θ and ϕ respectively, where in the first equation, we use x̃
15

Note that during training, σ 2 is absorbed into the learning rates and does not
need to be estimated.


56

Basics for EBMs

Algorithm 7 The inclusive-NRF algorithm for learning EBMs for
continuous data with latent-variable auxiliary models
repeat

Sampling: Draw a minibatch M = (x̃i , xi , hi ), i = 1, · · · |M|
from pemp (x̃)pθ (x)qϕ (h|x) (see Algorithm 8);
Updating:
1 P
Update θ by ascending: |M|
(x̃,x,h)∼M [∇θ uθ (x̃) − ∇θ uθ (x)];
1 P
Update ϕ by ascending: |M| (x̃,x,h)∼M ∇ϕ log qϕ (x, h);
until convergence
and x to differentiate samples from the empirical distribution pemp (x)
and those from the model distribution pθ (x).
Proposition 1. The gradients for optimizing the two objectives in Eq.
(2.34) can be derived as follows:

∂


KL [pemp (x)||pθ (x)] = Epemp (x̃) [∇θ uθ (x̃)] − Ep (x) [∇θ uθ (x)]
−
θ
∂θ
∂


−
KL [pθ (x)||qϕ (x)] = Epθ (x)qϕ (h|x) [∇ϕ logqϕ (x, h)]
∂ϕ

(2.36)

By Proposition 1, we can obtain the gradients w.r.t. θ and ϕ (to be
ascended). In practice, we apply minibatch based stochastic gradient
descent (SGD) to solve the optimization problem Eq. (2.34), as shown
in Algorithm 7.
Ideally, the learning of θ could be conducted without ϕ, by using an
MCMC sampler (e.g. LD) to draw samples from pθ (x). But the chain
often mixes between modes so inefficiently that severely slow down the
learning of θ especially when the target density pθ (x) is multimodal.
This is the main difficulty that hinders the effective training of NRFs.
Introducing auxiliary generator qϕ to approximate the target NRF pθ
is inspired by and related to two advanced MCMC ideas - auxiliary
variable MCMC (Neal, 2011) and adaptive MCMC (Andrieu and Thoms,
2008; Roberts and Rosenthal, 2009).
• The classic example of adaptive MCMC is adaptive scaling of the
variance of the step-size in random-walk Metropolis (Roberts and


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

57

Rosenthal, 2009). In inclusive-NRF, the auxiliary generator acts
like an adaptive proposal, updated by using samples from the
target density16 .
• Further to be detailed next, the target density is extended to
be pθ (x)qϕ (h|x), which leaves the original target as the marginal,
but sampling in the augmented space (x, h) can be easier (more
efficiently), with the help of the adaptive proposal qϕ (x, h). This
follows the basic idea of auxiliary variable MCMC (Neal, 2011) sampling in an augmented space could be more efficient.
Developing stochastic gradient samplers for EBMs for continuous
data. In Algorithm 7, we need to draw samples (x, h) ∈ Rdx +dh in the
augmented space defined by the target joint distribution pθ (x)qϕ (h|x)
given current θ and ϕ. Gradient guided samplers (Section 2.3.1), such as
Langevin dynamics (LD) and Hamiltonian Monte Carlo (HMC) (Neal,
2011), are known to be efficient in exploring the continuous state space.
The gradients of the target distribution can be derived as follows:

∂
∂


log [pθ (x)qϕ (h|x)] =
[log pθ (x) + log qϕ (h, x) − log qϕ (x)]


∂x

∂x


∂
∂


log [pθ (x)qϕ (h|x)] =
log qϕ (h, x)

∂h

∂h

(2.37)
It can be seen that it is straightforward to obtain the gradient w.r.t. h
and the first two terms in the gradient w.r.t. x. However, calculating the
∂
third term ∂x
log qϕ (x) in the gradient w.r.t. x is intractable. Therefore
we are interested in developing stochastic gradient variants of those
∂
samplers, which rely on using noisy estimate of ∂x
log qϕ (x).
By considering z ≜ (x, h), p(z; λ) ≜ pθ (x)qϕ (h|x), λ ≜ (θ, ϕ)T , and
Eq. (2.37), we can use Theorem 2.1 to develop the sampling step for
Algorithm 7, as presented in Algorithm 8. For the gradient w.r.t. x, the
∂
intractable term ∂x
log qϕ (x) is estimated by a stochastic gradient.

16

Minimizing the inclusive-divergence tends to drive the generator (the proposal)
to have higher entropy than the target density, which is a desirable property for
proposal design in MCMC.


58

Basics for EBMs

Proposition 2. Given qϕ (h, x), we have
∂
∂
log qϕ (x) = Eh∗ ∼qϕ (h∗ |x)
log qϕ (h∗ , x) .
∂x
∂x




(2.38)

Proof. By using Fisher equality (Appendix B.2).

■

Motivated by Proposition 2, ideally we draw h∗ ∼ qϕ (h∗ |x) and then
∂
∂
use ∂x
log qϕ (h∗ , x) as an unbiased estimator of ∂x
log qϕ (x). In practice,
(l−1)
(l−1)
at step l, given x
and starting from h
, we run one step of LD
sampling over h targeting qϕ (h|x(l−1) ), to obtain h(l−1)∗ and calculate
∂
log qϕ (h(l−1)∗ , x(l−1) ). This gives a biased but tractable estimator
∂x(l−1)
∂
to ∂x
log qϕ (x). It is empirically found in experiments in (Song and Ou,
2018) that more steps of this inner LD sampling do not significantly
improve the performance for NRF learning.
∂
So instead of using the exact gradient ∂z
log p(z; λ) as shown in
Eq. (2.37), Song and Ou, 2018 developed a tractable biased stochastic
gradient ∆(z; λ) as follows:
∆(z; λ) ≜

∂
∗
∂x [log pθ (x) + log qϕ (h, x) − log qϕ (h , x)]
∂
∂h log qϕ (h, x)

!

,

(2.39)

where h∗ is an approximate sample from qϕ (h∗ |x) obtained by running
one step of LD from (h, x). Remarkably, as shown in Algorithm 8, the
starting point (h(0) , x(0) ) for the SGLD/SGHMC recursions is obtained
from an ancestral sampling from qϕ (h, x). Thus at step l = 1, h(0) is
already a sample from qϕ (h|x(0) ) given x(0) , and we can directly use h(0)
as h(0)∗ without running the inner LD sampling. Afterwards, for l > 1,
the conditional distribution of h(l−1) given x(l−1) is close to qϕ (h|x(l−1) ),
though strictly not. One or more steps of LD could be run to obtain
h(l−1)∗ to reduce the bias in the stochastic gradient estimator.
With the above stochastic gradients in Eq. (2.39), the sampling
step in Algorithm 7 can be performed by running |M| parallel chains,
each chain being executed by running finite steps of SGLD/SGHMC
with tractable gradients w.r.t. both x and h, as shown in Algorithm 8.
Intuitively, the auxiliary generator first gives a proposal (x′ , h′ ), and
then the system follows the gradients of pθ (x) and qϕ (h, x) (w.r.t. x
and h respectively) to revise (x′ , h′ ) to (x, h). The gradient terms pull


2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD

59

samples moving to low energy region of the random field and adjust the
latent code of the generator, while the noise term brings randomness. In
this manner, we obtain Markov chain samples in the augmented space
defined by pθ (x)qϕ (h|x).

Algorithm 8 Sampling in the augmented space defined by pθ (x)qϕ (h|x)
1. Conduct ancestral sampling from the auxiliary generator qϕ (x, h),
i.e. first draw h′ ∼ q(h′ ), and then draw x′ ∼ qϕ (x′ |h′ );
2. Starting from (x′ , h′ ) = z (0) , run finite steps of SGLD (l = 1, · · · , L)
to obtain (x, h) = z (L) , which we call sample revision, according to
Eq. (2.24).
In particular, the SGLD recursions are conducted as follows:

h
∂

x(l) = x(l−1) + δl
log pθ (x(l−1) ) + log qϕ (h(l−1) , x(l−1) )


(l−1)

∂x

i p




− log qϕ (h(l−1)∗ , x(l−1) ) + 2δl ηx(l) ,
p

∂

(l)

h(l) = h(l−1) + δl (l−1) log qϕ (h(l−1) , x(l−1) ) + 2δl ηh ,



∂h




(l)
(l) (l) T

η

≜ (ηx , ηh ) ∼ N (0, I)

(2.40)
where, for l > 1, h(l−1)∗ , which is an approximate sample from
qϕ (h|x(l−1) ) given x(l−1) , is obtained from running one step of LD as
follows, starting from h(l−1) :
h(l−1)∗ = h(l−1) + δl∗

∂

log qϕ (h(l−1) , x(l−1) ) +
∂h(l−1)
(l)∗
ηh ∼ N (0, I);

q

(l)∗

2δl∗ ηh ,
(2.41)

for l = 1, we directly use h(0) as h(0)∗ , since, by initialization, h(0) is
an exact sample from qϕ (h|x(0) ) given x(0) .
Return (x, h), i.e. z (L) .


60
2.3.5

Basics for EBMs
Non-MLE methods for learning EBMs

Maximum likelihood estimation (MLE) has been the most widely used
objective for learning probabilistic models. When training an EBM
with MLE, we need to sample from the EBM per training iteration.
The training efficiency of the MLE method highly depends on the
mixing efficiency of the Markov chain. In high-dimensional problems,
it is very challenging to design Markov chains with fast mixing rates.
Thus, MLE training of EBMs may converge slowly. As alternatives
to MLE training, non-MLE methods for learning EBMs (as a kind of
unnormalized models) have been explored, such as noise-contrastive
estimation (Gutmann and Hyvärinen, 2010; Gutmann and Hyvärinen,
2012) and score matching (Hyvärinen and Dayan, 2005).
Another motivation to pursue non-MLE methods is that the optimization criterion used has profound effect on the behavior of the
optimized model (Theis et al., 2016). Maximizing likelihood is equivalent
to minimizing the KL divergence between pora (x) and pθ (x), because
KL[pora (x)||pθ (x)] = −Ex∼pora (x) [log pθ (x)] + Ex∼pora (x) [log pora (x)]
= −Ex∼pora (x) [log pθ (x)] + constant
≈ −Ex∼pemp (x) [log pθ (x)] + constant

(2.42)

= −L(θ) + constant
where pora (x) denote the the underlying (oracle) data distribution, and
L(θ) is the log-likelihood in Eq. (2.17). Eq. (2.42) is an unbiased Monte
Carlo integration since the expectation under pora (x) is approximated
by empirical samples {xi }N
i=1 .
It is known that the KL divergence is asymmetric, and optimizing
which direction of the KL divergence leads to different trade-offs. The KL
approximation covers the data distribution while reverse-KL has more
of a mode-seeking behavior (Minka, 2005). The basic score matching
(SM) objective minimizes a discrepancy between two distributions called
the Fisher divergence:


1
2
DF (pora (x)||pθ (x)) = Ex∼pora (x) ||∇x log pora (x) − ∇x log pθ (x)||
2
Learning under most criteria is provably consistent given infinite
model capacity and data. Most of the methods are firstly examined over


2.4. LEARNING EBMS BY NOISE-CONTRASTIVE ESTIMATION
(NCE)

61

continuous data, with few on discrete data. The score matching method
is based on minimizing the expected squared distance of the score
function 17 of the data distribution and the score function given by the
model, and thus is not applicable to training EBMs over discrete data
such as natural languages. In contrast, the noise-contrastive estimation
method has no such limitation, which will be detailed below.
2.4

Learning EBMs by noise-contrastive estimation (NCE)

Noise-contrastive estimation (NCE) is proposed in (Gutmann and
Hyvärinen, 2010; Gutmann and Hyvärinen, 2012), as a typical nonMLE method, for learning unnormalized statistical models. Its basic
idea is “learning by comparison”, i.e. to perform nonlinear logistic regression to discriminate between data samples drawn from the data
distribution pora (x) and noise samples drawn from a known noise distribution q(x). An advantage of NCE is that the normalizing constant
can be treated as a normal parameter and updated together with the
model parameters.
Denote the target unnormalized model by:
pθ (x) =

1
p̃θ (x)
Zθ

(2.43)

where we highlight that the model, parameterized by θ, is unnormalized,
R
and the normalizing constant Zθ = p̃θ (x)dx. To apply NCE, we
treat log Zθ as an additional parameter ζ and rewrite Eq. (2.43) in the
following form:
pθ,ζ (x) = exp [−ζ + log p̃θ (x)]
(2.44)
As shown below, model parameters (θ, ζ) will be jointly estimated in
NCE.
Introduce a fixed, known noise distribution denoted by q(x), and
consider a binary classification. There are two classes of samples, with
prior probabilities P (C = +) and P (C = −). For class +, the sample
is drawn from the data distribution pora ; for class −, the sample is
17

The gradient of log-density with respect to the data vector x is called the score
function.


62

Basics for EBMs

drawn from the noise distribution q. This defines a generation process
of samples in NCE.
Given a sample x from such a generation process, the class posterior
probabilities are defined as follows:
p(C = +|x) =

p(C = +)p(x|C = +)
p(C = +)p(x|C = +) + p(C = −)p(x|C = −)

p(C = −|x) = 1 − p(C = +|x)
By our design of the binary classification experiment, p(x|C = −) is
the noise distribution q(x). The (unknown) class-conditional density for
class + is assumed to be modeled by the target model pθ,ζ (x). Let the
ratio between the prior probabilities p(C=−)
p(C=+) be ν (i.e., the ratio of noise
sample size to real sample size). Then the posterior probabilities can
be parameterized as follows:
p(C = +|x; θ, ζ) =

pθ,ζ (x)
pθ,ζ (x) + νq(x)

(2.45)

p(C = −|x; θ, ζ) = 1 − p(C = +|x; θ, ζ)
NCE estimates the model parameters (θ, ζ) by optimizing the twoclass classification through maximizing the following conditional loglikelihood:
JNCE (θ, ζ) = Ex∼pora (x) [log p(C = +|x; θ, ζ)]+νEx∼q(x) [log p(C = −|x; θ, ζ)]
(2.46)
The objective function J(θ, ζ) is a sum of two expectations. The first
is the expectation w.r.t. the data distribution pora (x), which can be
approximated by randomly drawing samples from training data, namely
approximating pora (x) by pemp (x). The second is the expectation w.r.t.
the noise distribution q(x), which can be approximated by drawing
samples from the noise distribution.
Setting to zeros the gradients of J(θ, ζ) w.r.t. (θ, ζ), we can apply the
SA algorithm to find its root and thus solve the optimization problem
in Eq. (2.46). The pseudocode of NCE is shown in Algorithm 9. The
relevant gradients can be further simplified as follows:
∇θ,ζ log p(C = +|x; θ, ζ) = p(C = −|x; θ, ζ)∇θ,ζ log pθ,ζ (x)
∇θ,ζ log p(C = −|x; θ, ζ) = −p(C = +|x; θ, ζ)∇θ,ζ log pθ,ζ (x)

(2.47)


2.4. LEARNING EBMS BY NOISE-CONTRASTIVE ESTIMATION
(NCE)

63

Algorithm 9 NCE for fitting an unnormalized model
repeat
Sampling: Draw an empirical minibatch D ∼ pora (x) and a noise
minibatch B ∼ q(x), satisfying ν = |B|/|D|;
Updating: Update (θ, ζ) by ascending:
ν X
1 X
∇θ,ζ log p(C = +|x; θ, ζ)+
∇θ,ζ log p(C = −|x; θ, ζ)
|D| x∼D
|B| x∼B
until convergence
It is shown in (Gutmann and Hyvärinen, 2012) that under the ideal
situation of infinite amount of data and infinite model capacity, we have
the following theorem (nonparametric estimation). It is further shown
(Gutmann and Hyvärinen, 2012) that the NCE estimator is consistent.
Theorem 2.4 (Nonparametric estimation). JNCE (θ, ζ) attains its maximum at pθ,ζ (x) = pora (x). There are no other extrema if the noise density
q(x) is chosen such that it is nonzero whenever pora (x) is nonzero.
The noise distribution q and the ratio ν have an influence on the
accuracy of the NCE estimate of model parameters (θ, ζ). A natural
question to ask in applying NCE is what, from a statistical standpoint,
the best choice of q and ν is, to get estimates with a small estimation
error. This question is discussed in the original paper of NCE (Gutmann
and Hyvärinen, 2012), which give the following suggestions:
1. Choose noise for which an analytical expression for log q is available.
2. Choose noise that can be sampled easily.
3. Choose noise that is in some aspect, for example with respect to
its covariance structure, similar to the data.
4. Make the noise sample size as large as computationally possible.


64
2.4.1

Basics for EBMs
Dynamic noise-contrastive estimation (DNCE)

Remarkably, there exist two problems in applying NCE learning. First,
reliable NCE needs a large ν, especially when the noise distribution is
not close to the data distribution. And the time and memory cost for
gradient calculation are almost linearly increased with ν. Second, the
expectation w.r.t. the data distribution pora in Eq. (2.46) is approximated by the expectation w.r.t. the empirical distribution pemp (namely
the training data), which is rather sparse for high-dimensionality data
modeling. The model estimated by NCE is thus easily overfitted to the
empirical distribution. Dynamic noise-contrastive estimation (DNCE)
was proposed in (Wang and Ou, 2018a) to address the above problems,
with two modifications.
First, instead of using a fixed noise distribution, a dynamic noise
distribution qϕ (x) with parameter ϕ is introduced in DNCE. In addition to maximizing w.r.t. (θ, ζ) the NCE objective function JNCE (θ, ζ),
DNCE simultaneously performs maximum likelihood optimization of ϕ
over training data:
max Ex∼pora (x) [log qϕ (x)]
ϕ

The motivation is to push the noise distribution to be close to the data
distribution, so that we can achieve reliable model estimation even
using a small ν. Theoretically, one can optimize the noise distribution
beforehand and then use a fixed noise density in NCE. It is found that
dynamic noise distribution helps optimization, by gradually increasing
the difficulty of the two-class discrimination task (Wang and Ou, 2018a).
If the noise distribution qϕ is too different from the data distribution
pora , the two-class discrimination problem might be too easy and would
not require the system to learn much about the structure of the data.
But if qϕ is too close to pori from the beginning, the discrimination
problem might be too difficult to proceed.
Second, instead of using the standard NCE objective function
JNCE (θ, ζ) in Eq. (2.46), a modified objective function is proposed


2.4. LEARNING EBMS BY NOISE-CONTRASTIVE ESTIMATION
(NCE)

65

as follows18 :
JDNCE (θ, ζ) = Ex∼pint (x) [log p(C = +|x; θ, ζ)]+νEx∼qϕ (x) [log p(C = −|x; θ, ζ)]
(2.48)
where
pint (x) = αpora (x) + (1 − α)qϕ (x)
denotes an interpolation of the data distribution and the noise distribution, and 0 < α < 1 is the interpolating factor. p(C = +|x; θ, ζ) and
p(C = −|x; θ, ζ) are defined the same as in Eq. (2.45), except that the
fixed noise distribution q(x) is replaced by the dynamic noise distribution qϕ (x). Intuitively, as the noise distribution qϕ converges to the data
distribution, using the interpolated distribution pint will increase the
number of data-like samples by adding samples drawn from the noise
distribution. This could avoid the model to be overfitted to the sparse
training set.
Putting the two modifications together, DNCE conducts the following joint optimization,


JDNCE (θ, ζ)
 max
θ,ζ

 max Ex∼pora (x) [log qϕ (x)]
ϕ

which can be solved by applying minibatch-based stochastic gradient
ascent.
At each iteration, a set of data samples, denoted by D, is sampled
from pora , with the number of samples in D denoted as |D|. Additionally,
two sets of noise samples are drawn from the noise distribution qϕ ,
ν
denoted by B1 and B2 , whose sizes satisfy |B1 | = 1−α
α |D| and |B2 | = α |D|
respectively. As a result, the union of D and B1 can be viewed as samples
drawn from the interpolated distribution pint , with |D∪B1 | = |D|
α . Model
parameters (θ, ζ) are updated by ascending the following stochastic
gradients:
α X
p(C = −|x; θ, ζ)∇θ,ζ log pθ,ζ (x)
|D| x∈D∪B
1
α X
p(C = +|x; θ, ζ)∇θ,ζ log pθ,ζ (x)
−
|D| x∈B
2

18

In JDNCE (θ, ζ) and pint (x), we suppress their dependency on ϕ, since, as will be
shown later, the optimization of JDNCE (θ, ζ) is taken only over (θ, ζ) while fixing ϕ.


66

Basics for EBMs

Noise model parameter ϕ are updated by ascending the following stochastic gradient:
1 X
∇ϕ log qϕ (x)
|D| x∈D
A final remark is that the theoretical consistency of DNCE learning
in the nonparametric limit can be shown by the following theorem.
Theorem 2.5. Suppose that an arbitrarily large number of data samples
can be drawn from pora , and the model distribution pθ (x) and the noise
distribution qϕ (x) have infinite capacity. Then we have
(i) The KL divergence KL(pora ||qϕ ) can be minimized to attain zero;
(ii) If KL(pori ||qϕ ) attains zero at ϕ∗ , and the conditional log-likelihood
Eq. (2.48) attains a maximum at (θ∗ , ζ ∗ ), then we have
pθ∗ (x) = qϕ∗ (x) = pora (x)
Proof. (i) This conclusion can be easily seen by consistencey of MLE,
since minimizing KL(pora ||qϕ ) is equivalent to MLE of qϕ .
(ii) From KL(pori ||qϕ∗ ) = 0, we have qϕ∗ = pora .
By Theorem 2.4, with fixed ϕ∗ , Eq. (2.48) has the only extremum
at pθ∗ (x) = pint (x)|ϕ=ϕ∗ = αpora (x) + (1 − α)qϕ∗ (x).
The conclusion is clear from combining the above two equations. ■
2.5

Generation from EBMs

Given an EBM, an important inference task is sampling from the model,
i.e., drawing or generating samples from the model. Sampling is not
only a critical step in maximum likelihood learning of EBMs (as we
introduce in Section 2.3), but also itself forms as an important class
of applications. Generating text, images, speech, or other media has
received increasing interests, and recently has been collectively referred
to as generative AI19 .
Transformer-based (Vaswani et al., 2017) autoregressive language
models (ALMs), generating text sequentially from left to right, have
19

https://en.wikipedia.org/wiki/Generative_artificial_intelligence


2.5. GENERATION FROM EBMS

67

been the dominant approach for text generation (Radford et al., 2018).
Key to their success is local normalization, i.e. they are defined in
terms of a product of conditional distributions, one for each token in
the sequence. These models can be trained efficiently via maximum
likelihood teacher-forcing, and sampling from ALMs is straightforward
by ancestral sampling . Unfortunately, local normalization also brings
some drawbacks for these locally-normalized sequence models, when
compared to globally-normalized sequence models (namely EBM based
sequence models)20 . As will be detailed in Section 4.1.2 and Section
4.4.1, the drawbacks include:
• Discrepancy between training and inference (related to exposure
bias);
• Limitation in long-range coherency due to only left-to-right modeling and decoding (related to label bias);
• Inflexibility in controlled generation (e.g., satisfying hard lexical
constraints and/or soft topical constraints in text generation).
There are similar concerns for generating other sequence media (e.g.,
speech). There have been studies for speech synthesis by EBMs (Sun
et al., 2023) and also by some non-autoregressive models, such as
FastSpeech 2 (Ren et al., 2020) and diffusion models (Popov et al.,
2021).
Remarkably, given a learned EBM, some applications need likelihood evaluation (e.g., in language modeling for speech recognition),
while other tasks require generation from the learned generative model
(e.g., in many NLP tasks such as summarization, dialog, and machine
translation). Generation from generative models basically is sampling
from them. In practice, generating from locally-normalized sequence
models, sometimes also referred to as decoding, can be readily realized
by greedy decoding (beam search) or nucleus sampling (Holtzman et al.,
20

There are some studies, which do not involve EBM modeling, but use a masked
language modeling objective to train the model. This approach, referred to as
non-autoregressive generation, performs iterative decoding, i.e., generating nonautoregressively, then masking out and regenerating, and so cycles for a number of
iterations (Ghazvininejad et al., 2019).


68

Basics for EBMs

Table 2.1: A survey of different sampling methods used in generating text from
EBMs. The target model is the EBM, while a proposal is required for both MCMC
and IS. Different proposals are used in different applications. Shorthands: ALMs (autoregressive language model), MLM (masked language model), SNIS (self-normalized
importance sampling), ASR (automatic speech recognition), CTG (controlled text
generation), CTGAP (conditional text generation after prefix).

Sampling method

Proposal

MH within
Gibbs sampling

Conditional
of word class
ALM
MLM

SNIS

Langevin dynamics

ALM

-

Application
ASR (Wang et al., 2015; Wang
et al., 2018)
ASR (Wang and Ou, 2017)
CTG (Miao et al., 2019; Goyal
et al., 2022; Mireshghallah et
al., 2022) (see Section 4.4.2)
CTG (Parshakova et al., 2019;
Khalifa et al., 2021); CTGAP
(Deng et al., 2020) (see Section
4.4.1)
CTG (Qin et al., 2022)

2019) as engineering variants of ancestral sampling. There is no easy
methods for sampling from EBMs. Basically we have to resort to Monte
Carlo methods, such as MCMC and importance sampling (IS), which is
usually not as computational efficient as ancestral sampling. Perhaps
this is the most challenging practical limitation of EBMs for their applications in generation, and the dominant approach to text generation
is still based on large neural auto-regressive models.
Theoretically, sampling from EBMs can be performed by the MCMC
and importance sampling methods, which are introduced in Section 2.3.1
and Section 2.3.2 respectively. Gradient-based MCMC methods (Section
2.3.1), such as Langevin dynamics, are good choices for sampling for
continuous data (e.g., images), by using the gradient of the potential
∇x Uθ (x). However, since text is discrete, the gradient is not well-defined,
making it non-trivial to apply gradient-based MCMC methods for
sampling text from EBMs.


2.5. GENERATION FROM EBMS

69

Various MCMC and IS methods have been explored in applications
of EBMs for generating text. In Table 2.1, we survey some recent
studies, which provide concrete examples. Remarkably, both MCMC
and IS methods need proposal distributions, and the design of proposal
distributions heavily depends on specific applications. We comment on
the particular proposal distributions used in different applications in
Table 2.1. some further discussions are as follows.
MH within Gibbs sampling. Suppose we use Gibbs sampling to generate a sequence of n tokens, x = (x1 , · · · , xn ), from an EBM distribution
p(x1 , · · · , xn ). Exact Gibbs sampling needs to calculate the conditional
distribution p(xi |x\i ) of token xi for each position i, given all the other
tokens x\i ≜ (x1 , · · · , xi−1 , xi+1 , · · · , xn ). For modern EBMs developed
for text, such as in (Wang et al., 2015; Wang et al., 2018; Wang and Ou,
2017) and so on, this is computational expensive, because calculating
p(xi |x\i ) needs to enumerate all possible values of xi from V and to
compute the joint probability p(xi , x\i ) for each possible value, where V
denotes the vocabulary. Metropolis-Hastings (MH) within Gibbs sampling has been explored, with a proposal, to draw MCMC samples from
p(xi |x\i ).
In (Wang et al., 2015; Wang et al., 2018), word classing is introduced
to accelerate sampling, which means that each word is assigned to a
single class. Through applying MH within Gibbs sampling, we first
sample the class by using a reduced model as the proposal, which
includes only the features that depend on xi through its class21 , and
then sample the word. This reduces the computational cost from |V| to
|C| + |V|/|C| on average, where |C| denotes the number of classes.
The computation reduction in using word classing in EBMs with
neural features (i.e., parameterized by neural networks) is not as significant as in EBMs with discrete features, because EBMs parameterized
by neural networks involve a much larger context, which makes the
sampling computation with the reduced model still expensive. In later
work in (Wang and Ou, 2017), an ALM (autoregressive language model)
21

This is possible, since features used in (Wang et al., 2015; Wang et al., 2018)
are discrete features (n-gram features). (See introduction in Section 3.3.1)


70

Basics for EBMs

is introduced to propose for p(xi |x\i ), and in (Miao et al., 2019; Goyal
et al., 2022; Mireshghallah et al., 2022), a MLM (masked language models) is used as the proposal. The proposal model can be jointly trained
with the EBM, as in (Wang and Ou, 2017), or pre-trained language
models can be directly used for the proposal (Miao et al., 2019; Goyal
et al., 2022; Mireshghallah et al., 2022).
Self-normalized importance sampling (SNIS). The basics are introduced in Section 2.3.2. See Section 4.4.1 for details in applications.
Langevin dynamics. The basics are introduced in Section 2.3.1. See
Section 4.4.2 for details in applications.


3
EBMs for sequential data with applications in
language modeling

In this chapter, we are mainly concerned with learning the (marginal)
distribution of observation x itself by EBMs. Considering the sequential
nature of speech and language, we show how to develop EBMs for
sequential data, or more generally, for trans-dimensional data.
EBMs are mostly developed in fixed-dimensional settings, for example, in the modeling of fixed-size images. Trans-dimensional setting
means that the observations can be of different dimensions. A familiar
case is temporal modeling of sequential data, where each observation is
a sequence of a random length. Language modeling falls exactly in this
trans-dimensional setting, where an observation sequence x is a natural
language sentence (i.e., a token sequence).
3.1

Autoregressive language model (ALM)

Language modeling involves determining the joint probability p(x) of a
sentence x, which can be denoted as a pair x = (l, xl ), where l is the
length and xl = (x1 , . . . , xl ) is a sequence of l tokens. Currently, the
dominant approach to language modeling is the locally-normalized or
conditional modeling, which decomposes the joint probability of xl into
71


72

EBMs for sequential data with applications in language modeling

a product of conditional probabilities by using the chain rule,
p(x1 , . . . , xl ) =

l
Y

p(xi |x1 , . . . , xi−1 ).

(3.1)

i=1

Language models (LMs) in the form of Eq. (3.1) is known as autoregressive language models (ALMs). Remarkably, for an ALM to make
the sum of the probabilities of all sequences equal to 1, it is necessary
to place a special token ⟨EOS⟩ at the end of sentences and to include
this in the product of Eq. (3.1) (Chen and Goodman, 1999). Otherwise,
the sum of the probabilities of all sequences of a given length is 1, and
the sum of the probabilities of all sequences is then infinite.
In early days before the deep learning era, the history of xi , denoted
as (x1 , · · · , xi−1 ), is often reduced to equivalence classes through a
mapping ϕ(x1 , · · · , xi−1 ) with the assumption
p(xi |x1 , · · · , xi−1 ) ≈ p(xi |ϕ(x1 , · · · , xi−1 )).
A classic example is the traditional n-gram language models (LMs) with
ϕ(x1 , · · · , xi−1 ) = (xi−n+1 , . . . , xi−1 ),
assuming that current token xi depends on history only through the
previous n−1 tokens, i.e., the (n−1)-order Markov assumption. Various
smoothing techniques have been used for parameter estimation, and
particularly, the modified Kneser-Ney (KN) smoothed n-gram LMs are
still widely used because of its simplicity and good performance (Chen
and Goodman, 1999).
Recently, neural network LMs have begun to surpass the traditional
n-gram LMs, and also follow the locally-normalized approach. The
mapping ϕ(x1 , · · · , xi−1 ) condenses the history into a hidden vector
hi ∈ RD through a neural network (NN), which can be a feedforward NN
(Schwenk, 2007), a recurrent NN (Mikolov et al., 2011; Sundermeyer
et al., 2012), or more recently, a Transformer NN (Vaswani et al.,
2017). Specifically, in either manners, the neural network calculates the
conditional probability at each position as follows:
exp(zk )
p(xi = k|x1 , · · · , xi−1 ) = P|V|
j=1 exp(zj )

(3.2)


3.2. ENERGY-BASED LANGUAGE MODEL (ELM)

73

which is in the form of a multi-class logistic regression, as introduced in
Eq. (2.3). The logits zk = wkT hi + bk , k = 1, · · · , K are calculated from
a linear layer on top of hidden vector hi . wk ∈ RD , bk ∈ R denote the
weight vector and bias of the linear layer, respectively. V denotes the
vocabulary of all possible tokens.
Drawbacks of ALMs. Both the classic n-gram LMs and the recent
neural network LMs are autoregressive language models, which are
locally normalized. Unfortunately, local normalization in ALMs brings
some drawbacks. As will be detailed in Section 4.1.2, ALMs are prone
to exposure bias (Wiseman and Rush, 2016; Ranzato et al., 2016) and
label bias (Lafferty et al., 2001; Andor et al., 2016).
3.2

Energy-based language model (ELM)

3.2.1

Globally-normalized ELM (GN-ELM)

Energy-based language models (ELMs) parameterize an unnormalized
distribution for natural sentences and are radically different from autoregressive language models. Let x be a natural sentence (i.e., a token
sequence). An energy-based language model (ELM) is defined as follows
pθ (x) =

exp(Uθ (x))
Z(θ)

(3.3)

where Uθ (x) denotes the potential function with parameter θ, Z(θ) =
P
x exp(Uθ (x)) is the normalizing constant, and pθ (x) is the probability
of sentence x. For reasons to be clear below (mainly to be differentiated
from TRF-LM), the model in Eq. (3.3) is called globally-normalized
ELM (GN-ELM).
ELMs potentially address the drawbacks of ALMs introduced above,
as they do not require any local normalization. Early attempts on
building energy-based language models are GN-ELMs and date back
to (Rosenfeld et al., 2001), which proposes whole-sentence maximum
entropy (WSME) language models1 . Specifically, a WSME model has
1

Due to the connection between log-linear model and maxent model as we
introduced before in Section 2.1.2, this model is called WSME.


74

EBMs for sequential data with applications in language modeling

the log-linear form
p(x; λ) =

1 λT f (x)
e
.
Z(λ)

(3.4)

Here f (x) is a vector of features, which are computable functions of x
such as n-grams conventionally used, λ is the corresponding parameter
P
T
vector, and Z(λ) = x eλ f (x) is the global normalizing constant.
There has been little work on WSME-LMs, mainly in (Rosenfeld et
al., 2001; Amaya and Benedi, 2001; Ruokolainen et al., 2010). Although
the whole-sentence approach has the potential advantage of being able
to flexibly integrate a richer set of features, the empirical results of
previous WSME-LMs are not satisfactory, almost the same as traditional
n-gram LMs. After incorporating lexical and syntactic information, a
mere relative improvement of 1% and 0.4% respectively in perplexity
and in WER (word error rate) was reported for the resulting WSME-LM
(Rosenfeld et al., 2001). Subsequent studies of using WSME LMs with
grammatical features, as in (Amaya and Benedi, 2001) and (Ruokolainen
et al., 2010), reported perplexity improvement above 10% but no WER
improvement when using WSME LMs alone.
In recent years, there are encouraging progresses in both theories and
applications of ELMs. A new class of ELMs, called trans-dimensional
random fields (TRFs), have been developed, which are different from
GN-ELMs and present the first strong empirical evidence supporting the
power of using energy-based approach to language modeling (Wang et al.,
2015; Wang et al., 2018). Applications of ELMs have covered computation of sentence likelihoods (up to a constant) for speech recognition
(Wang et al., 2015; Wang et al., 2018; Wang and Ou, 2017; Wang and
Ou, 2018b; Wang and Ou, 2018a; Gao et al., 2020) (to be introduced in
the next section), text generation (Deng et al., 2020) (to be covered in
Section 4.4.1), language model pre-training (Clark et al., 2020b) (to be
covered in Section 3.4), calibrated natural language understanding (He
et al., 2021) (to be covered in Section 5.5), and so on.
3.2.2

Trans-dimensional random field (TRF) LMs

To describe trans-dimensional observations in general, a new energybased probabilistic model, called the trans-dimensional random field


3.2. ENERGY-BASED LANGUAGE MODEL (ELM)

75

(TRF) model, has been proposed in (Wang et al., 2015; Wang et al.,
2018), which explicitly mixes a collection of random fields in sample
spaces of different dimensions. A GN-ELM is globally-normalized over
all sequences of all lengths. In contrast, a TRF-LM is a collection
of random fields, each normalized on subspaces of different lengths
separately and weighted by empirical length probabilities.
Suppose that it is of interest to build random field models for
multiple sets of observations of different dimensions, such as images
of different sizes or sentences of different lengths. Denote by xj an
observation in a sample space X j of dimension j, ranging from 1 to
j
m. The space of all observations is then the union X = ∪m
j=1 X . To
emphasize the dimensionality, each observation in X can be represented
as a pair x = (j, xj ), even though j is identified as the dimension of xj .
By abuse of notation, write f (x) = f (xj ) for features of x.
For j = 1, . . . , m, assume that the observations xj are distributed
from a random field in the form
1
j
pj (xj ; λ) =
eUθ (x ) ,
Zj (λ)
Uθ (xj ) : X j → R denotes the potential function which assigns a scalar
value to each configuration of x in X and can be very flexibly parameterized through linear functions or nonlinear functions with neural
networks of different architectures, to be explained in Section 3.3.1. θ
denotes the set of parameters, and Zj (θ) is the normalizing constant:
Zj (θ) =

X

j

eUθ (x ) ,

j = 1, . . . , m.

xj

Moreover, assume that dimension j is associated with a probability πj
P
j
for j = 1, . . . , m with m
j=1 πj = 1. Therefore, the pair (j, x ) is jointly
distributed as
πj Uθ (xj )
e
,
(3.5)
p(j, xj ; π, θ) = πj pj (xj ; θ) =
Zj (θ)
where π = (π1 , . . . , πm )T .
Here we actually define a mixture of random fields for joint modeling
sentences of different dimensions (namely lengths). There is a random
field for each length. By maximum likelihood, the mixture weights can
be estimated to be the empirical length probabilities.


76

EBMs for sequential data with applications in language modeling
Table 3.1: The development of TRF-LMs.

Work

Contribution

Wang et al., 2015;
Wang et al., 2018

• Discrete features
• Augmented stochastic approximation (AugSA)
for model training
• Potential function as a deep CNN
• Model training by AugSA plus JSA (joint
stochastic approximation)
• Potential function in the form of exponential
tilting, revisited in residual EBMs (Deng et al.,
2020)
• Use LSTM on top of CNN
• NCE is introduced to train TRF-LMs
• Simplify the potential definition by using only
bidirectional LSTM
• Propose Dynamic NCE for improved model
training
• Mixed-feature TRFs, by integrating discrete
and neural features
• Pre-trained language models (PLMs) are used
as the backbones of energy functions, noise distributions in NCE and proposal distributions in
Monte Carlo

Wang and Ou,
2017
Wang and Ou,
2018b

Wang and Ou,
2018a

Gao et al., 2020
Liu and Ou, 2023

As outlined in Table 3.1, there are a series of works in the development of TRF-LMs, each with its own contribution in different
parameterizations of potential functions and model training methods.
Before expanding introductions around Table 3.1, let us first recognize
the differences between GN-ELM and TRF-LM.


3.2. ENERGY-BASED LANGUAGE MODEL (ELM)
3.2.3

77

Comparison between GN-ELM and TRF-LM

The following comment on the connection and difference between GNELM and TRF-LM models is adapted from the comparison between
WSME and TRF models in (Wang et al., 2018). Suppose that we add
the dimension features in the GN-ELM model Eq. (3.3) and obtain
p(j, xj ; λ, ν) =

1
T
j
eν δ(j)+Uθ (x ) ,
Z(θ, ν)

(3.6)

where δ(j) = (δ1 (j), · · · , δm (j))T denotes the dimension features such
that δl (j) = 1(j = l). ν = (ν1 , . . . , νm )T denotes the corresponding
parameter vector, and Z(θ, ν) is the global normalizing constant
Z(θ, ν) =

m X
X
j=1 xj ∈X j

T

j

eν δ(j)+Uθ (x ) =

m
X

eνj Zj (θ).

(3.7)

j=1

Similar to Proposition 1 in (Wang et al., 2018), it can be seen that
when both fitted by maximum likelihood estimation, model Eq. (3.6)
is equivalent to model Eq. (3.5) but with different parameterization.
The parameters in model Eq. (3.6) are (θ, ν), whereas the parameters
in model Eq. (3.5) are (π, θ). Therefore, an important distinction of
TRF-LM from GN-ELM lies in the use of dimension features, which has
significance consequences in both model definition and model learning.
First, it is clear that model Eq. (3.5) is a mixture of random fields on
subspaces of different dimensions, with mixture weights explicitly as free
parameters. Hence model Eq. (3.5) will be called a trans-dimensional
random field (TRF). Moreover, by maximum likelihood, the mixture
weights can be estimated to be the empirical dimension probabilities.
Second, it is instructive to point out that model Eq. (3.3) is essentially also a mixture of random fields, but the mixture weights implied
are fixed to be proportional to the normalizing constants Zj (θ):
p(j, xj ; θ) =

Zj (θ)
1
j
·
eUθ (x ) ,
Z(λ) Zj (θ)

(3.8)

where Z(θ) = x eUθ (x) = m
j=1 Zj (θ). Typically the unknown mixture
weights in Eq. (3.8) may differ from the empirical length probabilities
and also from each other by orders of magnitudes, e.g. 1040 or more in
P

P


78

EBMs for sequential data with applications in language modeling

the experiments of (Wang et al., 2018). As a result, it is very difficult
to efficiently sample from model Eq. (3.3), in addition to the fact that
the length probabilities are poorly fitted for model Eq. (3.3). Setting
mixture weights to the known, empirical length probabilities enables
us to develop an effective learning algorithm, as introduced in (Wang
et al., 2018). Basically, the empirical weights serve as a control device to
improve sampling from multiple distributions (Liang et al., 2007; Tan,
2017).
3.3

ELMs for speech recognition

As an important application, ELMs have been successfully used as a
means for calculating sentence scores in automatic speech recognition
(ASR). The design of energy function and the optimization of parameters
are central research questions in applying ELMs, which are introduced
in the following two subsections respectively.
3.3.1

Architectures of energy functions

One is generally free to choose the energy function in ELMs, as long as
it assigns a scalar energy to every sentence, no matter for TRF-LMs
or GN-ELMs. A subtle difference between defining the energy function
−Uθ (x) in Eq. (3.3) for GN-ELMs and defining −Uθ (xj ) in Eq. (3.5)
for TRF-LMs is whether to include the special token ⟨EOS⟩ or not. We
do not need to include ⟨EOS⟩ at the end of xj in calculating Uθ (xj ) for
TRF-LMs. For GN-ELMs, we often include ⟨EOS⟩ at the end of x in
calculating Uθ (x), hoping to help modeling of sentence lengths. Except
this difference, the architectures of energy functions for TRF-LMs can
be readily used for GN-ELMs, and vise versa.
Broadly speaking, there are three types of energy functions.
• Early ELMs are log-linear models using discrete features, including
Rosenfeld et al., 2001 (WSME-LM) and Wang et al., 2015; Wang
et al., 2018 (TRF-LM). These ELMs could thus be referred to as
discrete ELMs.
• Later, based on CNN and LSTM networks, ELMs using neural network based energy functions (neural ELMs) have been developed


3.3. ELMS FOR SPEECH RECOGNITION

79

(Wang and Ou, 2017; Wang and Ou, 2018b; Wang and Ou, 2018a),
outperforming neural ALMs with similar model sizes. ELMs using
neural network based energy functions could be regarded as using
neural features, by following the discussion in Section 2.1.2.
• By integrating discrete and neural features, mixed-feature TRFs
have also been proposed (Gao et al., 2020), demonstrating the
advantage of energy-based models in integrating discrete and
neural features.
Recently, based on Transformer networks (Vaswani et al., 2017) and
large pre-trained lanugage models (PLMs) such as BERT (Devlin et al.,
2018) and GPT2 (Radford et al., 2019), neural ELMs have been further
advanced (Liu and Ou, 2023). Extensive experiments are conducted on
two datasets, AISHELL-1 (Bu et al., 2017) and WenetSpeech (Zhang
et al., 2022a). The results show that the best ELM achieves competitive
results with the finetuned GPT2 and performs significantly better than
the finetuned BERT. Further analysis show that the ELM obtains better
confidence estimate performance than the finetuned GPT2.
In the following, we summarize the architectures used to define Uθ (x)
in the literature, in roughly chronological order. We first introduce the
classic log-linear energy function using discrete features. Then, a suite of
nonlinear energy functions are shown, which are called Hidden2Scalar,
SumInnerProduct, SumTargetLogit, SumMaskedLogit and SumTokenLogit,
respectively.
Notably, although one energy architecture is proposed in one context
of either TRF-LMs or GN-ELMs, it can be applied in both TRF-LMs
and GN-ELMs. Let x = {xi }i=1...|x| , where xi ∈ {1, · · · , V } is the i-th
token in x. |x| denotes the length of sentence x in tokens. V denotes
the size of token vocabulary.
Linear energy using discrete features
Linear energy functions using discrete features basically corresponds to
log-linear models, as illustrated in Example 2.1, i.e., defining
Uλ (x) = λT f (x)

(3.9)


80

EBMs for sequential data with applications in language modeling

Figure 3.1: Example of discrete features.

Here we follow the notations in (Wang et al., 2018), where f (x) =
(f1 (x), f2 (x), . . . , fd (x))T is a feature vector, λ = (λ1 , λ2 , . . . , λd )T is the
corresponding parameter vector (instead of using θ).
A feature fi (x), i = 1, . . . , d, can be any computable function of
the input x. For various applications such as language modeling, the
parameters of local potential functions across locations are often tied
together (Pietra et al., 1997). Each feature fi (x) is often defined in
P
the form fi (x) = k fi (x, k), where fi (x, k) is a binary function of x
evaluated at position k. In a trigram example, fi (x, k) equals to 1 if three
specific words appear at positions k to k + 2 and k ≤ j − 2. The binary
features fi (x, k) share the same parameter λi for different positions
k and dimensions j, so called position-independent and dimensionindependent. Hence, the feature fi (x) indicates the count of nonzero
fi (x, k) over k in the observation xj and takes values as non-negative
integers. Intuitively, fi (x) returns the count of a specific phrase (often
called a n-gram feature) observed in the input sentence x, as shown in
Figure 3.1.
The energy-based language modeling approach allows a very flexible
use of features, not limited to ordinary n-gram features. In (Wang et al.,
2018), a variety of features as shown in Table 3.2 are used, mainly
based on word and class information. Each word is deterministically
assigned to a single class, by running the automatic clustering algorithm
proposed in (Martin et al., 1998) on the training dataset. In Table
3.2, wi , ci , i = 0, −1, . . . , −5, denote the word and its class at different
position offset i, e.g., w0 , c0 denotes the current word and its class. The
word/class n-gram features “w”/“c”, skipping n-gram features “ws”/“cs”
(Goodman, 2001), higher-order skipping features “wsh”/“csh”, and the


3.3. ELMS FOR SPEECH RECOGNITION

81

Table 3.2: Feature definition in TRF LMs (Wang et al., 2018)

Type

Features

w
c
ws
cs
wsh
csh
cpw
tied

(w−3 w−2 w−1 w0 )(w−2 w−1 w0 )(w−1 w0 )(w0 )
(c−3 c−2 c−1 c0 )(c−2 c−1 c0 )(c−1 c0 )(c0 )
(w−3 w0 )(w−3 w−2 w0 )(w−3 w−1 w0 )(w−2 w0 )
(c−3 c0 )(c−3 c−2 c0 )(c−3 c−1 c0 )(c−2 c0 )
(w−4 w0 ) (w−5 w0 )
(c−4 c0 ) (c−5 c0 )
(c−3 c−2 c−1 w0 ) (c−2 c−1 w0 )(c−1 w0 )
(c−9:−6 , c0 ) (w−9:−6 , w0 )

crossing features “cpw” (meaning class-predict-word) are introduced in
(Wang et al., 2015). In (Wang et al., 2018), the tied long-skip-bigram
features “tied” (Shazeer et al., 2015) are further introduced, in which
the skip-bigrams with skipping distances from 6 to 9 share the same
parameter. In this way we can leverage long distance contexts without
increasing the model size. All the features f (x) in model Eq. (3.9) are
constructed from Table 3.2 in a position-independent and dimensionindependent manner, and only the features observed in the training
data are used. It is shown in (Wang et al., 2018) that the TRF-LM using
features “w+c+ws+cs+wsh+csh+tied” outperforms the KN 5-gram
LM significantly with 10% relative error reduction.
Non-linear energy: Hidden2Scalar
Generally speaking, like in (Wang and Ou, 2017; Wang and Ou, 2018b;
Deng et al., 2020; He et al., 2021), we can use a text encoder to encode x
and denote the encoder output (hidden vectors) by encθ (x). At position
i, we have encθ (x)[i]. Then, the potential function can be defined as


|x|
X
Uθ (x) = Linear  encθ (x)[i]

(3.10)

i=1

where Linear(·) denotes a trainable linear layer whose output is a scalar.
This energy function is obtained by transforming neural hidden vectors
into a scalar, hence it is named by Hidden2Scalar.


82

EBMs for sequential data with applications in language modeling

Figure 3.2: Hidden2Scalar: a deep CNN architecture used to define the potential
function Uθ (x). Shadow areas denote the padded zeros. (Wang and Ou, 2017)


3.3. ELMS FOR SPEECH RECOGNITION

83

Figure 3.3: Hidden2Scalar: a bidirectional LSTM on top of CNN used to define the
potential function Uθ (x). (Wang and Ou, 2018b)

ℎ𝑓,1

ℎ𝑓,2

ℎ𝑓,3

ℎ𝑓,4

ℎ𝑓,5

Forward LSTM

𝑒1

𝑒2

𝑒3

𝑒4

𝑒5

Word embedding

ℎ𝑏,1

ℎ𝑏,2

ℎ𝑏,3

ℎ𝑏,4

ℎ𝑏,5

Backward LSTM

Figure 3.4: SumInnerProduct: a bidirectional LSTM used to define the potential
function Uθ (x). (Wang and Ou, 2018a)


84

EBMs for sequential data with applications in language modeling

The text encoder can be based on a fully CNN architecture (Wang
and Ou, 2017) as shown in Figure 3.2, or a bidirectional LSTM (BLSTM)
stacked on top of CNN as shown in Figure 3.3. Recently, BERT based
text encoders have been used in (Deng et al., 2020; He et al., 2021; Liu
and Ou, 2023). In (Deng et al., 2020), in the final layer of RoBERTa (Liu
et al., 2019), the mean-pooled hidden states are projected to a scalar
energy value. In (He et al., 2021), three variants of energy functions
(scalar, hidden, and sharp-hidden) are defined on top of a RoBERTa
based text encoder, which will be detailed in Section 5.5.
Non-linear energy: SumInnerProduct
In (Wang and Ou, 2018a), a bidirectional LSTM based potential function
is defined as follows, illustrated in Figure ??. First, each word xi
(i = 1, . . . , |x|) in a sentence is mapped to an embedded vector ei ∈ Rd .
Then the word embedding vectors are fed into a bidirectional LSTM
to extract the long-range sequential features from the forward and
backward contexts. Denote by hf,i , hb,i ∈ Rd the hidden vectors of the
forward and backward LSTMs respectively at position i. Finally, we
calculate the inner product of the hidden vector of the forward LSTM
at current position and the embedding vector at the next position, and
calculate the inner product of the hidden vector of the backward LSTM
at current position and the embedding vector at the pervious position
(dash line in Figure ??). The potential function ϕ(xl ; θ) is computed by
summing all the inner products, hence named by SumInnerProduct,
|x|−1

Uθ (x) =

X

hTf,i ei+1 +

i=1

|x|
X

hTb,i ei−1

(3.11)

i=2

where θ denotes all the parameters in the neural network. The SumInnerProduct
energy provides a theoretical-solid framework to incorporate the bidirectional LSTM features.
Non-linear energy: SumTargetLogit
The SumInnerProduct energy uses a bidirectional network. In order
to exploit pre-trained language models, which mostly are ALMs and


3.3. ELMS FOR SPEECH RECOGNITION

85

unidirectional, we could consider energy definition tailored to ALMs
(Liu and Ou, 2023). Given history x1:i−1 , let the output logits to predict
the next token be denoted by zθ (x1:i−1 ), whose dimension is equal to V .
The k-th logit is denoted by zθ (x1:i−1 )[k]. Then, the potential is defined
as
Uθ (x) =

|x|
X

zθ (x1:i−1 )[xi ]

(3.12)

i=1

This potential function sums the logits corresponding to the target token
(next token) at each position, hence it is named by SumTargetLogit. In
contrast, the ALM applies local normalization (softmax) to the logits
zθ (x1:i−1 ) to obtain the conditional probability of xi given history x1:i−1 .
Non-linear energy: SumMaskedLogit
For masked language model (MLM), e.g., BERT, pseudo-log-likelihood
(PLL) is introduced for scoring sentences (Wang and Cho, 2019). Inspired
by this, we can define the potential function as follows (Liu and Ou,
2023):
Uθ (x) =

|x|
X

gθ (mask(x, i))[i][xi ]

(3.13)

i=1

where gθ denotes the MLM, whose output, at each position, is the
logits before softmax. gθ (mask(x, i)) means masking the i-th token
in x and sending the masked sequence into the MLM for a forward
pass. At position i, the logit corresponding to the masked token xi is
denoted as gθ (mask(x, i))[i][xi ]. In Eq. (3.13), the potential is defined by
summing the logits corresponding to masked tokens, hence it is named
by SumMaskedLogit. Notably, this architecture is much time-consuming
than others, since it requires |x| forward passes to calculate the energy
of one sentence, therefore this architecture is primarily for stimulating
ideas rather than conducting experiments.
Non-linear energy: SumTokenLogit
To overcome the deficiency of SumMaskedLogit, a simplication is proposed (Liu and Ou, 2023), i.e., omitting the masking step and feeding x


86

EBMs for sequential data with applications in language modeling

directly to the MLM, so that the logits at all positions can be calculated
in parallel. The potential is defined as:
Uθ (x) =

|x|
X

gθ (x)[i][xi ]

(3.14)

i=1

Comparison of non-linear energy architectures
For comparing the different non-linear energy architectures, experiments in (Liu and Ou, 2023) find that the bi-directional architectures
(Hidden2Scalar and SumTokenLogit) based on BERT are generally
better than the unidirectional architecture (SumTargetLogit).
3.3.2

Integrating discrete and neural features

There has been a long recognition that discrete features (n-gram features) and neural network based features have complementary strengths
for language models (LMs). Generally, LMs with neural features (e.g.
neural ALMs, neural TRF-LMs) outperform LMs with discrete features
(e.g. KN n-gram LMs, discrete TRF-LMs), but interpolation between
them usually gives further improvement. This suggests that discrete
and neural features have complementary strengths. Presumably, the
n-gram features mainly capture local lower-order interactions between
words, while the neural features particularly defined by LSTMs can
learn higher-order interactions. Additionally, by embedding words into
continuous vector spaces, neural LMs are good at learning smoothed regularities, while discrete LMs may be better suited to handling symbolic
knowledges or idiosyncrasies in human language, as noted in (Ostendorf,
2016). Currently, model interpolation, either linear or log-linear (Chen
et al., 2019; Wang et al., 2016), is often a second step, after the discrete
and neural models are separately trained beforehand. The interpolation weights are ad-hoc fixed or estimated over held-out data (different
from the training data in the first step). This two-step integration is
sub-optimal.
The ELM approach can provide a unified and simplified approach
in integrating both discrete and neural features, based on its capability
in flexibly integrating a rich set of features. Basically, with the ELM


3.3. ELMS FOR SPEECH RECOGNITION

87

10
Discrete TRF full
Neural TRF
Mixed TRF

9.5

WER

9
8.5
8
7.5
7

0

20

40

60

80

100

epoch
Figure 3.5: The WER curves of the three TRF-LMs during the first 100 training
epochs are plotted. (Gao et al., 2020)

approach, one is free to define the potential function in any sensible
way with much flexibility. It is straightforward to define a mixed-feature
TRF-LM (Gao et al., 2020), in which the potential function is a sum of
a linear potential using discrete features and a nonlinear potential using
neural features. Mixed-feature TRF-LMs can be trained by applying
the dynamic noise-contrastive estimation (DNCE) method (Wang and
Ou, 2018a), as introduced in Section 2.4.1.
Mixed-feature TRF-LMs represent the first single LM model that
incorporates both discrete and neural features without relying on a
second-step interpolation. Apart from naturally integrating discrete
and neural features, another bonus from using mixed-feature TRF-LMs
is that faster training convergence and shorter training time can be
achieved, using only 58% training epochs when compared to training
neural TRF-LMs alone (see Figure 3.5). Notably, the log-likelihood of the
training data with respect to (w.r.t.) the parameters of discrete features
is concave. This helps to reduce the non-convexity of the optimization
problem for maximum likelihood training. Also, after incorporating
the linear potential, the nonlinear potential only needs to capture the
residual interactions between tokens. This may also explain the faster
training convergence of mixed-feature TRF models.


88

EBMs for sequential data with applications in language modeling

In (Gao et al., 2020), various LMs are trained over the Wall Street
Journal (WSJ) portion of Penn Treebank (PTB) (Marcus et al., 1993)
and Google one-billion-word corpus2 , and evaluated in N-best list rescoring experiments for speech recognition. Among all single LMs (i.e. without model interpolation), the mixed-feature TRF-LMs perform the best,
improving over both discrete TRF-LMs and neural TRF-LMs alone, and
also being significantly better than LSTM ALMs. Compared to interpolating two separately trained models with discrete and neural features
respectively, the performance of mixed-feature TRF-LMs matches the
best interpolated model, and with simplified one-step training process
and reduced training time.
3.3.3

Residual ELMs

As mentioned previously in this section, there are three types of energy
functions based on discrete, neural and mixed features, respectively.
Orthogonal to this3 , there is another architecture for EBMs, i.e., in the
form of exponential tilting of a reference distribution. In the fields of
lanugage modeling, this model formulation dates back to Rosenfeld et al.,
2001, Wang and Ou, 2018b, and recently Deng et al., 2020. Specifically,
the probability of a sentence x is defined as follows:
pθ (x) =

1
q(x)eUθ (x)
Z(θ)

(3.15)

where a reference distribution q(x) is introduced as the baseline distribution; Uθ (x) denotes the residual potential function with parameter
P
θ; Z(θ) = x q(x) exp(Uθ (x)) is the normalizing constant. Hence, Eq.
(3.15) is called a residual ELM , after Deng et al., 2020.
The role of residual energy −Uθ (x) is to fit the difference between
the data distribution and the reference distribution. If q(x) is a good
approximation of the data distribution, such as the LSTM based ALMs
used in (Wang and Ou, 2018b), the Transformer based ALMs in (Deng
et al., 2020), fitting the difference between the data distribution and the
reference distribution q(x) shall be much simpler than fitting the data
2

https://github.com/ciprian-chelba/1-billion-word-language-modeling-benchmark
It means that the energy architectures described previously can be applied to
Uθ (x) in residual ELMs.
3


3.4. ENERGY-BASED CLOZE MODELS FOR REPRESENTATION
LEARNING OVER TEXT
89
distribution directly. Moreover, when the reference distribution q(x) is
used as the proposal distribution in Monte Carlo methods for learning
a residual EBM, we have the importance weight in the following simple
form:
pθ (x)
∝ eUθ (x)
q(x)
Similarly, when the reference distribution q(x) is used as the noise
distribution in NCE methods for learning a residual EBM, we will have
a simple form of Eq. (2.45).
3.3.4

Training methods

Different types of ELMs, including GN-ELMs Eq. (3.3), TRF-LMs Eq.
(3.5) and residual ELMs Eq. (3.15), all obeys the general from of general
EBMs Eq. (2.13). Basically, we can use the methods introduced in
Section 2.3 (MLE) and Section 2.4 (NCE) for learning general EBMs
to train ELMs. A GN-ELM Eq. (3.3) has the same form as a EBM Eq.
(2.13), so those methods can be seamlessly applied. In TRF-LMs Eq.
(3.5), length probabilities are introduced and normalization is conducted
for each length separately, some special care is needed.
For more details about MLE for learning TRF-LMs, refer to (Wang
et al., 2018; Wang and Ou, 2017). For more details about DNCE for
learning TRF-LMs, refer to (Wang and Ou, 2018a; Gao et al., 2020).
3.4

Energy-based cloze models for representation learning over text

Motivation
In this section, we show the capability of EBMs for representation
learning over text (Clark et al., 2020b). The cloze task of predicting
the identity of a token given its surrounding context has proven highly
effective for representation learning over text. BERT (Devlin et al.,
2018), as a representative masked language model (MLM), implements
the cloze task by replacing input tokens with a special placeholder token
[MASK], but there are some drawbacks with the BERT approach:
• It suffers from the drawback in efficiency (only 15% of tokens are
masked out at a time);


90

EBMs for sequential data with applications in language modeling

Figure 3.6: Comparison of BERT and Electric. Both model the conditional probability of a token given its surrounding context. BERT produces normalized conditional
distribution for masked positions, while Electric calculates unnormalized conditional
probabilities for all input tokens. (Clark et al., 2020b)

• It introduces a pre-train/fine-tune mismatch where BERT sees
[MASK] tokens in training but not in fine-tuning;
• In principle, it does not produce log-likelihoods for sentences
(even up to an additive constant) and, in this sense, does not
define a language model. The pseudo-log-likelihood (PLL) has
been introduced for scoring sentences by BERT (Wang and Cho,
2019), but calculating the PLL for a sentence x requires |x| passes
of the transformer (once with each token masked out), and is thus
computationally expensive.
Some details are as follows. BERT and related masked language
models (Devlin et al., 2018; Liu et al., 2019) train a large neural network
to perform the cloze task. In contrast to the standard language modeling
task to learn the joint probability pora (x), these models try to learn the
conditional probabilities of masked tokens occurring in their surrounding
context. Specifically, multiple positions (e.g., 15%) in the input sentence
x = (x1 , · · · , xl ) are randomly drawn, denoted by R = {t1 , · · · , tk },
1 ≤ tj ≤ l, j = 1, · · · , k, and those positions are replaced by [MASK]
(i.e., masked). The masked sentence, denoted by mask(x, R), is encoded
into vector representations by a Transformer network (Vaswani et al.,
2017). Then the vector representation at position tj is passed into a
softmax layer to calculate a distribution over the vocabulary for position
tj , j = 1, · · · , k. The conditional probabilities are maximized for learning
the model parameters θ:
k
X
j=1

log pθ (xtj |mask(x, R))


3.4. ENERGY-BASED CLOZE MODELS FOR REPRESENTATION
LEARNING OVER TEXT
91
The Electric model
The new model proposed in (Clark et al., 2020b), called Electric, is
closely related to the Electra pre-training method (Clark et al., 2020a),
and implements the cloze task based on using conditional EBMs.
Specifically, Electric does not use masking or a softmax layer. Electric first maps the unmasked input x = (x1 , · · · , xl ) into contextualized
vector representations h(x) = (h1 , · · · , hl ) using a Transformer network. Then, the conditional probability of a token xt occurring in the
surrounding context x\t = (x1 , · · · , xt−1 , xt+1 , · · · , xl ) is modeled by a
conditional EBM :
1
pθ (xt |x\t ) =
exp(−wT ht ), 1 ≤ t ≤ l
(3.16)
Zθ (x\t )
where w is learnable weight vector, and ht is the vector representation at
position t. A comparison of BERT and Electric is illustrated in Figure
3.6.
Training of the Electric model
The conditional EBMs defined in Eq. (3.16) can be trained using NCE
(Section 2.4), and more specifically its conditional version (Section 4.1.3).
First, we define the un-normalized output
p̂θ (xt |x\t ) = exp(−wT ht )
Denote the training dataset by D. In NCE, a binary classifier is trained
to distinguish positive token xt vs negative token x̂t , with k negatives
and l positives (e.g., k = ⌈0.15l⌉) for a sentence x of length l. Denote
the random positions by R = {t1 , · · · , tk }, 1 ≤ tj ≤ l, j = 1, · · · , k.
Formally, referring to Eq. (2.46), the NCE loss L(θ) is as follows:
l · p̂θ (xt |x\t )
− log
l · p̂θ (xt |x\t ) + k · q(xt |x\t )

"

l · Ex∼D,t∼Uni(1,l)

k · q(x̂t |x\t )
− log
l · p̂θ (x̂t |x\t ) + k · q(x̂t |x\t )

"

+k · Ex∼D,t∼R,
x̂t ∼q(x̂t |x\t )

#

#

(3.17)

where q(xt |x\t ) denotes the noise distribution, which was realized by a
two-tower cloze model (Clark et al., 2020b). Specifically, the noise model


92

EBMs for sequential data with applications in language modeling

runs two transformers TLTR and TRTL over the input sequence. These
transformers apply causal masking so one processes the sequence leftto-right (LTR) and the other operates right-to-left (RTL). The model’s
predictions come from a softmax layer applied to the concatenated
states of the two transformers:
←
−
→
−
h = TLTR (x), h = TRTL (x)
→
−
←
−
q(xt |x\t ) = softmax(W [ h t−1 , h t+1 ])xt
The noise distribution is trained simultaneously with Electric using
standard maximum likelihood estimation over the data. Thus, the
training method used by Electric is in fact DNCE (Section 2.4.1).
Note that the NCE loss Eq. (3.17) does not introduce additional
parameters for normalizing constants Zθ (x\t ). In the setting of nonparametric estimation (Theorem 2.4), the NCE loss is minimized when
p̂θ (xt |x\t ) matches the oracle density. Thus, we assume that the model
is of sufficient capacity and the model can learn to be self-normalized
such that Zθ (x\t ) = 1.
A further examination of Eq. (3.17) reveals that its optimization
is computationally expensive to run. It requires k + 1 forward passes
through the Transformer to compute the p̂θ ’s, once for the positive
samples xt |x\t and once for every negative sample x̂t |x\t . So a modified
algorithm, allowing more efficient calculation, is to replace the k random
positions simultaneously by negative tokens x̂t ∼ q(xt |x\t ). The resulting
noised sequence is denoted by
xnoised = replace(x, R, (x̂t1 , · · · , x̂tk ))
To apply this efficiency trick, Electric assumes p̂θ (·|x\t ) = p̂θ (·|xnoised
),
\t
for “·” taking xt or x̂t , i.e., assuming that extra noise replacing does
not change the conditional distribution much, for both positive and
negative tokens. The modified loss for a sentence x of length l becomes


3.4. ENERGY-BASED CLOZE MODELS FOR REPRESENTATION
LEARNING OVER TEXT
93
as follows4 :
(l − k) · p̂θ (xt |xnoised
)
\t

"

l · Ex∼D,t∼Uni(1,l) − log
"

+k · Ex∼D,t∼R,
x̂t ∼q(x̂t |x\t )

#

(l − k) · p̂θ (xt |xnoised
) + k · q(xt |x\t )
\t

k · q(x̂t |x\t )
− log
(l − k) · p̂θ (x̂t |xnoised
) + k · q(x̂t |x\t )
\t

#

(3.18)

Calculating Eq. (3.18) requires just one pass through the Transformer
for k noise sample and l − k data samples. This brings significant
computation reduction.
Performance of the Electric model
Experiments in (Clark et al., 2020b) on GLUE natural language understanding benchmark (Wang et al., 2019) and SQuAD question answering
dataset (Rajpurkar et al., 2016) show that Electric substantially outperforms BERT but slightly under-performs ELECTRA. However, Electric
is particularly useful in its ability to efficiently produce pseudo-loglikelihood (PLL) scores for text, as defined below.
PLL(x) =

l
X
t=1

log p̂θ (xt |x\t ) =

l
X

−wT ht

(3.19)

t=1

Experiments on the 960-hour LibriSpeech corpus (Panayotov et al.,
2015) show Electric is better at re-ranking the outputs of a speech
recognition system than GPT2 (Radford et al., 2019) and is much faster
at re-ranking than BERT because it scores all input tokens simultaneously rather than having to be run multiple times with different tokens
masked out. It appears that EBMs are a promising alternative to the
standard ALMs and MLMs currently used for language representation
learning.

4

To compare with Eq. (3.17), this modified loss is written for l − k positives and
k negatives, while Eq. (3.17) is for l positives and k negatives.


4
Conditional EBMs with applications

In Chapter 3, we mainly introduce EBMs for modeling the marginal
distribution of natural language sentences, with applications to language
modeling for speech recognition and language representation learning.
In this chapter, we introduce EBMs for modeling conditional distributions, i.e., using EBMs for conditional models (Section 1.1.2). In
Section 4.1, we present basics for conditional EBMs, or equivalently,
conditional random fields (CRFs). In the following sections, we show
how they can be applied in not only discriminative tasks such as speech
recognition (Section 4.2) and natural language labeling (Section 4.3)
but also conditional text generation tasks (Section 4.4).
4.1

CRFs as conditional EBMs

As introduced in (Section 1.1.2), many real-world applications are solved
by conditional models. Conditional random fields (CRFs) (Lafferty et
al., 2001; Sutton, McCallum, et al., 2012) have been known to be
one of the most successful conditional models, especially for sequence
labeling. A CRF is basically a conditional distribution p(y|x) defined as
a random field, or equivalently, an undirected graphical model. A formal
definition is as follows. Similar to the equivalent meaning of random
94


4.1. CRFS AS CONDITIONAL EBMS

95

Figure 4.1: Graphical model representation of a conditional random field (CRF).

fields and EBMs in unconditional setting, conditional random fields and
conditional EBMs are exchangeable terms.
Definition 4.1 (Conditional random field). Consider probability distributions p(y|x), where x represents an observation corresponding to the
input variable, and y is the output variable that we wish to predict. A
variable can either be scalar- or vector-valued. Suppose y = y1 , · · · , yT .
In an undirected graph consisting of x and (y1 , · · · , yT ) (e.g., as shown
in Figure 4.1), let C denote the set of cliques in the subgraph induced
by y. Associated with each clique C ∈ C, let ϕC (yC , x) denote a (log)
potential function. An conditional random field (CRF) in terms of this
undirected graph consists of a family of distributions that factorize as:
p(y|x) =

1 Y
ϕC (yC , x)
Z(x) C∈C

(4.1)

where Z(x) is the normalizing constant given by
Z(x) =

X Y

ϕC (yC , x)

(4.2)

y C∈C

Eq. (4.1) actually means that for every assignment x, p(y|x) factorizes according to the subgraph induced by y. Since x is given, x is
treated as a constant, and included in the definition of potentials.
4.1.1

Linear-chain CRFs

Linear-chain CRFs are a class of CRFs, particularly useful for sequence
labeling in the area of natural language processing (NLP), such as part-ofspeech (POS) tagging (Collobert et al., 2011), named entity recognition


96

Conditional EBMs with applications

(NER) (Lample et al., 2016; Ma and Hovy, 2016), chunking (Søgaard
and Goldberg, 2016) and syntactic parsing (Durrett and Klein, 2015),
and also in other areas such as bioinformatics (Sato and Sakakibara,
2005; Peng et al., 2009). Formally, in sequence labeling, the input x is a
sequence with the same length of the output y. Thus, given a sequence
of observations x = (x1 , · · · xT ) = x1:T , the task of sequence labeling is
to predict a sequence of labels y = (y1 , · · · yT ) = y1:T , with one label
for one observation in each position. yi ∈ {1, · · · , K} denotes the label
at position i.
A linear-chain CRF defines a conditional distribution for label
sequence y given observation sequence x in the following form:
p(y|x) ∝ exp

( T
X

ϕt (yt , x) +

t=1

T
X

)

ψt (yt−1 , yt , x) .

(4.3)

t=1

Here the labels yt ’s are structured to form a chain, giving the term
linear-chain. Over the linear-chain, we define node potentials and edge
potentials.
• ϕt (yt , x) is often called the node potential at position t. Traditionally, people use discrete features, as introduced in Example 2.1 and
Section 3.3.1, to define node potentials. The feature functions (or
simply, the features) are usually hand-crafted indicator functions,
which we think are useful for labeling, as shown below.
f1 (yt , x) =δ(yt = prep, xt = on)
f2 (yt , x) =δ(yt = adv, xt ends in ly)
···
Then, the potential function at position t can be defined as follows:
ϕt (yt , x) =

X

λi fi (yt , x)

(4.4)

i

where i indexes the different features. Notably, the potential
functions ϕt (·, x) defined above at different positions take identical
forms, i.e., position-independent.
• A recent progress is the development of Neural CRFs (NCRFs),
which combines the sequence-level discriminative ability of CRFs


4.1. CRFS AS CONDITIONAL EBMS

97

and the representation ability of neural networks (NNs). In different studies, these models are called conditional neural field (Peng
et al., 2009), neural CRF (Artieres et al., 2010), recurrent CRF
(Mesnil et al., 2015), and LSTM-CRF (Lample et al., 2016; Ma
and Hovy, 2016). Though there are detailed differences between
these existing models, generally they are all defined by using NNs
(of different network architectures) to implement the non-linear
node potentials in CRFs, while still keeping the linear-chain hidden structure, i.e., using a bigram table as the edge potential (to
be detailed below). Suppose that the input sequence x1:T is transformed into vector representations h1:T = (h1 , · · · hT ) ∈ RT ×D
via an appropriate neural network. The vector representations
h1:T can be viewed as features, and the neural network is referred
as a feature extractor, as discussed previously in Section 2.1.1.
Then, the node potential can be calculated via a linear layer on
top of the vector representations:
ϕt (yt = k, x) = wkT ht + bk ≜ ϕkt , k = 1, · · · , K

(4.5)

where wk ∈ RD , bk ∈ R denote the weight vector and bias of the
linear layer, independent of t. ϕkt denotes the potential value at
positon t for label k.
• ψt (yt−1 , yt , x) is the edge potential defined on the edge connecting
yt−1 and yt . It is mostly implemented as a transition matrix A:
ψt (yt−1 = j, yt = k, x) = Aj,k
4.1.2

(4.6)

Label bias and exposure bias

It is known that locally-normalized sequence models are prone to label bias (Lafferty et al., 2001; Andor et al., 2016) and exposure bias
(Wiseman and Rush, 2016; Ranzato et al., 2016). Depending on locallynormalized or globally-normalized, unconditional or conditional, there
are four classes of models, as overviewed in Table 4.1.
• The label bias problem was raised initially in the conditional
setting (Lafferty et al., 2001). Later in this section, we will show
that the label bias problem also exist in the unconditional setting.


98

Conditional EBMs with applications

Table 4.1: A general classification of sequence models, with some common examples.

Locally-normalized
Globally-normalized

Unconditional

Conditional

ALM
ELM

seq2seq
CRF/Conditional EBM

• The exposure bias is related to the manner of model training,
rather than caused by the model itself; so it exist in both unconditional and conditional settings.
In summary, the problems of label bias and exposure bias exist in
both unconditional and conditional settings. An advantage of globallynormalized sequence models is that they avoid the problems of label
bias and exposure bias, as explained below.
The label bias problem
Conditional, locally-normalized sequence models such as maximum
entropy Markov models (MEMMs) (McCallum et al., 2000) and sequenceto-sequence models (seq2seq) (Sutskever et al., 2014) are potential
victims of the label bias problem. In general, a conditional, locallynormalized sequence model is described by the conditional probability
p(y|x) with the following decomposition, where x = (x1 , · · · xT ) = x1:T
is an input sequence and y = (y1 , · · · yL ) = y1:L is its corresponding
output sequence whose length L may differ from T .
p(y1:L |x1:T ) =

L
Y

p(yi |x1:T , y1 , · · · , yi−1 ).

(4.7)

i=1

The ouput probabilities at each time-step, p(yi |x1:T , y1 , · · · , yi−1 ), are
locally normalized, so successors of incorrect histories receive the same
mass as do the successors of the true history (Wiseman and Rush, 2016).
So locally normalized sequence models often have a very weak ability
to revise earlier decisions (Andor et al., 2016). This problem has been
called the label bias problem.
The label bias problem is more severe when the output probability
of yi can only depend on a partial input sequence x1:t(i) , where t(i)


4.1. CRFS AS CONDITIONAL EBMS

99

Figure 4.2: State transitions resulting from estimating an autoregressive language
model from training data - “Tom likes tea”, “John likes tea”, and “Alice like tea”.
For some transitions not appeared in the training data, the transition probabilities
are smoothed to take small values ϵ. We pad the beginning and the end of a sentence
with special tokens, ⟨s⟩ and ⟨/s⟩, respectively (Chen and Goodman, 1999).

denotes the available length of the partial input when producing yi .
For example, in streaming speech recognition, each token yi must be
recognized shortly after it was spoken. It is found in (Variani et al.,
2022) that by switching from a locally normalized model to a globally
normalized model, the streaming speech recognition performance can
be significantly improved.
In the case of using p(yi |x1:t(i) , y1 , · · · , yi−1 ), the model becomes:
p(y1:L |x1:T ) =

L
Y

p(yi |x1:t(i) , y1 , · · · , yi−1 ).

(4.8)

i=1

The label bias problem can be understood by considering the independence assumption made by the model. In this case, we have
y1:i ⊥ xt(i)+1:T |x1:t(i) , i.e.,
p(y1:i |x1:T ) = p(y1:i |x1:t(i) )
Thus, observations from later in the input sequence xt(i)+1:T has no
effect on the posterior probability of history input y1:i . Intuitively, we
would like the model to be able to revise an earlier decision made during
search, when later evidence becomes available that rules out the earlier
decision as incorrect (Andor et al., 2016).
The above discussion of the label bias problem is for conditional,
locally-normalized sequence models. In the following, we will show,


100

Conditional EBMs with applications

Figure 4.3: Estimating a globally-normalized energy-based language model (ELM)
from training data - “Tom likes tea”, “John likes tea”, and “Alice like tea”. The
bi-gram features used by the ELM are similar to those used in the bigram ALM, and
so can also be illustrated by a graph. The estimated parameters are shown over the
edges, which represent the corresponding bi-gram features.

through an empirical example1 , that the label bias problem causes
trouble not only in conditional, locally-normalized sequence models, but
also in unconditional, locally-normalized sequence models; and can be
naturally overcome in globally-normalized sequence models. Suppose
that we would like to build a model for sentences with the following
training data, which consist of three sentences:
Tom likes tea
John likes tea
Alice like tea
Since the training data are often noisy, there exist some samples with
some incorrect, infrequent patterns such as in “Alice like tea”. We
estimate different models over these training data, apply them to score
test data
Alice likes tea
Tom like tea
and examine the performance of different models.
Let us first consider an autoregressive language model (ALM), which
is a typical unconditional, locally-normalized sequence model, trained
1

This English example is adapted from a Chinese example in Section 3.4.1 of
(Wang, 2018)’s.


4.1. CRFS AS CONDITIONAL EBMS

101

over these training data. We estimate a smoothed bi-gram ALM, i.e., we
smooth the transition probabilities for unseen bi-grams. The resulting
ALM with estimated transition probabilities are shown in Figure 4.2. We
can see that transitions from the wrong history (“like”) and the correct
history (“likes”) to “tea” get the same score, due to local normalization
of conditional probabilities. Consequently, when scoring test data “Alice likes tea” and “Tom like tea”, the bi-gram ALM cannot score
them correctly - the two test samples are scored as being of the same
probability:
P (Alice likes tea) = P (Alice|⟨s⟩)P (likes|Alice)P (tea|likes)P (⟨s⟩|tea)
= 0.33 × ϵ × 1.0 × 1.0 = 0.33ϵ
P (Tom like tea) = P (Tom|⟨s⟩)P (like|Tom)P (tea|like)P (⟨s⟩|tea)
= 0.33 × ϵ × 1.0 × 1.0 = 0.33ϵ
In fact, “Alice likes tea” should be more likely than “Tom like tea”,
considering that the correct form of verb appears more often than
the incorrect form in the training data. We can see that the bi-gram
ALM seems not to be a good model choice for this example, due to
local normalization. To be more precise, this label bias problem is
caused by the improper local normalization following incorrect histories.
Incorrect histories occur in training data, as part of noise. Successors
after incorrect histories are in fact biased labels, even they appear in
training data. This issue should be somehow fixed in order to better
model the regularities in the data2 . However, it is worthwhile to remark
that how adverse this label bias issue will cause depends on the particular
data and model under investigation. Further theoretical analysis will
be interesting future work.
Next, let us consider a globally-normalized energy-based language
model (GN-ELM) (Section 3.2.1), trained over the same training data,
but with a different model assumption from ALMs. The globallynormalized model, which is defined as follows, also uses the bi-gram
2
We usually do not assume that the data generating distribution is exactly the
same as the empirical distribution, but rather the modeling task is to seek a smoothed
distribution based on the empirical distribution (See discussion of the concept of
learning in Section 1.1).


102

Conditional EBMs with applications

features, similar to those used in the bi-gram ALM:
9
X
1
P (x1 , x2 , x3 ) = exp
λk fk (x1 , x2 , x3 )
Z
k=1

!

(4.9)

where the bi-gram features are indicator functions as follows:
f1 = δ(x1 = Tom),
f2 = δ(x1 = John),
f3 = δ(x1 = Alice),
f4 = δ(x1 = Tom, x2 = likes)),
f5 = δ(x1 = John, x2 = likes),
f6 = δ(x1 = Alice, x2 = like),
f7 = δ(x2 = likes, x3 = tea),
f8 = δ(x2 = like, x3 = tea),
f9 = δ(x3 = tea).
Maximum likelihood estimate (MLE) of Eq. (4.9) can be performed by
the stochastic approximation (SA) method or the improved iterative
scaling (IIS) method, as introduced in (Wang et al., 2018). The estimated
parameters, {λk , k = 1, · · · , 9}, are shown in Figure 4.3. We show the
parameters over the edges, which represent the corresponding bi-gram
features. The estimated ELM model can be used to score test data “Alice likes tea” and “Tom like tea”:
log P (Alice likes tea) = 2.53 + 5.21 + 5.18 − log Z = −7.06
log P (Tom like tea) = 2.88 + 4.12 + 5.18 − log Z = −7.79
where the log normalizing constant, in this simple example, can be
directly calculated as follows:
log Z = log

X
x1 ∈{Tom,John,Alice},
x2 ∈{likes, like},x3=tea

exp

9
X

!

λk fk (x1 , x2 , x3 )

= 19.98

k=1

Interestingly, according to the ELM modeling, “Alice likes tea” is naturally scored as being more probable than “Tom like tea”, without


4.1. CRFS AS CONDITIONAL EBMS

103

Figure 4.4: Illustration of exposure bias. y: real, ŷ: predicted.

relying on any other ad-hoc tricks. It seems that the ELM model is
smoothed more correctly (not disturbed by the noise in the training
data) - this is a desirable result. Compare to the ALM modeling, the
ELM modeling abstains from using local normalization. There is no
improper local normalization following incorrect histories. In this sense,
we could say that the label bias issue is avoided.
The exposure bias problem
Locally-normalized sequence models, whether conditional or unconditional, suffers from the exposure bias problem. Locally-normalized
sequence models are usually trained in a manner called teacher forcing
(Williams and Zipser, 1989). Take the training of a ALM as an example.
• In training, the model maximizes the likelihood of each successive
target word, conditioned on the gold history of the target word.
As shown in Figure 4.4, in training, the model is only exposed
to real data, which predicts ŷi given the real data of the history,
y1 , · · · , yi−1 .


104

Conditional EBMs with applications

• In testing, the model predicts the next step ŷi , using its own
predicted words in testing, i.e., ŷ1 , · · · , ŷi−1 .
Such mismatch between training (teacher forcing) and testing (prediction) of locally-normalized sequence models has been called the exposure
bias problem. The model is never exposed to its own errors during
training, and so the inferred histories at test-time do not resemble the
gold training histories (Wiseman and Rush, 2016).
Remarkably, exposure bias results from training in a certain way,
which maybe alleviated by some ad-hoc methods such as scheduled
sampling (Bengio et al., 2015). In contrast, label bias results from
properties of the model itself. Thus, it may be more difficult to overcome
label bias than to avoid exposure bias.
4.1.3

Training of CRFs

To introduce the training methods for CRFs/conditional EBMs, we
write the model in Definition 4.1 in a simpler form, with input x and
output y:
1
pθ (y|x) =
exp [Uθ (x, y)]
(4.10)
Zθ (x)
Uθ (x, y) : X × Y → R denotes the (log) potential function, which assigns
a scalar value to each combined configuration of x in X and y in Y, and
can be very flexibly parameterized through neural networks of different
architectures. X and Y denotes the space of all possible values of x and
y, respectively. Normalizing is taken only over Y, and Zθ (x) denotes
the normalizing constant:
Zθ (x) =

X

exp [Uθ (x, y)]

(4.11)

y∈Y

Learning CRFs by conditional maximum likelihood (CML)
Suppose we have a training dataset consisting of N independent and
identically distributed (IID) data points D = {(xi , yi ), i = 1, · · · , N }.
We can fit pθ (x) to data by maximizing the log conditional likelihood


4.1. CRFS AS CONDITIONAL EBMS

105

of training data, defined by
L(θ) ≜

N
N
1 X
1 X
log pθ (yi |xi ) =
{Uθ (xi , yi ) − log Zθ (xi )}
N i=1
N i=1

(4.12)

as a function of θ.
Taking the derivative of the log conditional likelihood with respect
to θ and making use of Eq. (2.18) about the derivative of the log
normalizing constant, we obtain the core formula in learning conditional
EBMs:
∇θ L(θ) =

N n
o
1 X
∇θ Uθ (xi , yi ) − Epθ (y|xi ) [∇θ Uθ (xi , y)]
N i=1

(4.13)

The maximum likelihood estimate of θ is obtained as a solution to
∇θ L(θ) = 0. It can be easily seen that the gradients ∇θ L(θ) in Eq. (4.13)
in learning conditional EBMs exactly follows the form of Eq. (2.31),
as summarized in Theorem 2.3. So the problem of learning conditional
EBMs by conditional maximum likelihood (CML) can then be solved by
setting the gradients to zeros and applying the SA algorithm to finding
the root for the resulting system of simultaneous equations. Techniques
in learning (unconditional) EBMs are applicable in learning conditional
EBMs here. See Section 2.3.3 for details. For example:
• Minibatching from training data when N is large;
• When calculating the model expectation Epθ (y|xi ) [∇θ Uθ (xi , y)] is
intractable, we can resort to Monte Carlo methods and conduct
Monte Carlo averaging.
• The model expectation can be exactly calculated under some
limited circumstances, mostly in low tree-width3 random fields
(e.g., chain-structured) with moderately sized state spaces 4 .
3

The tree-width of a graph is defined as the minimum width over all possible
tree decompositions of the graph, which measures roughly how close the graph is to
a tree. The tree-width of a tree is 1.
4
The state space of a multivariate model is the set of all possible values for each
coordinate of a multivariate observation.


106

Conditional EBMs with applications

CML training of neural linear-chain CRFs
Here we provide more details about CML training of a neural linearchain CRF, for which the model expectation can be exactly calculated.
Continue with the notations in Section 4.1.1, and consider a neural
linear-chain CRF as follows, with input x = (x1 , · · · xT ) = x1:T , output
y = (y1 , · · · yT ) = y1:T . Combining Eq. (4.3), Eq. (4.5) and Eq. (4.6),
we obtain
1
pθ (y|x) =
exp [Uθ (x, y)]
Zθ (x)
Uθ (x, y) =

T
X
 y

ϕt t + Ayt−1 ,yt

(4.14)



t=1

where the parameters θ consists of the network parameters and the
transition matrix A.
Suppose the state space of y1:T is {1, · · · , K}, i.e., yi ∈ {1, · · · , K}.
Thus, at each position t = 1, · · · , T , there are K node potential values,
ϕkt , k = 1, · · · , K, which are calculated by a neural network, as defined
in Eq. (4.5). The network is trained to optimize the log conditional
likelihood Eq. (4.12). The gradient of the log conditional likelihood
w.r.t. θ for a single data point (xi , yi ) is:
∂ log pθ (yi |xi )
∂Uθ (xi , yi )
∂Uθ (xi , y)
=
− Epθ (y|xi )
∂θ
∂θ
∂θ




(4.15)

Note that log pθ (yi |xi ) depends on the network parameters through
ϕyt t . Thus, an important quantity in calculating Eq. (4.15) for the
network parameters is the gradient w.r.t. to the potential values, which
can be derived from Eq. (4.14) as follows:
∂ log pθ (yi |xi )
= δ(yt = k) − Epθ (y|xi ) [δ(yt = k)]
∂ϕkt
= δ(yt = k) − p(yt = k)
This difference between the empirical count and the expected count
is the error signal received by the NN based feature extractor during
training. The expected count p(yt = k) is often known as the posterior
state occupation probability, which can be calculated using the alphabeta variables from the forward-backward algorithm (Rabiner, 1989).


4.1. CRFS AS CONDITIONAL EBMS

107

The gradients for the network parameters can be calculated from the
gradient w.r.t. to the potential values based on the back-propagation
procedure.
Learning CRFs by conditional NCE
A theoretical analysis of NCE in the estimation of conditional unnormalized models in the form of Eq. (4.10) has been given in (Ma and
Collins, 2018). A subtle but important question when generalizing NCE
to the conditional case is raised, i.e., the unconditional EBM in Eq.
(2.13) has a single partition function, which is estimated as a parameter
of the model, whereas the conditional model in Eq. (4.10) has a separate
partition function Zθ (x) for each value of x.
Introduce the unnormalized density p̃θ (y|x) = exp [Uθ (x, y)] and
rewrite Eq. (4.10) as follows:
pθ (y|x) =

1
p̃θ (y|x)
Zθ (x)

(4.16)

where, for each value of x, the partition function is defined by
Zθ (x) =

X

p̃θ (y|x)

y∈Y

Applying NCE in estimating conditional unnormalized models yields
the conditional NCE method. Suppose we have a training dataset
consisting of N independent and identically distributed (IID) data
points D = {(xi , yi ), i = 1, · · · , N }. The basic idea of NCE is to perform
nonlinear logistic regression to discriminate between data samples drawn
from the data and noise samples drawn from a known noise distribution
q(x|y). Formally, referring to Eq. (2.46), conditional NCE estimates the
model parameters θ by maximizing the following objective function:
p̃θ (yκ |xκ )
p̃θ (yκ |xκ ) + νq(yκ |xκ )


νq(y|xκ )
+ νEy∼q(y|xκ ) log
p̃θ (y|xκ ) + νq(y|xκ )


J(θ) = Eκ∼U ni(1,N ) log

where ν represents the ratio of noise sample size to real sample size.
It is shown in (Ma and Collins, 2018) that the conditional NCE
method gives consistent parameter estimates under the assumption that


108

Conditional EBMs with applications

Zθ (x) is constant with respect to x. Equivalently, the conditional NCE
method is consistent under the assumption that the function p̃θ (y|x)
is powerful enough to incorporate Zθ (x). Thus, under the assumption
that the model is of sufficient capacity, the model can learn to be selfnormalized such that Zθ (x) = 1. Remarkably, the discussion in (Ma and
Collins, 2018) is in line with Theorem 2.4 about NCE (nonparametric
estimation), since in the setting of nonparametric estimation, the NCE
loss is minimized when p̃θ (y|x) matches the oracle density and thus is
self-normalized.
As a final note, it can be easily seen that training of the Electric
model in Section 3.4 is exactly an instance of the conditional NCE
method.
4.2

CRFs for speech recognition

In this section, we show the development of CRFs for automatic speech
recognition (ASR). First, we introduce some background knowledge
in ASR, particularly the connectionist temporal classification (CTC)
method for recent end-to-end ASR. Then, we elaborate on the recently
developed approach of CRF-based single-stage acoustic modeling with
CTC topology (Xiang and Ou, 2019; An et al., 2020).
ASR basically is a sequence discriminative problem. Given acoustic
observations x = (x1 , · · · xT ) = x1:T , the task of ASR is to find the most
likely labels y = (y1 , · · · yL ) = y1:L . Acoustic observations are usually
spectral feature vectors. Different units can be used for labels, such as
phone (namely monophone), triphone, character, wordpiece and word,
as shown in Figure 4.5.
4.2.1

Connectionist Temporal Classification (CTC)

The DNN-HMM hybrid approach
The history of speech recognition dates back to 1970s (Jelinek, 1976)
and even earlier. The classic model for speech recognition is HMMs,
as we review in Section 2.1.1. In building an HMM for speech recognition, consisting of acoustic sequence x = (x1 , · · · xT ) = x1:T and state


4.2. CRFS FOR SPEECH RECOGNITION

109

Figure 4.5: Different units of labels can be used in speech recognition.

sequence π = (π1 , · · · πT ) = π1:T , we need to specify two distributions,
the state-transition distribution and the state-output distribution.
The state sequence π forms a Markov chain. In terminology of
Markov chains, state transitions in a Markov chain are determined by
its state topology, or say, its state transition graph. The state transition
graph is determined by combining some knowledge sources, as illustrated
in Figure 4.6. For example, we need a language model to model how
words are connected to form a sentence. We need a lexicon to model
how phones are connected to form a word. Depending on left and right
phonetic contexts, we can define different context-dependent phones,
e.g., triphones. Each context-dependent phone can be decomposed into
several phonetic states, which intuitively correspond to the beginning,
steady, closing phases of a phone. The resulting state transition graph,
often represented by a weighted finite-state transducer (WFST), encodes
common constraints in human speech5 . In practice, each knowledge
source can be represented by a component WFST, and the combination
of several knowledge sources can be easily realized by applying the
composition and some compression operations to the component WFSTs
and producing a single integrated WFST (Mohri et al., 2008).
Formally, a WFST represents a weighted relation between sequences
of input symbols and sequences of output symbols (Mohri et al., 2008).
5

For subtle differences between the two graphs (WFSTs and state transition
graphs), readers can refer to (Ou and Xiao, 2010).


110

Conditional EBMs with applications

Figure 4.6: State transitions in HMMs for speech recognition are constrained by a
number of knowledge sources.

A state sequence π for an utterance of T frames can be seen as a path
traversing in the integrated WFST for T steps, with π = (π1 , · · · πT )
matching the input symbols and the emitted output symbols corresponding to y = (y1 , · · · yL ). It can be seen that in this way, a path π
uniquely determines a label sequence y, but not vice versa. Thus, based
on the integrated WFST, a many-to-one mapping, BHMM : π → y, is
defined. The state topology basically determines the mapping. We also
say that the mapping represents the state topology.
For state-output distribution p(xt |πt ), Gaussian mixture models
(GMMs) were commonly used in the so-called GMM-HMM approach,
before the deep learning era. Recently, deep neural networks (DNNs) of
various architectures have become dominantly used for modeling the
state-output distributions. Based on the following Bayesian formula,
the state likelihood p(xt |πt ) can be calculated from the state posteriori
p(πt |xt ), divided by the state prior p(πt ), while the marginal likelihood
p(xt ), being a constant, can be ignored.
p(xt |πt ) =

p(πt |xt )p(xt )
p(πt )

State prior probabilities are estimated from the training data. State
posterior probabilities are calculated from the DNN, but we need framelevel alignments in training of the DNN.
In summary, the approach described above is often referred to as
the DNN-HMM hybrid approach for ASR (Dahl et al., 2012). It is
featured by using the frame-level loss (cross-entropy) to train the DNN
to estimate the posterior probabilities of HMM states.


4.2. CRFS FOR SPEECH RECOGNITION

111

End-to-end ASR
Notably, building DNN-HMM systems for ASR runs in multi-stages. A
GMM-HMM training is firstly needed to obtain frame-level alignments
and then the DNN-HMM is trained. The hybrid approach usually
consists of an DNN-HMM based acoustic model (AM), a state-tying
decision tree for context-dependent phone modeling, a pronunciation
lexicon and a language model (LM), which can be compactly combined
into a weighted finite-state transducer (WFST). The WFST not only
operationally represents the state topology, but also is useful as a
compact data structure for efficient decoding.
A recent trend in ASR is to develop end-to-end ASR models (Graves
et al., 2006; Miao et al., 2015; Graves, 2012; Chorowski et al., 2014). The
end-to-end approach is characterized by eliminating the construction
of GMM-HMMs and phonetic decision-trees, training the DNN from
scratch (in single-stage) and, even ambitiously, removing the need for a
pronunciation lexicon and training the acoustic and language models
jointly rather than separately. For developing end-to-end ASR models,
there are two main issues.
• The first is how to handle alignment between x and y, since the
two sequences generally differ in length (T ̸= L).
• The second is how to obtain p(y|x), the posteriori probability of
the label sequence y given the acoustic sequence x. To be end-toend, we need a differentiable sequence-level loss of mapping the
acoustic sequence x to the label sequence y.
Three widely-used end-to-end models are based on connectionist
temporal classification (CTC) (Graves et al., 2006), RNN-transducer
(RNN-T) (Graves, 2012), and attention based encoder-decoder (AED)
(Chorowski et al., 2014) respectively. The three models are featured by
different losses. In CTC and RNN-T, the alignment is handled explicitly
by introducing hidden state sequence π, while AED implements implicit,
soft alignment via the attention mechanism.
Remarkably, all the three models are (conditional) locally normalized
sequence models, according to the classification of sequence models
as shown in Table 4.1. In Section 4.2.2, we will introduce a recently


112

Conditional EBMs with applications

Figure 4.7: Overview of CTC architecture.

developed (conditional) globally normalized sequence model, called
CTC-CRF, for end-to-end ASR.
The CTC method
The motivation of CTC (Graves et al., 2006) is to enable the training
of p(y|x) without requiring the frame-level alignments between the
acoustics x and the transcripts y. To this end, CTC introduces a blank
symbol <b> in addition to the ordinary labels, and further introduces a
state sequence π = (π1 , · · · πT ) = π1:T , which aids the aligning between
x1:T and y1:L . See Figure 4.7 for an overview of CTC architecture.
Handle alignment. Given acoustic sequence x1:T , at each frame t, the
possible values that πt can freely take is Y ∪ <b>, where Y denotes
the the alphabet of ordinary labels. Suppose the alphabet size is K,
then number of possible values that πt can freely take is K + 1. When
given y1:L , the state transitions followed by π is constrained by the state
topology, so that the output sequence is y1:L . CTC defines a special
state topology, which is enforced by a special many-to-one mapping
BCTC : π1:T → y1:L . The mapping BCTC is defined by reducing repetitive
symbols in π to a single symbol, and removing all blank symbols, e.g.,
BCTC (−CC − −AA − T−) = CAT
It can be seen that for given acoustic sequence x1:T and label sequence
y1:L , all possible alignments between them can be organized in a lattice,


4.2. CRFS FOR SPEECH RECOGNITION

113

Figure 4.8: Illustration of the lattice, which contains all the possible alignments
between the acoustic sequence and the label sequence ‘CAT’. Also illustration of
the forward-backward algorithm. Black circles represent ordinary labels, and white
circles represent blanks. Arrows signify allowed transitions. (Graves et al., 2006)

as shown in Figure 4.8. A path from the top left to the bottom right in the
lattice represents an alignment between x and y, which, in terminology
of CTC (Graves et al., 2006), is denoted by π1:T ∈ {Y ∪ <b>}T .
Obtain p(y|x). We can use any kind of neural network (NN) to
calculate the high-level feature vectors h1:T = (h1 , · · · hT ) ∈ RD×T
from the raw spectral features x1:T , or simply say, we encode x into
h. For each frame t, we obtain ht . The vector representations h1:T can
be viewed as features, and the neural network is referred as a feature
extractor as discussed previously in Section 2.1.1, or an acoustic encoder
in the context of ASR.
Then, we can apply a linear layer followed by a softmax layer to
calculate the posteriori distribution of πt , as follows:
zt = W T ht + b ∈ RK+1
exp(ztk )
k
p(πt = k|x) = PK
j ≜ pt , k = 1, · · · , K + 1
j=1 exp(zt )
where W ∈ R(K+1)×D , b ∈ RK+1 denote the weight matrix and bias


114

Conditional EBMs with applications

vector of the linear layer, respectively. pkt represents the the probability
of observing label k at time t. The un-normalized outputs zt are often
called logits, and ztk denotes the k-th logit corresponding to label k.
CTC assumes the conditional independence between states in a path
π1:T , and defines the path posteriori as follows:
p(π|x) =

T
Y

(4.17)

p(πt |x)

t=1

Finally, we use the CTC topology to calculate the posteriori probability
of the label sequence y, by summing over all possible paths, which map
to y:
p(y|x) =

X

(4.18)

p(π|x)

π:BCTC (π)=y

CTC model training. In the previous introduction of the CTC model,
we suppress the model parameters in formula. In the following, we make
explicit of the model parameters θ, which parameterizes the acoustic
encoder network.
The network is trained to optimize the log conditional likelihood
Eq. (4.18). According to Fisher equality Eq. (B.8), the gradient of the
log conditional likelihood w.r.t. θ for a single data point (x, y) is:
∂ log pθ (y|x)
∂ log pθ (π, y|x)
= Epθ (π|x,y)
∂θ
∂θ


∂ log pθ (π|x)
= Epθ (π|x,y)
∂θ




(4.19)

where the second line holds because π deterministically determines y
with BCTC .
Note that log pθ (y|x) depends on the network parameters through
logits ztk ’s. Thus, an important quantity in calculating Eq. (4.19) for
the network parameters is the gradient w.r.t. to the logits, which can


4.2. CRFS FOR SPEECH RECOGNITION

115

be derived as follows:
∂ log pθ (y|x)
log pθ (π|x)
= Epθ (π|x,y)
(∵ Fisher equality Eq.(B.8))
k
∂zt
∂ztk
"

#

"

log pπt t
(∵ Eq. (4.17))
∂ztk

= Epθ (π|x,y)

#

h

= Epθ (π|x,y) δ(πt = k) − pkt

i

= pθ (πt = k|x, y) − pkt

(4.20)

This difference between the posteriori probability and the prior probability pkt (without observing y) is the error signal received by the NN based
feature extractor during training. pθ (πt = k|x, y) is often known as the
posterior state occupation probability, which can be calculated using
the alpha-beta variables from the forward-backward algorithm (Rabiner,
1989). The gradients for the network parameters can be calculated
from the gradient w.r.t. to the logits based on the back-propagation
procedure.
4.2.2

CRF-based acoustic modeling with CTC topology

Motivation
From Eq. (4.17), we see that the CTC model assumes the conditional
independence between states in a path. To overcome this drawback, the
RNN-T model (RNN-transducer) and the CTC-CRF model (CRF with
CTC topology) have been developed in (Graves, 2012) and (Xiang and
Ou, 2019), respectively.
A second motivation is that we are interested in bridging the hybrid
and the end-to-end approaches for ASR, trying to inherit the dataefficiency of the hybrid approach and the simplicity of the end-toend approach. Remarkably, when comparing the hybrid and end-toend approaches (modularity versus a single neural network, separate
optimization versus joint optimization), it is worthwhile to note the
pros and cons of each approach.
• The end-to-end approach aims to subsume the acoustic, pronunciation, and language models into a single neural network and


116

Conditional EBMs with applications
perform joint optimization. This appealing feature comes at a cost,
i.e. the end-to-end ASR systems are data hungry, which require
above thousands of hours of labeled speech to be competitive with
the hybrid systems (Lüscher et al., 2019; Chiu et al., 2018; Tüske
et al., 2019).

• In contrast, the modularity of the hybrid approach permits training
the AM and LM independently and on different data sets. A
decent acoustic model can be trained with around 100 hours of
labeled speech, whereas the LM can be trained on text-only data,
which is available in vast amounts for many languages. In this
sense, modularity promotes data efficiency. Due to the lack of
modularity, it is difficult for an end-to-end model to exploit the
text-only data, though there are recent efforts to alleviate this
drawback (Toshniwal et al., 2018; Zheng et al., 2022).
The CTC-CRF model
The CTC-CRF approach consists of separable AM and LM, which meets
the principle to be data efficient by keeping necessary modularity. In
the following, we mainly describe the CTC-CRF based AM. Different
types of LMs, whether autoregressive LMs or energy-based LMs, can
be used with the CTC-CRF based AM. Different schemes are available
for decoding, such as WFST based decoding with n-gram LMs, onepass decoding with shallow fusion of AM and LM scores, or two-pass
decoding with LM rescoring. More details can be found in the toolkit
(An et al., 2020).
Continue with the notations in CTC, and note that the core formula
in establishing the CTC model is Eq. (4.17) and Eq. (4.18), while Eq.
(4.17) makes the conditional independence assumption. The main idea
of CTC-CRF is that we can still use the CTC topology to define the
posteriori probability of the label sequence y, pθ (y|x), from the path
posteriori pθ (π|x), as defined in Eq. (4.18), but we define the path
posteriori as a globally normalized sequence model, or say, a condition
random field (CRF), as follows:
exp(ϕθ (π, x))
′
π ′ exp(ϕθ (π , x))

pθ (π|x) = P

(4.21)


4.2. CRFS FOR SPEECH RECOGNITION

117

Here ϕθ (π, x) denotes the potential function of the CRF, defined as:
ϕθ (π, x) = log p(B(π)) +

T
X

log pθ (πt |x)

(4.22)

t=1
t=1 log pθ (πt |x) defines the node potential, which is calculated from
the acoustic encoder network with parameters θ. log p(B(π)) defines the
edge potential, realized by an n-gram LM of labels.
Remarkably, regular CTC suffers from the conditional independence
between the states in π. In contrast, by incorporating log p(B(π)) into
the potential function in CTC-CRF, this drawback is naturally avoided.
The difference between the CTC model and the CTC-CRF model can
be clearly seen from their graphical model representations, as shown
in Figure 4.9. Note that the n-gram LM of labels means the transition
structure between labels is of (n-1)-th order. The transition structure
between πt ’s, when represented by a WFST, is determined by the
composition of two component WFSTs, i.e., the WFST representation
of the CTC topology and the WFST representation of the n-gram LM
of labels. For reasons to be clear in the following, the resulting WFST is
referred to as the denominator WFST. Due to the composition operation,
the order of the transition structure between states (πt ’s) is larger than
(n-1)-th order. Thus, as a reminder for reading Figure 4.9(b), the edge
potential does not involve exactly n consecutive nodes for a n-gram LM
of labels. The graphical model representation of CTC-CRF in Figure
4.9(b) is mainly for concept illustration. In practice, in training of a
CTC-CRF model, the forward-backward algorithm involving log p(B(π))
can be conducted in the denominator WFST (to be detailed in the
following).
Finally, note that we may use a n-gram LM of words in decoding. It
is reminded not to confuse the n-gram LM of labels used in defining the
potential in CTC-CRF and the n-gram LM of words used in decoding.

PT

Training of the CTC-CRF model
Note that only the parameters from the acoustic encoder network,
denoted by θ, need to be trained in the CTC-CRF model, since the


118

Conditional EBMs with applications

Figure 4.9: Graphical model representation of the CTC model (a) and the CTCCTF model (b). Note that the edge potential does not involve exactly n consecutive
nodes for a n-gram LM of labels, as detailed in the text of Section 4.2.2.

n-gram LM of labels is estimated from the training transcripts and
fixed in estimating θ.
The network is trained to optimize the log conditional likelihood Eq.
(4.18). Making use of the Fisher equality Eq. (B.8) and the gradient of
CRF’s log conditional likelihood Eq. (4.13), the gradient of CTC-CRF’s
log conditional likelihood w.r.t. θ for a single data point (x, y) is:
∂ log pθ (y|x)
log pθ (π|x)
= Epθ (π|x,y)
(∵ Eq.(B.8), similar to CTC)
∂θ
∂θ



∂ϕθ (π ′ , x)
∂ϕθ (π, x)
− Epθ (π′ |x)
= Epθ (π|x,y)
∂θ
∂θ




∂ϕθ (π, x)
∂ϕθ (π ′ , x)
= Epθ (π|x,y)
− Epθ (π′ |x)
∂θ
∂θ
(4.23)




where the second term in the last line does not depend on π and thus
can be moved out of the expectation w.r.t. π. Notably, pθ (π ′ |x) denotes
the model distribution, as defined in Eq. (4.21) but with π ′ to represent
the path. As commonly found in estimating CRFs, the above gradient
is the difference between empirical expectation and model expectation.
The two expectations are similar to the calculations using the numerator
graph and denominator graph in LF-MMI respectively (Povey et al.,
2016).
Remarkably, Eq. (4.23) can be derived in another way, which reveals
the concept of numerator and denominator. Combining Eq. (4.18) and


4.2. CRFS FOR SPEECH RECOGNITION

119

Eq. (4.21) yields CTC-CRF’s log conditional likelihood:
π:BCTC (π)=y exp(ϕθ (π, x))
P
′
π ′ exp(ϕθ (π , x))

P

log pθ (y|x) = log

(4.24)

It can be easily seen that the gradient of the above objective function
involves two gradients calculated from the numerator and denominator
respectively, which can be shown to be equal to the two terms in Eq.
(4.23).
Denote log p(πt = k|x) = ϕkt , and note that log pθ (y|x) depends on
the network parameters through potential values ϕkt , 1 ≤ t ≤ T, 1 ≤
k ≤ K + 1. Thus, an important quantity in calculating Eq. (4.23) for
the network parameters θ is the gradient w.r.t. to the potential values,
which can be derived as follows:
∂ϕθ (π, x)
∂ϕθ (π ′ , x)
∂ log pθ (y|x)
′ |x)
=
E
−
E
p
(π|x,y)
p
(π
θ
θ
∂ϕkt
∂ϕkt
∂ϕkt
"

#

"

= Epθ (π|x,y) [δ(πt = k)] − Epθ (π′ |x) δ(πt′ = k)




#

(4.25)

This difference is the error signal received by the NN based feature
extractor during training. The gradients for the network parameters θ
can be calculated from the gradient w.r.t. to the potential values based
on the back-propagation procedure.
Both terms in Eq. (4.25) can be obtained via the forward-backward
(FB) algorithm.
• Calculating the first term, often referred to the numerator calculation, amounts to running the FB algorithm over the WFST
determined by y, which is similar to conducting the FB algorithm
in calculating the first term of Eq. (4.20) in CTC.
• Calculating the second term, often referred to the denominator
calculation, involves running the forward-backward algorithm over
the denominator WFST Tden . Tden is an composition of the CTC
topology WFST and the WFST representation of the n-gram
LM of labels. The n-gram LM of labels is thus often called the
denominator n-gram LM, to be differentiated from the word-level
LM in decoding.


120

Conditional EBMs with applications

Table 4.2: Comparison of different models for ASR. HMM topology denotes that
labels (including silence) are modeled by multiple states with left-to-right transitions,
possible self-loops and skips. CTC topology denotes the special state transitions used
in CTC (including blank). Locally/globally normalized denotes the formulation of the
model distribution. In defining the joint distribution of a model, locally normalized
models use conditional probability functions, while globally normalized models use
un-normalized potential functions. SS-LF-MMI is classified as globally normalized,
though it is cast as MMI-based discriminative training of a pseudo HMM and the
HMM model is locally normalized. AED does not use states to align label sequence
y and observation sequence x.

Model

State
topology

Training
objective

Locally/globally
normalized

HMM
CTC
SS-LF-MMI
CTC-CRF
RNN-T
AED

HMM
CTC
HMM
CTC
RNN-T
-

p(x|y)
p(y|x)
p(y|x)
p(y|x)
p(y|x)
p(y|x)

local
local
global
global
local
local

Related work
In Table 4.2, we give a brief review of existing models in ASR, depending on state topologies, training objectives and whether the model
distribution is locally or globally normalized. We differentiate HMM
topology and CTC topology, though the later may be interpreted as
a special HMM topology (Zeyer et al., 2017). The two differ not only
in the state transition structure but also in the label inventory used
(which affects not only the definition of the whole state space but also
the estimation of the denominator LM).
Further, graphical model representations of existing ASR models
are plotted in Figure 4.10, which clearly show the differences between
those models. An ASR model involves an acoustic observations x =
(x1 , · · · xT ) = x1:T and a label sequence y = (y1 , · · · yL ) = y1:L . HMM,
CTC and CTC-CRF are defined in Eq. (2.2), Eq. (4.17), and Eq. (4.21),
respectively. RNN-transducer (RNN-T) (Graves, 2012) is defined by
p(π1:T +L |x1:T ) =

TY
+L
j=1

p(πj |π1:j−1 )

(4.26)


4.2. CRFS FOR SPEECH RECOGNITION

121

Figure 4.10: Graphical model representations of different ASR models: (a) HMM,
defined in Eq. (2.2), (b) CTC, defined in Eq. (4.17), (c) RNN-T, (d) AED, (e)
CTC-CRF, defined in Eq. (4.21).

Here π1:T +L = (π1 , · · · πT +L ) denote the state sequences, or say, the
path with T blanks and L labels in RNN-T, such that removing the
blanks in π1:T +L yields y1:L (see Section 4.3.1 for details). Attention
based encoder-decoder (AED) (Chorowski et al., 2014) is defined by
p(y1:L |x1:T ) =

L
Y

p(yi |x, y1 , · · · , yi−1 )

(4.27)

i=1

In summary, from Table 4.2 and Figure 4.10, we can cleary see that CTCCRF is fundamentally different from those prior models. For comparison
between CTC-CRF and single-stage (SS) lattice-free maximum-mutualinformation (LF-MMI) (Hadian et al., 2018), readers can refer to (Xiang
and Ou, 2019).
Relation to CRF-based acoustic models. ASR is a sequence transduction problem in that the input and output sequences differ in lengths,
and both lengths are variable. An idea in applying CRFs to ASR is to
introduce a (hidden) state sequence π to align the label sequence y and
observation sequence x, and define a CRF p(π|x) over the (hidden) state
sequence π. As shown in Eq. (4.18), deriving p(y|x) based on p(π|x)
depends on the mapping between π and y, which is determined by the
state topology that allows for different choices, e.g., CTC topology or


122

Conditional EBMs with applications

HMM topology. This kind of hidden CRFs was explored in (Gunawardana et al., 2005) for phone classification, using zero, first and second
order features. Generally speaking, (hidden) CRFs using neural features
for ASR are underappreciated. The CTC-CRF model proposed in (Xiang and Ou, 2019) represents the first exploration of CRFs with CTC
topology and advances the CRF-based approach with strong empirical
results. Segmental CRFs (Lu et al., 2016) provide another solution to
the alignment problem.
Performance of the CTC-CRF model
The CTC-CRF model inherits the data-efficiency of the hybrid approach
and the simplicity of the end-to-end approach. CTC-CRF eliminates the
conditional independence assumption in CTC and performs significantly
better than CTC on a wide range of benchmarks, including WSJ (80-h),
AISHELL (170-h Chinese), Switchboard (260-h), Librispeech (1000-h),
and Fisher-Switchboard (2300-h) (the numbers in the parentheses are
the size of training data in hours) (Xiang and Ou, 2019; An et al., 2020).
It has been also shown (Xiang and Ou, 2019; An et al., 2020) that
CTC-CRF is on par with other state-of-the-art end-to-end models, like
RNN-T and AED.
The CTC-CRF models have also been used in a variety of tasks in
ASR, which show their great potential.
• Streaming ASR, particularly the Chunking, Simulating Future
Context and Decoding (CUSIDE) approach (An et al., 2022);
• Neural architecture search (Zheng et al., 2021a);
• Children Speech Recognition (Yu et al., 2021);
• Modeling with wordpieces and Conformer architectures (Zheng
et al., 2021b);
• Multilingual and crosslingual speech recognition, particularly the
JoinAP (Joining of Acoustics and Phonology) approach (Zhu et
al., 2021).


4.3. CRFS FOR SEQUENCE LABELING IN NLP
4.3

123

CRFs for sequence labeling in NLP

Conditional random fields (CRFs) have been shown to be one of the most
successful approaches to sequence labeling. Various linear-chain neural
CRFs (NCRFs) have been developed, as introduced in Section 4.1.1. The
node potential modeling is improved by using NNs, but the linear-chain
structure is still kept, i.e., using a bigram table as the edge potential.
NCRFs represent an extension from conventional CRFs, where both node
potentials and edge potentials are implemented as linear functions using
discrete indicator features. However, linear-chain NCRFs capture only
first-order6 interactions and neglect higher-order dependencies between
labels, which can be potentially useful in real-world sequence labeling
applications, e.g., as shown in (Zhang et al., 2018) for chunking and
NER. How can we improve CRFs to capture long-range dependencies
in the label sequence (preferably non-Markovian)?
Related work. Extending CRFs to model higher-order interactions
than pairwise relationships between labels is an important research
problem for sequence labeling. There are some prior studies, e.g. higherorder CRFs (Chatzis and Demiris, 2013), semi-Markov CRFs (Sarawagi
and Cohen, 2004) and latent-dynamic CRFs (Morency et al., 2007),
but not using NNs. Using NNs to enhance the modeling of long-range
dependencies in CRFs is under-appreciated in the literature. A related
work is structured prediction energy networks (SPENs) (Belanger and
McCallum, 2016), which use neural networks to define energy functions that potentially can capture long-range dependencies between
structured outputs/labels. SPENs depend on relaxing labels from discrete to continuous and use gradient descent for test-time inference,
which is time-consuming. Training and inference with SPENs are still
challenging, though with progress (Tu and Gimpel, 2018).
Outside of the globally normalized sequence models, where CRFs
represent a typical class, attention-based encoder-decoder (AED) and
RNN-T exploit non-Markovian dependences between labels, but both
are locally normalized sequence models and thus suffer from the label
6

Fixed n-th order can be cast as first-order.


124

Conditional EBMs with applications

bias and exposure bias problems, as described in Section 4.1.2. The
work in (Wiseman and Rush, 2016) extends AED, by removing the final
softmax in the RNN decoder to learn global sequence scores, but cast as
a non-probabilistic variant of the sequence-to-sequence model. A recent
work in (Cui et al., 2021) aims to reducing exposure bias in training
RNN-T.
In this section, we mainly introduce a progress made by CRF transducers (Hu et al., 2019)7 , which introduce a LSTM-RNN to implement a
new edge potential so that long-range dependencies in the label sequence
are captured and modeled in CRFs. So there are two LSTM-RNNs in a
CRF transducer, one extracting features from observations to define the
node potential and the other capturing (theoretically infinite) long-range
dependencies between labels to define the edge potential. In this view,
a CRF transducer is similar to a RNN-transducer (RNN-T) (Graves,
2012), which also uses two LSTM-RNNs.
In the following, we firstly briefly introduce RNN-T, and then
describe CRF transducer in details. We continue with the notations in
Section 4.1.1 for sequence labeling. Given a sequence of observations
x = (x1 , · · · xT ) = x1:T , the task of sequence labeling is to predict
a sequence of labels y = (y1 , · · · yT ) = y1:T , with one label for one
observation in each position. yi ∈ {1, · · · , K} denotes the label at
position i.
4.3.1

RNN-Transducer (RNN-T)

RNN Transducers (RNN-T) are originally developed for general sequenceto-sequence learning (Graves, 2012), which do not assume that the input
and output sequences are of equal lengths and aligned, e.g., in speech
recognition. In the following, we introduce RNN transducers in a simple
form for applications in sequence labeling, i.e., for the aligned setting one label for one observation at each position. To this end, we define
p(y|x) =

T
Y

p(yi |y0:i−1 , x)

i=1

7

Reproducible code is at https://github.com/thu-spmi/SPMISeq

(4.28)


4.3. CRFS FOR SEQUENCE LABELING IN NLP

125

Table 4.3: Model comparison and connection.

Model
Linear-chain CRF
RNN Transducer
CRF transducer

Globally
normalized
√

Long-range dependencies
between labels
×
√
√

×
√

and implement p(yi |y0:i−1 , x) through two networks - transcription
network F and prediction network G as follows:
exp(fik + gik )
k′
k′
k′ =1 exp(fi + gi )

p(yi = k|y0:i−1 , x) = PK

(4.29)

Here F scans the observation sequence x and outputs the transcription
vector sequence f = (f1 , · · · fT ) = f1:T . G scans the label sequence
y0:T −1 and outputs the prediction vector sequence g = (g1 , · · · gT ) =
g1:T . y0 denotes the beginning symbol (<bos>) of the label sequence.
For a sequence labeling task with K possible labels, fi and gi are K
dimensional vectors. Superscript k is used to denote the k-th element of
the vectors. Remarkably, the prediction network G can be viewed as a
language model of labels, capable of modeling long-range dependencies in
y, which is exactly the motivation to introducing G in RNN transducers.
To ease comparison, we will also refer to the network below the
CRF layer in linear-chain NCRFs as a transcription network, which also
implement ϕt (yt = k, x) as ftk .
4.3.2

From RNN-T to CRF transducer

In the following, we introduce CRF transducers, which combine the
advantages of linear-chain NCRFs (globally normalized, using LSTMRNNs to implement node potentials) and of RNN transducers (capable
of capturing long-range dependencies in labels), and meanwhile overcome
their drawbacks, as illustrated in Table 4.3. Further, graphical model
representations of different models are plotted in Figure 4.11, which
clearly show the differences between those models.


126

Conditional EBMs with applications

Figure 4.11: Graphical model representations of (a) a linear-chain CRF, (b) a
RNN-T for the aligned setting, and (c) a CRF transducer. Notably, the graphical
representation of the RNN-T for the aligned setting, as defined in Eq. (4.28), is
different from that of the usual RNN-T as shown in Figure 4.10(c).

Model definition. A CRF transducer defines a globally normalized,
conditional distribution p(y|x; θ) as follows:
p(y|x; θ) =

exp {u(y, x; θ)}
.
Z(x; θ)

where Z(x; θ) = y′ ∈DT exp {u(y ′ , x; θ)} is the global normalizing term
and DT is the set of allowed label sequences of length T . The total
potential u(y, x; θ) is decomposed as follows:
P

u(y, x; θ) =

T
X

{ϕi (yi , x; θ) + ψi (y0:i−1 , yi ; θ)} .

i=1

where ϕi (yi , x; θ) is the node potential at position i, ψi (y0:i−1 , yi ; θ) is
the clique potential involving labels from the beginning up to position
i. Thus the underlying undirected graph for the label sequence y is
fully-connected, which potentially can capture long-range dependencies
from the beginning up to each current position.
Neural network architectures. Like in RNN transducers, we introduce
two networks in CRF transducers, as shown in Figure ??. The transcription network F implements the node potential ϕi (yi , x; θ), which
represents the score for yi based on observations x. In the experiments
on NLP sequence labeling, each word xi is represented by a concatenation of a pre-trained word embedding vector and another embedding
vector obtained from a character-level CNN (Hu et al., 2019). The
transcription network F is a bidirectional RNN (Rf ) that scans the sequence of the concatenated vectors for words to generate hidden vectors


4.3. CRFS FOR SEQUENCE LABELING IN NLP
𝒚𝟎

𝒚𝟏

𝒚𝒏−𝟏

𝒉𝟏

𝒈

𝒉𝟐

𝒈

𝒉𝒏

𝒈𝟏
𝒇𝟏

𝒈𝟐
𝒈
𝒇𝟐𝟏

𝒈𝒏
𝒇𝒏

𝒉𝟏

𝒇

𝒉𝟐

𝒇

𝒉𝒏

Observations 𝒙1

𝒙𝟐

𝒙𝒏

Labels
Prediction
network (𝑮)

𝒚𝒏

RNN (𝑹𝒈 )

Potentials

Transcription
network (𝑭)

127

𝒈

𝒇

Bi-RNN (𝑹𝒇 )

Figure 4.12: The architecture of a CRF transducer.

−
→ ←
−
hfi = [hfi ; hfi ], which are then fed to a linear layer with output size of
K to generate fi ∈ RK .
The prediction network G implements the clique potential ψi (y0:i−1 , yi ; θ),
which represents the score for yi by taking account of dependencies
between yi and previous labels y0:i−1 . In the experiments, each label
yi is represented by a label embedding vector, initialized randomly. G
is a unidirectional RNN (Rg ) that accepts the label sequence y and
−
→
generates hidden vectors hgi = hgi , which are then fed to a linear layer
with output size of K to generate gi ∈ RK .
It can be seen from above that a CRF transducer is similar to a RNN
transducer. The difference is that a RNN transducer is locally normalized
through softmax calculations as shown in Eq. (4.29), while a CRF
transducer is globally normalized, locally producing (un-normalized)
potential scores.

Potential design. Based on fi and gi , there are two possible designs
to implement the potentials ϕi and ψi , which can be chosen empirically
in the experiments. The first design is:
ϕi (yi = k, x; θ) = fik
ψi (y0:i−1 , yi = k; θ) = gik

(4.30)


128

Conditional EBMs with applications

The second design is:
exp(fik )
ϕi (yi = k, x; θ) = log PK
k′
k′ =1 exp(fi )

(4.31)

exp(gik )
ψi (y0:i−1 , yi = k; θ) = log PK
k′
k′ =1 exp(gi )

Decoding and training. CRF transducers break the first-order Markov
assumption in the label sequence as in linear-chain NCRFs and thus do
not admit dynamic programming for decoding. Instead, beam search
can be used to approximately find the most probable label sequence:
ŷ = arg max p(y ′ |x; θ) = arg max u(y ′ , x; θ).
y ′ ∈DT

y ′ ∈DT

Training data consists of inputs x paired with oracle label sequences
y ∗ . Stochastic gradient descent (SGD) on the negative log-likelihood of
the training data is conducted:
L(y ∗ ; θ) = −u(y ∗ , x; θ) + log Z(x; θ).
It is easy to calculate the gradient of the first term. However, the
gradient of the log normalizing term involves model expectation:
∇θ log Z(x; θ) = Ep(y′ |x;θ) ∇θ u(y ′ , x; θ)




The calculations of the normalizing term and the model expectation
can be exactly performed for linear-chain NCRFs (via the forward and
backward algorithm), but are intractable for CRF transducers. It is
empirically found in the experiments (Hu et al., 2019) that the method
of beam search with early updates (Collins and Roark, 2004) marginally
outperforms Monte Carlo based methods for training CRF transducers.
The basic idea is that we run beam search and approximate the
normalizing term by summing over the paths in the beam. Early updates
refer to that as the training sequence is being decoded, we keep track
of the location of the oracle path in the beam; If the oracle path falls
out of the beam at step j, a stochastic gradient step is taken on the
following objective:
∗
∗
L(y1:j
; θ) = −u(y1:j
; θ) + log

X
y ′ ∈Bj

n

′
exp u(y1:j
; θ)

o


4.3. CRFS FOR SEQUENCE LABELING IN NLP

129

where u(y1:j ; θ) = ji=1 {ϕi (yi , x; θ) + ψi (y0:i−1 , yi ; θ)} denotes the partial potential (with abuse of the notation of u). The set Bj contains all
∗ .
paths in the beam at step j, together with the oracle path prefix y1:j
P

Performance of CRF transducers. Different sequence labeling methods are evaluated over POS tagging, chunking and NER (English, Dutch)
in (Hu et al., 2019). Experiment results show that CRF transducers
achieve consistent improvements over linear-chain NCRFs and RNN
transducers across all the four tasks, and can improve state-of-the-art
results.


130
4.4

Conditional EBMs with applications
EBMs for conditional text generation

4.4.1

Residual energy-based models

Motivation
Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. In this section, we will
further introduce residual energy-based language models, as briefly
covered in Section 3.3.3, and show their application in (conditional)
text generation. The dominant approach to text generation is based on
autoregressive language models (ALMs), especially recent large ALMs
parameterized by large neural networks (Radford et al., 2018), which
are locally-normalized. Unfortunately, local normalization also brings
some drawbacks, as described in (Deng et al., 2020) and listed below.
First, the designer of the model needs to specify the order in which
tokens are generated. Second, at training time the model is conditioned
on ground truth context while at test time it is conditioned on its own
generations, a discrepancy referred to as exposure bias (Section 4.1.2).
Finally, while heuristics like beam search somewhat help rescore at the
sequence level, generation generally lacks long-range coherency because
it is produced by the greedy selection of one token at a time without
lookahead8 .
In principle, EBMs potentially address all these issues, as they do not
require any local normalization. EBMs are not prone to exposure bias
and label bias, and they can score the whole input at once. EBMs may
enable generation of large chunks of text, which should help improve
coherency. However, a difficulty of applying EBMs in text generation is
that sampling from EBMs is intractable, and so is maximum likelihood
training. A recent work in (Deng et al., 2020) develops residual EBMs
for text generation and shows impressive results, which will be detailed
below.

8

This drawback of generation without lookahead is related to the label bias
problem of locally-normalized sequence models, namely being weak in revising earlier
decisions (Section 4.1.2).


4.4. EBMS FOR CONDITIONAL TEXT GENERATION

131

The residual EBM model
The formulation of residual EBMs, as introduced in Section 3.3.3,
has multi-fold benefits for text generation (Deng et al., 2020). First,
by incorporating an autoregressive language model in the residual
formulation, we can leverage recent advancements in autoregressive
language modeling. Second, the autoregressive language model provides
a natural proposal distribution for training of residual EBMs, and the
training can be made efficient by using the conditional noise contrastive
estimation objective (Section 4.1.3). Lastly, this formulation enables
efficient evaluation and generation via importance sampling, as we
shall detail in the following. In some sense, this last point is perhaps
the central contribution of (Deng et al., 2020), as it allows estimating
perplexity of the residual EBM, and thus allows these EBMs to be
compared in a standard way to other models.
Deng et al., 2020 investigates an EBM trained on the residual of a
pre-trained autoregressive LM, particularly for conditional generation of
discrete sequences. Given a conditioning prefix x1 , · · · , xc with xj ∈ V
where V is the vocabulary, the probability of generating a sequence of
total length T > c is defined as follows:
pθ (xc+1 , · · · , xT |x1 , · · · , xc )
pLM (xc+1 , · · · , xT |x1 , · · · , xc ) exp(−Eθ (x1 , · · · , xc , xc+1 , · · · , xT ))
Zθ (x1 , · · · , xc )
p̃θ (xc+1 , · · · , xT |x1 , · · · , xc )
=
Zθ (x1 , · · · , xc )
(4.32)

=

where pLM denotes the pre-trained autoregressive LM and is fixed
throughout training, Eθ (·) is the residual energy function parameterized
by θ. In the experiment of (Deng et al., 2020), Eθ is initialized with
BERT/RoBERTa; in the final layer the mean-pooled hidden states
are projected to a scalar energy value. pθ is called the joint model.
Zθ (x1 , · · · , xc ) is the normalizing factor known as partition function. p̃θ
denotes the un-normalized probability.


132

Conditional EBMs with applications

Training of the residual EBM
The conditional EBMs defined in Eq. (4.32) can be trained using NCE
(Section 2.4), and more specifically its conditional version (Section
4.1.3). For the sake of reducing clutter in the notation, we will drop
the conditioning variables x1 , · · · , xc and use x to denote a target token
sequence (namely xc+1 , · · · , xT ) in the following discussion. Denote the
training dataset by D.
NCE requires two distributions: the model distribution and a noise
distribution. Here the model distribution is the joint model of Eq. (4.32),
pθ , while the noise distribution is the pre-trained LM, pLM . NCE then
trains a binary classifier on the difference of log-probability scores of
these two models. Since the joint model is the product of the energy
function (whose parameters we want to learn) with the pre-trained
LM, the difference reduces to: log p̃θ − log pLM = −Eθ . Therefore, under
these modeling assumptions of residual learning and noise model, the
NCE objective function becomes:

p̃θ (x+ )
pLM (x− )
+ E log
x
∼p
p̃θ (x+ ) + pLM (x+ )
p̃θ (x− ) + pLM (x− )
−
x+ ∼D
LM
1
1
+ E log
= E log
1 + exp(Eθ (x+ )) x− ∼pLM
1 + exp(−Eθ (x− ))
x+ ∼D
(4.33)
where x+ denotes the positive sequence taken from the human generated
training set, and x− the negative sequence drawn from the pre-trained
LM (for a given ground truth prefix). Again, we see that NCE training of
the energy function reduces to training a binary classifier to discriminate
between real text and text generated by a pre-trained autoregressive
language model. The experiment of (Deng et al., 2020) uses a prefix of
size 120 tokens and generates the following 40 tokens; with the notation
of Eq. (4.32), c = 120 and T = 160. For NCE training, for efficiency 16
samples per prefix for CC-News (Bakhtin et al., 2019) were generated
offline, and sampled uniformly from those samples at training time.
Jθ =

E log


4.4. EBMS FOR CONDITIONAL TEXT GENERATION

133

Algorithm 10 Top-k sampling for the residual EBM
Input: Number of samples n drawn from pLM , value of k in top-k
// Get a set of samples from pLM
Sample n samples {x1 , ..., xn } from pLM with top-k sampling
Calculate energies si = Eθ (xi ) for each xi ∈ {x1 , ..., xn }
// Resample from the set of LM samples
i)
Sample x = xi with probability Pnexp(−s
exp(−sj )
j=1

Return: x
Generation from the residual EBM
In order to generate from the residual EBM model Eq. (4.32) efficiently,
Deng et al., 2020 uses self-normalized importance sampling (SNIS)
(Section 2.3.2). Under the assumptions that the model from which we
wish to draw samples is the joint model, which is the product of the
autoregressive model and the energy function, and that the proposal
distribution is the autoregressive model itself, sampling proceeds simply
by: a) sampling from the autoregressive language model, followed by b)
resampling according to the energy function. The algorithm is shown
in Algorithm 10, where a top-k constraint on the pre-trained language
model is introduced to improve the quality of samples in the set. Without
the top-k constraint, as the number of samples goes to infinity, we would
recover exact samples from the joint model distribution.
Evaluation of the residual EBM
A commonly used protocol for evaluating generative sequence models,
especially language models, is perplexity (PPL), which is equal to
1
− T −c

PPL = 2

PT
i=c+1

log2 pθ (xi |x1 ,··· ,xi−1 )

PPL can be interpreted as the average number of tokens the model
is uncertain of at every time step. For the residual EBM model, the
log-likelihood required by PPL relies on estimating the partition function
Zθ =

X
x

pLM (x) exp(−Eθ (x)) = Ex∼pLM exp(−Eθ (x)).


134

Conditional EBMs with applications

Based on (Nowozin, 2018), two estimators are derived in (Deng et
al., 2020) for the lower and upper bounds of the partition function
respectively, as shown in the following theorem.
Theorem 4.1. Denote Tn as the empirical estimate of log Ex∼pLM exp(−Eθ (x))
P
with n samples xi ∼ PLM , i = 1, · · · , n: Tn = log n1 ni=1 exp(−E(xi )),
then ∀ϵ > 0, ∃N > 0 such that ∀n > N we have
Zθ − ϵ < E[Tn ] < Zθ < E[(2n − 1)Tn − 2(n − 1)Tn−1 ] < Zθ + ϵ. (4.34)
Similarly to locally normalized models, we can also factorize the
Q
probabilities of an entire sequence step by step, pθ (x) = Tt=1 pθ (xt |x<t ).
By marginalizing over the future, the following step-wise probabilities
are derived in (Deng et al., 2020):
pθ (xt |x<t ) = pLM (xt |x<t )

Ex′t+1:T ∼pLM (·|x≤t ) [exp(−Eθ (x≤t , x′t+1:T ))]

Ex′t:T ∼pLM (·|x≤t−1 ) [exp(−Eθ (x≤t−1 , x′t:T ))]
(4.35)
Note that both the numerator and the denominator in Eq. (4.35) take
the same form as the partition function, we can also use Eq. (4.34) to
estimate the upper and lower bounds. For example, the lower bound of
log pθ (xt |x<t ) can be obtained by using the lower bound of the numerator
and the upper bound of the denominator. Remarkably, for t = T , we
can calculate the log probability by exhaustive enumeration. This gives
us an idea of the true performance of the model at the last step, and it
also provides a sanity-check of the tightness of the estimators.

Performance of the residual EBM
In (Deng et al., 2020), experiments on two datasets, CC-News (Bakhtin
et al., 2019) and the Toronto Book Corpus (Zhu et al., 2015) show
that residual EBMs demonstrated improved generation ability against
strong autoregressive baselines, both in terms of estimated perplexity
and through human evaluation.


4.4. EBMS FOR CONDITIONAL TEXT GENERATION
4.4.2

135

Controlled text generation from pre-trained language models

Motivation
While large transformer-based autoregressive language models trained
on massive amounts of data exhibit exceptional capabilities to generate
natural language text, effective methods for generating text that satisfy global constraints and possess holistic desired attributes remains
an active area of research (Khalifa et al., 2021; Mireshghallah et al.,
2022). For instance, we may want to avoid toxic content; or steer generations towards a certain topic or style. Much of the prior work has
approached controlled generation via either training domain-conditioned
neural language models or finetuning/modifying an underlying large
pre-trained base model for attribute sensitive generation (see references
in Mireshghallah et al., 2022). Not only do these approaches involve
computational overhead and estimation errors associated with the training of language models, but they are also dependent on access to a large
amount of attribute-specific language data which can be impractical in
many scenarios and exacerbate privacy concerns.
In order to address these limitations, the study in (Mireshghallah
et al., 2022) thus aims to eschew training and focuses on generation-time
control from pre-trained modules. The mix-and-match method proposed
in (Mireshghallah et al., 2022), as shown in Figure 4.13, draws samples
from a test-time combination of pre-trained black-box experts that each
scores a desired property of output text - for example, fluency, attribute
sensitivity, or faithfulness to the context. Specifically, the product of
these black-box experts is viewed as a EBM, and sampling is performed
(without further training or fine-tuning), using a specialized Gibbs
sampler with a Metropolis-Hastings (MH) correction step (Goyal et al.,
2022), which is basically MH within Gibbs sampling, as introduced in
Section 2.3.1.
A related work in (Khalifa et al., 2021) proposes a distributional
approach for addressing controlled text generation from pre-trained
language models (PLMs). Both “pointwise” constraints (hard requirements on each individual) and “distributional” constraints (collective
statistical requirements such as moment constraints) are permitted to
added in the target LM, while minimizing KL divergence from the


136

Conditional EBMs with applications

initial pre-trained LM distribution. The optimal target distribution is
also uniquely determined as a residual EBM model and can be trained
through a variant of policy gradient based on importance sampling.
Also for controlled text generation, the work in (Qin et al., 2022)
is motivated by the need to enrich decoding algorithms that can work
directly with pre-trained language models (PLMs) without task-specific
fine-tuning, and support complex combinations of hard and soft constraints to control the generated text on the fly. Previous studies use
MH within Gibbs sampling or SNIS, as surveyed in Table 2.1. A new
decoding method, called Constrained decoding with Langevin dynamics
(COLD), is developed in (Qin et al., 2022), which introduces Langevin
dynamics to text-based EBMs for efficient gradient-based sampling.
Specifically, the COLD based text generation performs sampling by
iteratively updating a continuous relaxation of text using gradients of
the energy function. The resulting continuous text samples are then
mapped back to the discrete space with a simple guided discretization approach, yielding text sequences that are fluent and adhere to
the constraints. Experiments are conducted in three text generation
applications - lexically-constrained generation, abductive reasoning,
and counterfactual reasoning and show the effectiveness of the COLD
approach, both in terms of automatic and human evaluation.
The mix-and-match language model
In (Mireshghallah et al., 2022), the problem of performing controlled
generation is framed as a problem of sampling from a specialized energy
based (or globally normalized) sequence model that defines a probability
distribution that satisfies the desired constraints we wish to impose in
the controlled generation setting.
As described below, this energy based model is composed of pretrained components and does not require any further optimization.
Specifically, an energy-based sequence model defines the probability
distribution over the space of possible sequences X as:
pθ (X) = P

e−Eθ (X)
−Eθ (X ′ )
X ′ ∈X e


4.4. EBMS FOR CONDITIONAL TEXT GENERATION

137

Figure 4.13: Overview of mix-and-match LM. The Lego pieces show different
experts that can be used to form the energy LM and help control different features
in the generated text. The right side shows the i-th step in the the Gibbs sampling
chain, where a proposal is made by the MLM, and then it is accepted/rejected based
on the energy score. (Mireshghallah et al., 2022)

where Eθ (X) refers to the scalar energy of a sequence X that is
parametrized by θ. Lower energy corresponds to the higher likelihood
of X. In contrast to the common autoregressive sequence models, exact likelihood computation and efficient sampling from EBM models
is challenging. Despite these challenges, the EBM paradigm offers increased flexibility via sequence-level features and constraints. As we
discuss next, this capability lets us easily define expressive functions for
controlled generation of sequences which is not readily offered by the
autoregressive modeling paradigm.
Note that the task of controlled generation requires concentrating
probability mass over a small subspace of sequences in X that satisfies
various constraints pertaining to fluency, target attributes, and other
control variables. The EBM based mix-and-match language model, as
defined below, involves a linear combination of various black-box experts
in order to obtain a distribution whose samples satisfy the requirements
of a desired controlled generation task:
EM&M (X) =

k
X

αi Ei (X)

(4.36)

i=1

where the mix-and-match (M&M) energy is composed of k expert
energy components, which are weighted by scalar hyperparameters α.


138

Conditional EBMs with applications

The following black-box experts are used in (Mireshghallah et al., 2022):
• Emlm (X): Masked language models (MLMs) like BERT (Devlin
et al., 2018) are used as a black-box to model the form and fluency
of sentences. Particularly, Emlm (X) is defined as the negative
of the sum of unnormalized logits iteratively computed at each
position obtained via the forward pass of the MLM after masking
the corresponding position (Goyal et al., 2022).
• Edisc (X): This particular expert refers to the energy obtained
via the discriminator for the attributes of interest. What this
module returns is the raw logits of the discriminator, for the target
attribute. For instance, if we have a sentiment classifier, and want
to produce positive sentiment, then Edisc (X) = − log p(+|X).
• Ehamm (X, X ′ ): For a given sequence X ′ , this quantity refers to the
hamming distance between the sequence X and X ′ . This penalizes
token level deviation from X ′ which is useful if we are interested
in only making minor edits to X ′ .
• Efuzzy (X, X ′ ): Similar to the hamming distance, this quantity
refers to the BertScore (Zhang et al., 2020a) computed between
X and X ′ which can be viewed as a fuzzy hamming distance that
takes semantic similarity into account.
Sampling from the mix-and-match model
In (Mireshghallah et al., 2022), Metropolis-Hastings (MH) based MCMC
scheme (Section 2.3.1) is used to sample from the M&M model. The
proposal distribution is implemented by a masked language model
(MLM) like BERT. At each MH step, we mask out a token at a random
position i in current sequence X, propose a new token by sampling from
the MLM conditional softmax at the masked position9 , and obtain the
new sequence X. The new sequence is accepted with the probability
(

min 1,
9

exp(−EM&M (X))pmlm (Xi |X\i )

)

exp(−EM&M (X))pmlm (X i |X\i )

Note that the proposed move X i is generated independent of the previous state
Xi , thus the sampling algorithm used here is in fact MIS with Gibbs sampling.


4.4. EBMS FOR CONDITIONAL TEXT GENERATION

139

In experiments in (Mireshghallah et al., 2022), a MCMC chain is run
for sentence generation for more than 8 epochs, where an epoch refers
to one masking cycle over all positions of the sequence.
The performance of the mix-and-match model
Two kinds of controlled generation tasks are performed in (Mireshghallah
et al., 2022).
Prompted generation. This task focuses on generating well-formed
sentences that start with a specified prompt and also satisfy a target
attribute for which we have access to a discriminator. An example task
would be to generate positive sentiment sequences starting with This
movie. The energy function takes the form:
E(X) = Emlm (X) + αEdisc (X)
where α is a hyperparameter that controls the tradeoff between the
MLM score and the discriminator’s influence.
Controlled text revision. This task aims to edit a source sequence
X ′ to generate sequence X to satisfy the desired target attributes. The
energy function for this task is :
E(X) = Emlm (X) + αEdisc (X) + βEhamm (X, X ′ ) + γEfuzzy (X, X ′ )
This energy function, in addition to valuing well-formedness and satisfying target attribute requirements, also focuses on maintaining faithfulness to the source sequence X ′ .
In (Mireshghallah et al., 2022), the effectiveness of the mix-andmatch approach has been shown on controlled generation tasks (with
sentiment or topic) and style-based text revision tasks (controllable
debiasing, style transfer), by outperforming recently proposed methods
that involve extra training, fine-tuning, or restrictive assumptions over
the form of models.


140

Conditional EBMs with applications

Constrained decoding with Langevin dynamics (COLD)
Constrained text generation aims to produce text samples y that satisfy
a set of constraints (usually conditioned on an input x omitted for
brevity). Let y = (y1 , · · · , yT ) denote a discrete sequence where each yt
is a token from a vocabulary V. Assume each constraint can be captured
by a constraint function fi (y) ∈ R, where higher values of fi mean that
the text y better satisfies the constraint. Generating text under the
constraints can be seen as sampling from a energy-based distribution
y ∼ p(y):
p(y) ∝ exp {U (y)} , U (y) =

X

λi fi (y)

i

EBMs are flexible - any differentiable function that outputs a goodness
score of text can be used as a constraint function, as long as it reflects the
requirements of the target task. Three example constraints are shown
in (Qin et al., 2022): soft fluency constraint, future-token prediction
constraint, and N-gram similarity constraint.
For efficient sampling from p(y), we want to use Langevin dynamics,
which makes use of the gradient ∇y log p(y) = ∇y U (y). However, in
text generation, y is a discrete sequence and the gradient ∇y log p(y) is
not well-defined. To address this problem, (Qin et al., 2022) proposed
a new sampling/decoding method, called Constrained decoding with
Langevin dynamics (COLD), which consists of three steps - continuous
relaxation of text, performing Langevin dynamics with an energy defined
on a sequence of continuous “soft” token vectors, and finally, guided
discretization.
Continuous relaxation of text. Instead of defining the energy function
on discrete tokens, the energy function is defined on a sequence of
continuous vectors ỹ = (ỹ1 , · · · , ỹT ), which is called a soft sequence.
Each position in the soft sequence is a vector of logits ỹt ∈ R|V| , each
element corresponding to the logit of a word in the vocabulary. Taking
the softmax of ỹt yields a distribution over the vocabulary for position
t. An EBM model can be defined on the soft sequence ỹ as follows:
p(ỹ) ∝ exp {U (ỹ)}

(4.37)


4.4. EBMS FOR CONDITIONAL TEXT GENERATION

141

Langevin dynamics over soft tokens. In the continuous space, we
can perform gradient guided MCMC (Section 2.3.1) such as Langevin
dynamics to generate samples ỹ (1) , ỹ (2) , · · · , which ends up generating
samples from the distribution induced by the energy function Eq. (4.37).
From soft tokens to discrete text. After receiving a soft sequence
sample ỹ from running Langevin dynamics, we map the soft sequence
to a discrete text sequence, which we consider as the output of COLD
decoding. A simple method would be selecting the most-likely token at
each position t,
yt = arg max ỹt (v)
v∈V

However, the resulting text suffers from fluency issues even if the soft
fluency constraint is used, due to competing constraints that sacrifice
fluency. To overcome this, COLD uses the underlying pre-trained language model (PLM) as a “guardian” for obtaining the discrete sequence.
Specifically, at each position t, we first use the PLM to produce the
top-k most-likely candidate tokens based on its generation distribution
conditioning on preceding tokens, which we denote as Vtk . We then
select from the top-k candidates the most likely token based on the soft
sample ỹ:
yt = arg max ỹt (v)
v∈Vtk

We refer to this method as “top-k filtering”. The resulting text tends to
be fluent because each token is among the top-k most probable tokens
from the PLM.


5
Joint EBMs with applications

In this chapter, we introduce EBMs for modeling joint distributions for
both fixed-dimensional and sequential data, with the applications for
semi-supervised learning, training more calibrated models, and improved
language modeling with additional relevant linguistic tags (e.g., part-ofspeech tags). First, we present basics for semi-supervised learning. Then,
we introduce the fixed-dimensional case of joint EBMs (Section 5.2),
then move on to the sequential case (Section 5.3). Finally, we present the
application of EBM-based joint modeling for semi-supervised learning
and calibrated natural language understanding in Section 5.4 and Section
5.5, respectively.
5.1

Basics for semi-supervised learning

As we have witnessed, supervised learning from large amounts of labeled data, particularly with deep neural networks (DNNs), has achieved
tremendous success in various intelligence tasks, spanning speech processing, computer vision, and natural language processing (NLP). However,
collecting labeled data is difficult and expensive, but there are often
easily-available unlabeled data. This has motivated the community to
develop semi-supervised learning (SSL). SSL aims to leverage both la142


5.1. BASICS FOR SEMI-SUPERVISED LEARNING

143

beled and unlabeled data to train a conditional model for inference (for
either discriminative or generation tasks), which, basically, is to find the
posteriori of label y given observation x. A plethora of semi-supervised
learning methods have emerged to train deep neural networks (DNNs)
(Miyato et al., 2018; Laine and Aila, 2017; Tarvainen and Valpola, 2017;
Sohn et al., 2020; Chen et al., 2020; Clark et al., 2018), spanning over
various domains such as image classification, natural language labeling
and so on. People may wonder how unlabeled observations x’s may
help finding the posterior. A common belief is that the the information
contained in the unlabeled observations can provide some kind of priors,
or alternatively say, some regularizations or inductive biases, for finding
the posterior p(y|x).
Roughly speaking, recent SSL methods with DNNs can be distinguished by the priors they adopt, and, can be divided into two classes1
- based on generative models or discriminative models, which are referred to as generative SSL and discriminative SSL, respectively. An
intuitive way to differentiate between generative and discriminative
SSL is that generative SSL typically requires modeling the marginal
distribution of the data, while discriminative SSL only involves modeling
the conditional distribution.
5.1.1

Discriminative SSL

Discriminative SSL methods often assume that the outputs from the
discriminative classifier are smooth with respect to local and random
perturbations of the inputs. Examples include virtual adversarial training (VAT) (Miyato et al., 2018) and a number of recently developed
consistency-regularization based methods such as temporal ensembling
(Laine and Aila, 2017), mean teachers (Tarvainen and Valpola, 2017),
FixMatch (Sohn et al., 2020) and contrastive learning based methods
such as SimCLR (Chen et al., 2020).
Discriminative SSL methods thus heavily rely on domain-specific
data augmentations, e.g., RandAugment (Cubuk et al., 2020), which are
tuned intensively for images and lead to impressive performance in some
1

We mainly discuss the SSL methods for using DNNs. General discussion of SSL
can be referred to (Zhu, 2006).


144

Joint EBMs with applications

image domains. But discriminative SSL is often less successful for other
domains, where these augmentations are less effective (e.g., medical
images and text). For instance, random input perturbations are more
difficult to apply to discrete data like text (Clark et al., 2018). Although
there are some efforts to use data-independent model noises, e.g., by
dropout (Srivastava et al., 2014), domain-specific data augmentation is
indispensable.
5.1.2

Generative SSL

Generative SSL methods exploit unsupervised learning of generative
models over unlabeled data, which inherently does not require data
augmentations and generally can be applied to a wider range of domains. Generative SSL usually involves blending unsupervised learning
and supervised learning. These methods make fewer domain-specific
assumptions and tend to be domain-agnostic. The performance comparisons between generative and discriminative SSL methods are mixed.
It is found that consistency based discriminative SSL methods often
outperform generative SSL methods in image domain. However, in text
domain, the generative SSL methods such as those based on pre-trained
word vectors (Mikolov et al., 2013; Pennington et al., 2014) and pretrained language models (PLMs) (Radford et al., 2019; Raffel et al.,
2020) are more successful and widely used.
Considering observation x and label y, there exist two different
methods for the generative SSL approach - joint-training (Larochelle
et al., 2012; Kingma et al., 2014) and pre-training (Hinton et al., 2006).
• In joint-training, a joint model of p(x, y) is defined. When we have
label y, we maximize p(y|x) (the supervised objective), and when
the label is unobserved, we marginalize it out and maximize p(x)
(the unsupervised objective). Semi-supervised learning over a mix
of labeled and unlabeled data is formulated as maximizing the
(weighted) sum of log p(y|x) and log p(x). Given infinite model
capacity and data, the joint-learning based SSL is consistent and
can find the oracle p(y|x)2 .
2

This is because that the maximum likelihood estimator is consistent.


5.1. BASICS FOR SEMI-SUPERVISED LEARNING

145

Figure 5.1: An overview of SSL and a general categorization of generative SSL
methods. Examples are mainly chosen from NLP.

• In pre-training, we perform unsupervised representation learning on unlabeled data, which is followed by supervised training
(called fine-tuning) on labeled data. Thus, pre-training is usually
based on a model that only defines the marginal distribution p(x)
without the need to involve y. Its empirical effectiveness is mostly
understood from the perspective of representation learning. There
is no theoretical guarantee that this kind of pre-training will lead
to finding the oracle p(y|x). This method of pre-training followed
by fine-tuning has been widely used in natural language processing
(Radford et al., 2018)3 .
A categorization of generative SSL methods is shown in Figure
5.1. For either of joint-training and pre-training, we can use directed
models (locally-normalized) or undirected models (globally-normalized).
So there are four main classes for generative SSL. The models used in
joint-training could be latent-variable model (LVM) such as in LABES
(Zhang et al., 2020b), or JRF (Song et al., 2020) or say JEM (Zhao
et al., 2020). Pre-training could be based on masked language models
3

But, more recently in NLP, an approach of jointly modeling the input, the
output, and the task description, has emerged to gain more attention and achieved
superior performances (Radford et al., 2019). The input, the output, and the task
description can all be specified as a sequence of tokens, and a language model is
trained for estimating natural language sequences so that p(task, input, output) for
various tasks, inputs and outputs are implicitly trained.


146

Joint EBMs with applications

(Devlin et al., 2018), autoregressive language models (Radford et al.,
2018), or random-field language models (Wang et al., 2018). Further,
two approaches of pre-training and joint-training could be combined or
compared to each other. So there are many open questions in designing
semi-supervised methods for particular tasks.
EBM based SSL. Among existing generative SSL methods, energybased models (EBMs), as an important class of generative models,
have been shown with promising results for semi-supervised learning
across various domains. Early studies date back to the pre-training of
restricted Boltzmann machines (RBMs) (Hinton et al., 2006) (which
are a simple type of EBMs) and the joint-training with classification
RBMs (Larochelle et al., 2012).
Recently, it is shown in (Song and Ou, 2018) that joint-training via
EBMs produces state-of-the-art SSL results on images (MNIST, SVHN
and CIFAR-10), compared to previous generative SSL methods based on
Variational AutoEncoders (VAEs) and Generative Adversarial Networks
(GANs). It is also shown in (Zhao et al., 2020) that joint-training
via EBMs outperforms VAT (virtual adversarial training) (Miyato et
al., 2018) on tabular data from the UCI dataset repository. Further,
joint-training via EBMs has been extended in (Song et al., 2020) to
modeling sequences and consistently outperforms conditional random
fields (CRFs) (the supervised baseline) and self-training (the classic semisupervised baseline) on natural language labeling tasks such as POS
(part-of-speech) tagging, chunking and NER (named entity recognition).
Note that although both joint-training and pre-training of EBMs
have been used for SSL in the literature, very few studies evaluated
and compared the two methods. The results from previous individual
works are often not directly comparable to each other, since they are not
evaluated in a common experimental setup. In (Song et al., 2021), a suite
of experiments are conducted to systematically compare joint-training
and pre-training for EBM-based SSL. Both the amounts of labeled
and unlabeled data are varied to give a realistic whole picture of the
performances of the two methods for SSL (Oliver et al., 2018). It is found
that joint-training EBMs outperform pre-training EBMs marginally
but nearly consistently, presumably because the optimization of joint-


5.2. UPGRADING EBMS TO JOINT EBMS (JEMS) FOR
FIXED-DIMENSIONAL DATA

147

training is directly related to the targeted task, while pre-training does
not.
In the remainder of this chapter, we will detail EBM based SSL,
including both pre-training and joint-training. Note that pre-training
only involves the marginal distribution p(x), so the basics for EBM
based pre-training can be referred to Section 3 for learning with discrete
x such as natural languages, and for learning with continuous x such as
images, be referred to (Song and Ou, 2018). We will mainly introduce
the basics for EBM based joint-training, i.e., establishing EBMs for
modeling joint distributions, first in the fixed-dimensional case (Section
5.2) and then in the sequential case (Section 5.3). Although speech and
language processing is primarily concerned with the sequential case, the
introduction of the fixed-dimensional case can lay the foundation for
understanding the more complicated, sequential case.
5.2

Upgrading EBMs to Joint EBMs (JEMs) for fixed-dimensional
data

Motivation. Originally, EBMs are established for modeling the marginal
distribution p(x) of observations x, as introduced in Section 3. Recently,
a kind of EBM, called semi-supervised EBM, for modeling the joint distribution of observation x and label y has been developed and used for
semi-supervised classification (Song and Ou, 2018). In (Grathwohl et al.,
2020), a similar kind of EBM has been studied, which is called joint EBM
(JEM). It is established by reinterpreting a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y).
It is demonstrated that energy based training of the joint distribution
improves calibration, robustness, and out-of-distribution detection while
also enabling sample generation which rivals the quality of recent GAN
approaches. Hereafter, these EBMs are collectively referred to as JEMs.
Model definition. Note that different models are needed in unsupervised and semi-supervised learning, because SSL needs to additionally
consider labels apart from observations.
In semi-supervised tasks, we consider the following EBM for joint


148

Joint EBMs with applications

modeling of observation x ∈ Rdx and class label y ∈ {1, · · · , K}:
pθ (x, y) =

1
exp [Uθ (x, y)] .
Z(θ)

(5.1)

This is different from Eq. (2.13) for unsupervised learning which only
models x without labels. To implement the potential function Uθ (x, y),
we consider a neural network Ψθ (x) : Rdx → RK , with x as the input
and the output size being equal to the number of class labels, K. Then
we define
Uθ (x, y) = onehot(y)T Ψθ (x)
where onehot(y) represents the one-hot encoding vector for the label y.
In this manner, the conditional density pθ (y|x) is the classifier, defined
as follows:
pθ (x, y)
exp [Uθ (x, y)]
(5.2)
pθ (y|x) =
=P
′
pθ (x)
y ′ exp [Uθ (x, y )]
which acts like multi-class logistic regression using K logits calculated
from x by the neural network Ψθ (x). And we do not need to calculate
Z(θ) for classification.
With the definition the joint density in Eq. (5.1), it can be shown
that, with abuse of notation, the marginal density is
pθ (x) =
where Uθ (x) ≜ log

1
exp [Uθ (x)]
Z(θ)

(5.3)

′
y ′ exp [Uθ (x, y )].

P

Model learning. Suppose that among the data D = {x1 , · · · , xn },
only a small subset of the observations, for example the first m observations, have class labels, m ≪ n. Denote these labeled data as
L = {(x1 , y1 ), · · · , (xm , ym )}. Let pemp denote the empirical distribution over D. Then we can formulate the semi-supervised learning as
optimizing



min KL [pemp (x)||pθ (x)] − αd
θ



X
(x,y)∼L




log pθ (y|x)

(5.4)



which are defined by hybrids of generative and discriminative criteria,
similar to (Zhu, 2006; Larochelle et al., 2012; Kingma et al., 2014). The


5.3. UPGRADING CRFS TO JOINT RANDOM FIELDS (JRFS) FOR
SEQUENTIAL DATA
149
hyper-parameter αd controls the relative weight between generative and
discriminative criteria.
Once the JEM model and the training objective are established, the
inclusive-NRF training algorithm (Algorithm 7) developed in (Song and
Ou, 2018) can be applied. Other algorithms for training EBMs, which
can achieve proper density estimation such as those recently developed
in (Zhang et al., 2023), can also be employed.
5.3

Upgrading CRFs to Joint random fields (JRFs) for sequential
data

Motivation. Probabilistic generative modeling is a principled methodology that promisingly can learn from data of various forms (whether
labeled or unlabeled) to benefit downstream tasks, which, however, is
particularly challenging for sequence data. Two important sequence
tasks are sequence modeling and sequence labeling.
A basic problem of sequence modeling is to determine the probabilities of sequences. An example is language modeling (Chen and
Goodman, 1999), which, as described in Section 3.2, is a crucial component in many speech and language processing tasks. For sequences
of length l, xl ≜ x1 , x2 , ..., xl , this amounts to calculate p(l, xl ), where
we make explicit the role of the length l. Ideally, this density modeling
can be improved with additional relevant labels. e.g. incorporating partof-speech (POS) tags for language modeling. There are some previous
studies in (Rosenfeld et al., 2001; Sarikaya et al., 2010). The difficulty
is that the labels (e.g. POS) usually are not available in testing, so
a standalone POS tagger is needed to provide hypothesized labels in
testing.
The task of sequence labeling, as described in Section 4.3, is, given
observation sequence xl , to predict the label sequence y l ≜ y1 , y2 , ..., yl ,
with one label for one observation at each position. Sequence labeling has
been widely applied in various tasks, e.g., POS labeling (Collobert et al.,
2011; Ling et al., 2015), named entity recognition (NER) (Huang et al.,
2015; Lample et al., 2016; Ma and Hovy, 2016), and chunking (Huang
et al., 2015; Søgaard and Goldberg, 2016). As introduced in Section
5.1.2, it is desirable for the labeling model to leverage both labeled


150

Joint EBMs with applications

data (namely pairs of xl and y l ) and unlabeled data (namely xl without
labels), i.e., to conduct semi-supervised learning (SSL). Pre-training
has proved to be effective (Collobert et al., 2011; Devlin et al., 2018),
which, however, is task-independent followed by task-dependent finetuning. Besides pre-training, it is worthwhile to explore task-dependent
semi-supervised learning (SSL) in the manner of joint-training, which
learns for a task on a mix of labeled and unlabeled data. Self-training
is such a method with limited success (Scudder, 1965).
As introduced in Section 4.1, conditional random fields (CRFs)
(Lafferty et al., 2001) have been shown to be one of the most successful
approaches to sequence labeling. A CRF is a discriminative model,
which directly defines a conditional distribution p(y l |xl ), and thus
mainly depends on supervised learning with abundant labeled data.
It is proposed in (Song et al., 2020) to upgrade CRFs and obtain a
joint generative model of xl and y l , p(l, xl , y l ), called joint random
fields (JRFs). Specifically, the potential function U (xl , y l ) in the original
CRF p(y l |xl ) is borrowed as the potential function that defines a joint
distribution p(xl , y l ). This upgrading, operated in the sequential setting,
is similar to upgrade EBMs to JEMs in the fixed-dimensional setting
(Song and Ou, 2018; Grathwohl et al., 2020). Similar to the fixeddimensional setting, the conditional distribution of y l given xl induced
from this joint distribution is exactly the original conditional distribution
- the CRF p(y l |xl )4 ; and the marginal distribution of p(l, xl ) induced
from the joint distribution is a trans-dimensional random field (TRF)
language model (Wang et al., 2018; Wang and Ou, 2018a), as described
in Section 3.2.2.
This development from CRFs to JRFs benefits both modeling and
labeling of sequence data. For sequence modeling, the marginal likelihood p(l, xl ) can be efficiently calculated by JRFs, without the step
of producing hypothesized labels by a standalone POS tagger. For sequence labeling, JRFs admit not only supervised learning from labeled
data by maximizing the conditional likelihood p(y l |xl ) (which is like the
training of a CRF), but also unsupervised learning from unlabeled data
So writing the JRF as p(l, xl , y l ) and the CRF as p(y l |xl ) is correct, not an
abuse of notation.
4


5.3. UPGRADING CRFS TO JOINT RANDOM FIELDS (JRFS) FOR
SEQUENTIAL DATA
151
by maximizing the marginal likelihood p(l, xl ) (which is like the training of a TRF LM), thereby achieving task-dependent semi-supervised
learning.
Model definition. We will first briefly review linear-chain CRFs, as
described in Section 4.1.1, but with different notations, which facilitate
the introduction of JRFs. A linear-chain CRF defines a conditional
distribution with parameters θ for label sequence y l given observation
sequence xl of length l:
pθ (y l |xl ) =

1
exp(Uθ (xl , y l ))
Zθ (xl )

(5.5)

Here the potential function
Uθ (xl , y l ) =

l
X

ϕi (yi , xl ) +

i=1

l
X

ψi (yi−1 , yi , xl ),

(5.6)

i=1

is defined as a sum of node potentials and edge potentials, and Zθ (xl ) =
P
l l
l
y l exp(Uθ (x , y )) is the normalizing constant. ϕi (yi , x ) is the node
potential defined at position i, which, in recently developed neural CRFs
(Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Ma
and Hovy, 2016) is implemented by using features generated from a
neural network (NN) of different network architectures. ψi (yi−1 , yi , xl )
is the edge potential defined on the edge connecting yi−1 and yi , often
implemented as a matrix A, ψi (yi−1 = j, yi = k, xl ) = Aj,k , thereby
defines a linear-chain CRF. In linear-chain CRFs, there are efficient
algorithms for training (the forward-backward algorithm) and decoding
(the Viterbi algorithm).
Inspired from the idea of jointly modeling fixed-dimensional observations (e.g. images) and labels via JEMs in (Song and Ou, 2018;
Grathwohl et al., 2020), CRFs can be similarly upgraded and a joint
distribution over sequential observations and labels can be established,
called JRFs (Song et al., 2020). The keypoint is that we can use Uθ (xl , y l )
in Eq. (5.6) from the original CRF to define a joint distribution pθ (xl , y l ):
pθ (l, xl , y l ) = πl pθ (xl , y l ; l) =

πl
exp(Uθ (xl , y l ))
Zθ (l)

(5.7)


152

Joint EBMs with applications

where πl is the empirical prior probability for length l. Notably, a CRF is
a conditional distribution, normalizing over label sequences. In contrast,
a JRF is a joint distribution, normalizing over both observation and
label sequences, with the normalizing constant for length l defined as
Zθ (l) =

exp(Uθ (xl , y l )).

X

(5.8)

xl ,y l

Interestingly, it can be easily seen that the conditional distribution
of y l given xl induced from the JRF’s joint distribution Eq. (5.7) is
exactly the original CRF Eq. (5.5). Further, by marginalizing out y l ,
the marginal distribution of p(l, xl ) induced from the joint distribution
is:
πl
πl X
exp(Uθ (xl , y l )) =
exp(Uθ (xl ))
pθ (l, xl ) =
Zθ (l) l
Zθ (l)
y

which acts like a trans-dimensional random field (TRF) language model
(Wang et al., 2018; Wang and Ou, 2018a), with the potential defined by
Uθ (xl ) = log

X

exp(Uθ (xl , y l )).

yl

Notably this marginal potential Uθ (xl ) is exactly the normalizing constant log Zθ (xl ) Eq. (5.8) from the CRF. It can be calculated via the
forward algorithm from the linear-chain potential Uθ (xl , y l ).
Model learning. Given different data resources (labeled or unlabeled),
JRF can be trained under different settings (supervised, unsupervised,
or semi-supervised) and applied in different downstream tasks (sequence
modeling or labeling), as illustrated in Fig. 5.2. Note that the embedded
CRF and TRF inside a JRF share all parameters θ, which is different
from multi-task learning where only bottom-level parameters are shared
(Argyriou et al., 2007).
Supervised learning of JRFs amounts to the training of the embedded
CRF with the following supervised objective, given labeled data in the
form of empirical distribution plab (xl , y l ),
max Jsup (θ) = E(xl ,yl )∼plab (xl ,yl ) [log pθ (y l |xl )]
θ

(5.9)


5.3. UPGRADING CRFS TO JOINT RANDOM FIELDS (JRFS) FOR
SEQUENTIAL DATA
153

Figure 5.2: Overview of the JRF model. The node and edge potentials define a
JRF (a joint distribution over xl and y l ). Inducing the conditional and the marginal
from the joint yields a CRF and a TRF respectively. A JRF can be trained from
labeled data (acting like a CRF) and also from unlabeled data (acting like a TRF).
In practice, the node potentials are calculated from the logits oi , i = 1, · · · , l, from
the NN, and the edge potential follows a linear-chain definition.


154

Joint EBMs with applications

which can be solved by applying minibatch-based stochastic gradient
descent (SGD). At each iteration, a minibatch of sentences and labels is
sampled from plab (xl , y l ), denoted by Dlab , and the stochastic gradients
are:
1
∂Jsup (θ)
=
∂θ
|Dlab |

"

V

X
(xl ,y l )∈Dlab

∂Uθ (xl , y l ) ∂Uθ (xl )
−
∂θ
∂θ

#

Unsupervised learning of JRFs amounts to the training the embedded
TRF, by applying the dynamic noise-contrastive estimation (DNCE)
algorithm developed in (Wang and Ou, 2018a). Given unlabeled data
(e.g. sentences) in the form of empirical distribution punl (l, xl ), DNCE
jointly optimizes over a JRF and a noise distribution pϕ (l, xl ) (generally
a LSTM language model) parameterized by ϕ:

"
#
l)

p
(l,
x

θ

maxE
log
+

p
(l,xl )+pϕ (l,xl )


θ
pθ (l, xl ) + pϕ (l, xl )
(l,xl )∼ unl

2


#
"

l

pϕ (l, x )

E(l,xl )∼pϕ (l,xl ) log


pθ (l, xl ) + pϕ (l, xl )



h
i



l
l

 minKL punl (l, x )||pϕ (l, x )

≜ Juns (θ)

(5.10)

ϕ

Thanks to optimization of DNCE, the annoying normalizing constants
Zθ (l) in JRFs can be jointly estimated along with the parameter estimation. Specifically, we introduce parameters ζl for log Zθ (l) and
ζ = (ζ1 , ζ2 , ..., ζL ), where L is a pre-defined maximum length. Hereafter,
we denote by ξ = (θ, ζ) all the parameters in the JRF and rewrite pθ (·)
as pξ (·).
At each iteration, a minibatch of sentences are sampled from punl (l, xl ),
denoted by Dunl , two minibatches of sentences sampled from pϕ (l, xl ),
denoted by B1 , B2 (|B2 | = 2|B1 | = 2|Dunl |), and the stochastic gradients


5.3. UPGRADING CRFS TO JOINT RANDOM FIELDS (JRFS) FOR
SEQUENTIAL DATA
155
are:

X

pξ (l, xl )
1

 ∂Junl (ξ) =
g(l, xl ; ξ)

∂ξ

l ) + p (l, xl )

|B
|
p
(l,
x
2
ξ
ϕ


(l,xl )∈B2




X

pϕ (l, xl )
1
V

+

|Dunl | + |B1 |

pξ (l, xl ) + pϕ (l, xl )

g(l, xl ; ξ)



(l,xl )∈Dunl ∪B1





X

∂ log pϕ (l, xl )
 ∂KL(punl ||pϕ ) = − 1


∂ϕ


|Dunl | l
∂ϕ
V

(l,x )∈Dunl

where g(l, xl ; ξ) denotes the gradient of log pξ (l, xl ) w.r.t. ξ = (θ, ζ),
and the two gradient components w.r.t. θ and ζ are ∂Uθ (xl )/∂θ and
−(δ(l = 1), ..., δ(l = L)) respectively.
Semi-supervised learning of JRFs over a mix of labeled and unlabeled
data amounts to combining the above supervised and unsupervised
training with the following semi-supervised objective:

J(ξ) = Jsup (ξ) + αJuns (ξ)

max
ξ
h
i

 minKL punl (l, xl )||pϕ (l, xl )

(5.11)

ϕ

where α is the trade-off weight between supervised and unsupervised
learning, and ξ = (θ, ζ) is defined before.
The performance of the JRF model. The benefits of JRFs to sequence
modeling and labeling are demonstrated through two sets of experiments
in (Song et al., 2020). First, various traditional language models (LMs)
such as Kneser-Ney (KN) smoothed n-gram LM (Chen and Goodman,
1999), LSTM LM (Zaremba et al., 2014) and TRF LM (Wang and
Ou, 2018a) are trained on Wall Street Journal (WSJ) portion of Penn
Treebank (PTB) English dataset (without using POS tags). JRF LMs
are trained by using POS tags. These models are then used to rescore
the 1000-best list generated from the WSJ’92 test set, with similar
experimental setup as in (Wang and Ou, 2018a). The JRF model is
effective in incorporating POS tags and performs the best with the
lowest rescoring word error rate (WER). Second, experiments on three
sequence labeling tasks - POS tagging, chunking and NER, with Google


156

Joint EBMs with applications

Table 5.1: Applications of EBMs across different domains: comparison and connection (See text for details).

Observation
Label
Pre-training
Joint-training

Image classification

Natural language labeling

x ∈ RD
continuous, fixed-dimensional
y ∈ {1, 2, · · · , K}
Uθ (x) = wT h
Uθ (x, y) = Ψθ (x)[y]

x ∈ l Vl
discrete, sequence
S
y ∈ l {1, 2, · · · , K}l
Uθ (x) in Eq. (5.12)
Uθ (x, y) in Eq. (5.16)
S

one-billion-word dataset (Chelba et al., 2014) as the unlabeled data
resource, are conducted. It is found that the JRF based SSL consistently
outperforms the CRF baseline and self-training.
5.4

JEMs and JRFs for semi-supervised learning

Now we have introduced the basics for establishing JEMs and JRFs for
fixed-dimensional and sequential data in Section 5.2 and Section 5.3,
respectively. As introduced in Section 5.1.2, there exist two different
methods for EBM based SSL - pre-training and joint-training. Pretraining of RBMs once received attention in the early stage of training
DNNs (Hinton et al., 2006). Encouraging SSL results have been shown
recently for EBM based joint-training. In (Song and Ou, 2018; Zhao
et al., 2020; Song et al., 2020), state-of-the-art SSL results are reported
based on EBMs and across different data modalities (images, natural
languages, an protein structure prediction and year prediction from the
UCI dataset repository) and in different data settings (fix-dimensional
and sequence data).
So EBM-based SSL can be applied across different data modalities
(fix-dimensional and sequence data), by either of pre-training and jointtraining. Thus, there are four cases. In this section, we systematically
introduce these four cases, as summarized in Table 5.1, where we take
image classification and natural language labeling as representative tasks.
We continue with the notations in Definition 2.5 of EBMs parameterized
by neural networks.


5.4. JEMS AND JRFS FOR SEMI-SUPERVISED LEARNING

157

Figure 5.3: Illustration of EBM based semi-supervised image classification. (a)
Pre-training, (b) Fine-tuning, (c) Joint-training.

5.4.1

Pre-training via EBMs for SSL

Pre-training via EBMs for SSL consists of two stages. The first stage is
pre-training an EBM on unlabeled data. It is followed by a fine-tuning
stage, where we can easily use the pre-trained EBM to initialize a
discriminative model and further train over labeled data.
Pre-training of an EBM for semi-supervised image classification
essentially involves estimating pθ (x) as defined in Eq. (2.13) from unlabeled images. For the potential function Uθ (x), we can use a multi-layer
feed-forward neural network Φθ (x) : RD → R, which, in the final layer,
calculates a scalar via a linear layer, Uθ (x) = wT h, as shown in Figure
5.3(a). Here h ∈ RH denotes the activation from the last hidden layer
and w ∈ RH the weight vector in the final linear layer. For simplicity,
we omit the bias in describing linear layers throughout Section 5.4.
In fine-tuning, as shown in Figure 5.3(b), we throw away w and fed
h into a new linear output layer, followed by softmax(W h), to predict
y, where W ∈ RK×H denotes the new trainable weight parameters and
y ∈ {1, · · · , K} the class label. It can be seen that pre-training aims to


158

Joint EBMs with applications

learn representations that may be useful for multiple downstream tasks,
and information about the labels is not utilized until the fine-tuning
stage.
Pre-training of an EBM for semi-supervised natural language labeling
(e.g., POS tagging) is similar to the above procedure of pre-training
of an EBM for semi-supervised image classification. In pre-training,
we estimate an EBM-based language model pθ (x) from unlabeled text
corpus. Neural networks with different architectures can be used to
implement the potential function Φθ (x) : Vl → R given length l. With
abuse of notation, here x = (x1 , . . . , xl ) denotes a token sequence of
length l, and xi ∈ V, i = 1, · · · , l. For example, as shown in Figure
5.4(a), we can use the bidirectional LSTM based potential function in
(Wang and Ou, 2018a) as follows:
Uθ (x) =

l−1
X
i=1

hTf,i ei+1 +

l
X

hTb,i ei−1

(5.12)

i=2

where ei , hf,i and hb,i are of the same dimensions, denoting the output
embedding vector, the last hidden vectors of the forward and backward
LSTMs respectively at position i.
In fine-tuning, as shown in Figure 5.4(b), we add a CRF, as the discriminative model, on top of the extracted representations, (hf,i , hb,i ), i =
1, · · · , l, to do sequence labeling, i.e., to predict a sequence of labels
y = (y1 , . . . , yl ) with one label for one token at each position, where
yi ∈ {1, · · · , K} denotes the label at position i. Specifically, we concatenate hf,i and hb,i , add a linear output layer to define the node potential,
and add a matrix A ∈ RK×K to define the edge potential, as in recent
neural CRFs (Lample et al., 2016; Ma and Hovy, 2016). The parameters
to be fine-tuned are the weights in the linear output layer and the edge
potential matrix A.
5.4.2

Joint-training via EBMs for SSL

The above pre-training via EBMs for SSL considers the modeling of
only observations x without labels y. The joint-training refers to the


5.4. JEMS AND JRFS FOR SEMI-SUPERVISED LEARNING

159

Figure 5.4: Illustration of EBM based sequence labeling. (a) Pre-training, (b)
Fine-tuning, (c) Joint-training.

joint modeling of x and y:
pθ (x, y) =

1
exp [Uθ (x, y)]
Z(θ)

(5.13)

Then, it can be easily seen, as detailed in Section 5.2, that the conditional
density pθ (y|x) implied by the joint density Eq. (5.13) is:
pθ (y|x) =

pθ (x, y)
exp(Uθ (x, y))
=P
′
pθ (x)
y ′ exp(Uθ (x, y ))

(5.14)

And the implied marginal density is
pθ (x) =

1
exp(Uθ (x))
Z(θ)

(5.15)

where, with abuse of notation, Uθ (x) ≜ log y exp [Uθ (x, y)]. Different
from pre-training, the unsupervised objective pθ (x) depends on the
targeted task. The key for EBM based joint-training for SSL is to choose
suitable Uθ (x, y) such that both pθ (y|x) and pθ (x) can be tractably
optimized.
P

Joint-training of an EBM for semi-supervised image classification
considers a neural network Ψθ (x) : RD → RK , which, as shown in Figure
5.3(c), accepts the image x and outputs an vector, whose size is equal
to the number of class labels, K. Then we define Uθ (x, y) = Ψθ (x)[y],
where [y] denotes the y-th element of a vector. With the above potential


160

Joint EBMs with applications

definition, it can be easily seen that the implied conditional density
pθ (y|x) is exactly a standard K-class softmax based classifier, using the
K logits calculated by the neural network Ψθ (x) from the input x. And
we do not need to calculate Z(θ) for classification. Therefore, we can
conduct SSL over a mix of labeled and unlabeled data by maximizing the
(weighted) sum of log pθ (y|x) and log pθ (x), where both optimizations
are tractable as detailed in (Song and Ou, 2018).
Joint-training of an EBM for semi-supervised natural language labeling is similar to the above procedure of joint-training of an EBM
for semi-supervised image classification, with x = (x1 , . . . , xl ) and
y = (y1 , . . . , yl ), xi ∈ V, yi ∈ {1, · · · , K} , i = 1, · · · , l. As shown in
Figure 5.4(c), we consider a neural network Ψθ (x) : Vl → Rl×K and
define
Uθ (x, y) =

l
X
i=1

Ψθ (x)[i, yi ] +

l
X

A[yi−1 , yi ]

(5.16)

i=1

where [·, ·] denotes the element of a matrix and A ∈ RK×K models the
edge potential for adjacent labels. With the above potential definition,
it can be easily seen, as detailed in Section 5.3, that the conditional
density pθ (y|x) implied by the joint density Eq.(5.13) is exactly a CRF
with node potentials Ψθ (x)[i, yi ] and edge potentials A[yi−1 , yi ], and the
implied marginal density pθ (x) is exactly a trans-dimensional random
field (TRF) language model (Wang et al., 2018; Wang and Ou, 2017;
Wang and Ou, 2018b). Training of both models are tractable as detailed
in (Song et al., 2020; Wang and Ou, 2018a).
5.4.3

Comparison of joint-training and pre-training

In (Song et al., 2021), a suite of SSL experiments are conducted on
standard benchmark datasets in different domains, including the CIFAR10 and SVHN datasets (Song and Ou, 2018) for image classification
and the POS, chunking and NER datasets (Hu et al., 2019; Song et al.,
2020) for natural language labeling. It is revealed that joint-training
EBMs outperform pre-training EBMs marginally but nearly consistently.
Presumably, this is because that the optimization of joint-training is


5.4. JEMS AND JRFS FOR SEMI-SUPERVISED LEARNING

161

Table 5.2: SSL for image classification over CIFAR-10 with 4,000 labels for
a full training set of 50K images. The upper/lower blocks show the generative/discriminative SSL methods respectively. The means and standard deviations
are calculated over ten independent runs with randomly sampled labels.

Methods

error (%)

CatGAN (Springenberg, 2016)
19.58±0.46
Ladder network (Rasmus et al., 2015)
20.40±0.47
Improved-GAN (Salimans et al., 2016)
18.63±2.32
BadGAN (Dai et al., 2017)
14.41±0.30
Sobolev-GAN (Mroueh et al., 2018)
15.77±0.19
Supervised baseline
25.72±0.44
Pre-training+fine-tuning EBM
21.40±0.38
Joint-training EBM
15.12±0.36
Results below this line cannot be directly compared to those above.
VAT small (Miyato et al., 2018)
14.87
Temporal Ensembling (Laine and Aila, 2017)
12.16±0.31
Mean Teacher (Tarvainen and Valpola, 2017)
12.31±0.28
directly related to the targeted task, but pre-training is not aware of
the labels for the targeted task.
Full detailed experimental results are referred to (Song et al., 2021).
Here we present some results for illustration. In (Song et al., 2021), the
standard data split for training and testing is used. When changing the
amount of labeled and unlabeled data for training, varying proportions
(e.g., 10%, 100%) of labels are selected from the original full set of
labeled data. Hereafter, the amount of labels is thus described in terms
of proportions. “100% labeled” means 50K images for CIFAR-10, and
56K, 7.4K, 14K sentences for POS, chunking and NER, respectively.

SSL for Image Classification. In (Song et al., 2021), different generative SSL methods are compared over CIFAR-10. As in previous works,
4,000 labeled images are randomly sampled for training. The remaining
images are treated as unlabeled. It can be seen from Table 5.2 that semisupervised EBMs, especially the joint-training EBMs, produce strong


162

Joint EBMs with applications

results on par with state-of-art generative SSL methods5 . Furthermore,
joint-training EBMs outperform pre-training+fine-tuning EBMs by a
large margin in this task. Note that some discriminative SSL methods,
as listed in the lower block in Table 5.2, also produce superior results
but heavily utilize domain-specific data augmentations, and thus are
not directly compared to the generative SSL methods.
SSL for Natural Language Labeling. In this experiment, different SSL
methods are evaluated for natural language labeling over three tasks POS tagging, chunking and NER. The following benchmark datasets
are used - PTB POS tagging, CoNLL-2000 chunking and CoNLL-2003
English NER, as in (Ma and Hovy, 2016; Clark et al., 2018; Hu et al.,
2019; Song et al., 2020). Varying proportions of labels are sampled
as labeled training data and use the Google one-billion-word dataset
(Chelba et al., 2014) as the large pool of unlabeled sentences. A large
scale of experiments are conducted, covering the labeling proportions
of 2%, 10% and 100% with “U/L” (the ratio between the amount of
unlabeled and labeled) of 50, 250 and 500 for three tasks, which consist
of a total of 27 settings. The network architectures in (Song et al.,
2020) is used. After some empirical search, hyper-parameters (tuned
separately for different methods) are fixed, which are used for all the 27
settings.
Table 5.3 only show the relative numerics, absolute numerics are
referred to (Song et al., 2021). The main observations are as follows.
• The joint-training EBMs outperform the supervised baseline in 25
out of the 27 settings. Since we perform one run for each setting,
this indicates 2 outliers.
• The effects of increasing the labeling sizes on the improvements
of the joint-training EBMs over the supervised baseline with
a fixed “U/L” are mixed. For POS/chunking/NER, the largest
improvements are achieved under 2%/10%/100% labeled, respectively. It seems that the working point where an SSL method
5

As discussed in (Song and Ou, 2018), Bad-GANs could hardly be classified as a
generative SSL method.


5.4. JEMS AND JRFS FOR SEMI-SUPERVISED LEARNING

163

Table 5.3: Relative improvements by joint-training EBMs compared to the supervised baseline (abbreviated as sup.) and the pre-training+fine-tuning EBMs
(abbreviated as pre.) respectively. The evaluation metric is accuracy for POS and
F1 for chunking and NER. “Labeled” denotes the amount of labels in terms of the
proportions w.r.t. the full set of labels. “U/L” denotes the ratio between the amount
of unlabeled and labeled data.

Labeled
2%

10%

100%

U/L
50
250
500
50
250
500
50
250
500

POS
7.9
12.6
15.1
5.6
6.0
8.5
3.1
5.0
6.2

joint over sup.
Chunking NER
16.5
-2.7
16.6
1.5
20.3
4.5
18.0
0.9
18.3
-1.2
21.8
1.0
10.3
6.5
13.6
8.3
14.0
8.4

POS
4.7
4.2
4.1
3.8
3.8
5.2
3.5
3.5
4.3

joint over pre.
Chunking NER
3.4
3.7
0.9
0.1
-0.3
-1.5
3.0
5.0
9.4
-0.7
3.7
-4.1
5.3
1.1
7.4
3.6
6.4
2.5

brings the largest improvement over the supervised baseline is
task dependent. Suppose that the working point is indicated by
the performance of the supervised baseline, then the SSL method
brings the largest effect when the performance of the supervised
baseline is moderate, i.e., neither too low nor too high.
• Joint-training EBMs outperform pre-training EBMs in 23 out
of the 27 settings marginally but nearly consistently. A possible
explanation is that pre-training is not aware of the labels for the
targeted task and is thus weakened for representation learning.
In contrast, the marginal likelihood optimized in joint-training is
directly related to the targeted task.
• It seems that the degrees of improvements of the joint-training
EBMs over the pre-training EBMs are not much affected when
varying the labeling sizes and the “U/L” ratios.


164
5.5

Joint EBMs with applications
JRFs for calibrated natural language understanding

Motivation
In this section, we describe how the joint EBMs (JEMs), as introduced
in Section 5.2, can be used for calibrated natural language understanding. Calibration refers to how well a classification model’s confidence
(reflected by its output posterior probability) aligns with its actual
accuracy. As deep learning models achieve amazing accuracy in computer vision and natural language processing (NLP), more research
attention has been drawn to the calibration aspect of these models. As
shown by Guo et al., 2017, the high accuracy from deep models does
not always lead to better calibration. This motivates an important line
of works attempting to achieve a better trade-off between accuracy and
calibration.
Most existing calibration methods (see references in He et al., 2021)
generally rescale the posterior distribution predicted from the classifier after training. Such post-processing methods require a held-out
development set with a decent number of samples to be available. In
another line of work, Grathwohl et al., 2020 shows that one can train
a joint EBM together with the standard training of a neural classifier.
Although calibration is not explicitly addressed during EBM training,
the calibration of the resulting classifier is shown to be greatly improved.
However, the training framework proposed by Grathwohl et al., 2020
is designed for image classifiers, and it can not be readily applied to
discrete text data. He et al., 2021 investigates JEM training during the
finetuning of pre-trained text encoders (e.g., BERT or RoBERTa) for
natural language understanding (NLU) tasks.
The JEM model for NLU
He et al., 2021 considers the finetuning of pre-trained text encoder on
NLU tasks. Assume samples from the data distribution pdata are in
the form of (x, y) pairs, where x usually refers to a single or a pair
of sentences, and y refers to the corresponding label. The number of
classes are denoted by |Y|.
Given input x, a text encoder model (e.g., BERT or RoBERTa) is


5.5. JRFS FOR CALIBRATED NATURAL LANGUAGE
UNDERSTANDING

165

Figure 5.5: Comparison of the scalar and the hidden variants of energy functions.
The modules introduced for EBM are shaded in green. (He et al., 2021)

firstly used to encode x, and the resulting embedding is denoted by
enc(x). For the target classification task, a classifier fCLS , which could
be a simple linear transform or a multi-layer perception (MLP), will
be applied to enc(x). The output logits are denoted as fCLS (enc(x)),
whose dimension is equal to the number of possible classes |Y|. The y-th
logit is denoted by fCLS (enc(x))[y]. The posterior distribution pθ (y|x)
is obtained by applying a softmax operation to the logits, where θ refers
to the parameters in the model.
In standard finetuning, the cross-entropy (CE) loss and gradient
based optimizers are used to train the classifier:
LCE = E(x,y)∼pdata (− log pθ (y|x)).

(5.17)

In the following, we will introduce how a JEM model can be defined on
top of the text encoder for calibrated NLU.
He et al., 2021 considers a JEM model in a residual form, similar to
Section 3.3.3 and Section 4.4.1. The energy function for modeling the
marginal distribution of x is defined as follows:
Eθ (x) = − log pN (x) + Êθ (x)

(5.18)

where pN (x) is the base distribution, which will also be used as the
noise distribution during NCE training. He et al., 2021 examines three
variants of the residual energy function Êθ (x).
• Variant scalar. Another linear layer gS is introduced to define
the energy function, whose output is a scalar:
Êθ (x) = gS (enc(x)).


166

Joint EBMs with applications

• Variant hidden. Similar to (Grathwohl et al., 2020; Song and
Ou, 2018), starting from a softmax based classifier, an EBM can
be defined with the logits as follows:
|Y|

Êθ (x) = −LogSumExpy=1 (fCLS (enc(x))[y]).
Different from the scalar variant, here the energy function directly
uses the logits for prediction (visualized in Figure 5.5). Hence the
impact on the model’s classification behavior could be larger.
• Variant sharp-hidden. The hidden variant has a potential weakness that, the correlation between input x and the prediction
y is not addressed because the energy is distributed among all
the logits. Motivated by this potential issue, the following sharp
variant is proposed, which can be viewed as an approximation
to the hidden variant, and is found to work well in practice (He
et al., 2021).
Êθ (x) = − max fCLS (enc(x))[y].
y

NCE training of the JEM model
Similar to the discussion in Section 4.4.1, NCE is applied to train the
residual JEM model. NCE trains the model to discriminate between real
samples from pdata and noise samples from a given noise distribution
pN . The NCE loss is the same as Eq. (4.33):
LNCE = −

E

x+ ∼pdata

log

1
1
−ν E log
x− ∼pN
1 + ν exp(Eθ (x+ ))
1 + exp(−Eθ (x− ))/ν

where ν is the ratio of noise samples to real samples.
In (He et al., 2021), LCE and LN CE are jointly optimized during
training, that is:
Ljoint = LCE + LNCE
For constructing the noise distribution pN (x), (He et al., 2021)
finetunes the GPT-2 language model (Radford et al., 2019) with samples
from the target training set. However during NCE training, the energy
model is found to easily discriminate between data samples and noise


5.5. JRFS FOR CALIBRATED NATURAL LANGUAGE
UNDERSTANDING

167

samples, which makes training ineffective (He et al., 2021). To alleviate
this issue, an objective similar to the masked language model (MLM)
loss (Devlin et al., 2018) is adopted during the finetuning of the noise
model. With a given mask ratio M (e.g., 0.4), He et al., 2021 randomly
masks part of x, and trains the model to complete it:
LMLM = −

E

x∼pdata ,xm ∼mask(x,M )

log pN (x|xm )

During noise sample generation, adopting the same mask ratio M , a
masked xm is fed to pN (x is from the training set), and the generated
sample are used as the noise sample. In this way, the noise distribution
is made closer to the data distribution.
Evaluation of the Calibration performance
To measure calibration performance, expected calibration error (ECE)
metric is used, following (Guo et al., 2017; Grathwohl et al., 2020). Given
an input sample x, for each label y, we say that the model predicts
that x belongs to label y with confidence pθ (y|x). Assuming the test-set
contains n samples, we will have n × |Y| predictions.
ECE first partitions all predictions into B equally-spaced bins by its
confidence. Following (Grathwohl et al., 2020), B = 20 is used, which
means the width of each bin is 0.05. For example, the first bin contains
all predictions that have confidence in the range of [0, 0.05). Then for
each bin ECE computes how the average of confidence is different from
its actual accuracy:
|Y|

ECE =

B
|Byb |
1 XX
|acc(Byb ) − conf(Byb )|
|Y| y=1 b=1 n

(5.19)

where n is the number of test samples. acc(Byb ) is the ratio of samples
(x) whose true label is indeed y in the bin Byb , and conf(Byb ) is the
average confidence in that bin.
He et al., 2021 uses the RoBERTa-base model as the text encoder
and finetune it on eight GLUE tasks (Wang et al., 2019). It is found
that EBM training is able to reach a better trade-off between accuracy
and calibration. In most tasks, all three EBM variants get substantial


168

Joint EBMs with applications

Figure 5.6: The entropy of the posterior (pθ (·|x)) versus energy value Êθ (x) for
SST-2 test-set samples. (He et al., 2021)

improvement in ECE with little or no loss in accuracy comparing to
the (strong) baseline methods.
How does the model get better calibration? Figure 5.6 from He et al.,
2021 give some analysis. Figure 5.6 shows the energy value Êθ (x) versus
the entropy of the posterior distribution
H(pθ (·|x)) = −

|Y|
X

pθ (y|x) log pθ (·|x)

y=1

for samples in the SST-2 test set. It is shown that models trained with
the hidden and sharp-hidden variants tend to assign more conservative
predictions (reflected by higher entropy) for higher-energy (less likely)
samples. It is hypothesized that this is due to the strong coupling
between the energy function and the classification logits. However, this
interesting trend (Figure 5.6) is not observed in all datasets (e.g., QNLI).


6
Conclusion

6.1

Summary

Energy-based models (EBMs) form an important aspect of modern artificial intelligence and signal processing. Unlike most other probabilistic
models which are self-normalized, EBMs specify probability density
functions up to an unknown normalizing constant, and thus are also
referred to as non-normalized probabilistic models. Such model definition of not imposing normalization enables a great power and flexibility
to the modeling process. One is generally free to choose any nonlinear regression function as the energy function, as long as it remains
normalizable in principle. Accompanied with such flexibility, we have
also shown the advantages of EBMs in naturally overcoming label bias
and exposure bias suffered by locally-normalized models (Section 4.1.2)
and in hybrid generative-discriminative and semi-supervised learning
(Section 5).
On the other hand, although the flexibility of EBMs can provide
significant modeling advantages, both computation of the exact likelihood and exact sampling from these models are generally intractable,
which makes training of EBMs especially difficult and limits their applications. Moreover, the sequential nature of speech and language also
169


170

Conclusion

presents special challenges and needs treatment different from processing
fix-dimensional data (e.g., images).
We are making progress. This monograph presents a systematic introduction to energy-based models, including both algorithmic progress
and applications in speech and language processing, which is organized
into four chapters.
In Chapter 2, we provide a self-contained introduction to basics
for EBMs, including model definitions, learning algorithms, and sampling/generation algorithms. There are two remarkable features in our
introduction. First, we starts with the framework of probabilistic graphical models (PGMs). The PGM framework enables readers to appreciate
EBMs from the perspective of undirected graphical models and easily
understand the differences between undirected graphical models and
directed graphical models, and between globally-normalized and locallynormalized. Graphical models provide a natural tool for formulating
variations on classical methods, as well as for exploring entirely new
models. Second, our introduction to the stochastic approximation (SA)
methodology is very useful for readers to develop new algorithms for
learning EBMs, particularly learning with Monte Carlo sampling. The
SA methodology is more general than the ordinary stochastic gradient
descent (SGD) technique, while SGD is only one instance of SA.
The next three chapters are dedicated to present how to apply EBMs
in three different scenarios, i.e., for modeling marginal, conditional
and joint distributions, respectively. As visualized in Figure 1.2, our
such organization is comprehensive and distinctive. A wide range of
applications in speech and language processing are covered, including
language modeling, representation learning over text, speech recognition,
sequence labeling in NLP, text generation, semi-supervised learning,
and calibrated natural language understanding.
6.2

Future challenges and directions

EBM based methods represent an important class for the probabilistic
approach to many fields. Despite the progress achieved in these years,
much more work needs to be carried out.


6.2. FUTURE CHALLENGES AND DIRECTIONS

171

• Applications of EBMs are still mainly limited by lack of
effective and efficient training techniques. Training techniques are crucial to problem solving with EBMs, and will remain
an active direction for future research. EBMs have been especially
difficult to train. We mainly introduce maximum likelihood training with MCMC sampling, noise-contrastive estimation (NCE)
and briefly covers score matching (SM). There has been persistent and ongoing interest in developing effective modeling and
training techniques for learning EBMs (see Zhang et al., 2023
and its references), but those methods are mostly first studied
for fix-dimensional data. There is vast room to improve training
techniques for EBMs, especially for sequential data such as speech
and language.
Moreover, as we have discussed, learning EBMs over discrete
data are more challenging than over continuous data, due to the
combinatorial explosion of the state space. Langevin sampling,
a particular MCMC sampling method which utilizes gradients,
has been dominantly used in training EBMs over continuous data.
A recent progress in (Grathwohl et al., 2021; Qin et al., 2022)
begins to use gradients of the likelihood function with respect to
its discrete inputs to propose updates in a Metropolis-Hastings
sampler. More further research is needed.
• More downstream applications of EBMs are worthwhile
for exploration. Considering the potential advantages of EBMs
in modeling flexibility, overcoming label bias and exposure bias,
and hybrid generative-discriminative and semi-supervised learning,
the results reviewed in this monograph are only preliminary. There
are many interesting tasks which could be approached by EBMs.
The applications of EBMs require more skill and experience, apart
from using the mainstream deep learning toolkits. To help the
readers to get familiar with the techniques for developing and
applying energy-based models, we summarize some open-source
toolkits in Appendix C.
• We look forward to more foundational and interdisci-


172

Conclusion
plinary research around EBMs. There is an opportunity for
developing physics-based methods that could address the difficulty
of calculating or sampling the partition function of EBMs (Huembeli et al., 2022). There are emergent areas of research at the
interfaces of machine learning, quantum computing, many-body
physics, and optimization (see references in Huembeli et al., 2022).
Many recent biologically plausible algorithms utilize the framework
of energy-based models (Scellier and Bengio, 2017; Millidge et
al., 2023). Biologically plausible algorithms compute gradients
that approximate those computed by back-propagation (BP), and
operate in ways that more closely satisfy the constraints imposed
by neural circuitry. We anticipate that progress in neuroscience
and EBM based machine learning will benefit from an interplay
between both fields.


Acknowledgements

Thanks to collaborators and students: Zhiqiang Tan, Bin Wang, Hongyu
Xiang, Yunfu Song, Kai Hu, Keyu An, Huahuan Zheng, Silin Gao, Hong
Liu, Junlan Feng, and Yi Huang.
Thanks for funding supports from
• NSFC (National Science Foundation of China) through No. 60402029,
No. 61075020, No. 61473168, and No. 61976122;
• Ministry of Education and China Mobile joint funding through
No. MCM20170301;
• Joint Institute of Tsinghua University - China Mobile Communications Group Co. Ltd.;
• Beijing National Research Center for Information Science and
Technology;
• Tsinghua Initiative through No. 20121088069 and No. 20141081250;
• Toshiba Corporation;
• Apple Corporation.

173


Appendices


A
Notations and definitions

A.1

Notations

Example

Description

zi:j

For any generic sequence {zn }, we shall use zi:j to
denote zi , zi+1 , · · · zj . Similarly, wherever a collection
of indices appears in the subscript, we refer to the
corresponding collection of indexed variables, e.g.,
cl,1:H ≜ {cl,1 , cl,2 , · · · cl,H }.
x generally denotes a random variable, which can either be scalar- or vector-valued, and often denotes the
observation variable. For simplicity, we also use the
same notation x to denote the values taken by the
random variable x, e.g., in the argument of its density
function, which should be clear from the context.
The hidden variable.
The class label, or the output variable.
The cardinality/size of a set B
A superscript T denotes the transpose of a vector x or
matrix A
The K-dimensional probability simplex.

x

h
y
|B|
xT , AT
∆K

175


176

Notations and definitions

x f (x)

P

pora (·)
pemp (·)

The summation over x is a shorthand, which should be
an appropriate combination of summation and integration, depending on the components of x being discrete
variables, continuous variables, or a combination of the
two.
The (unknown) oracle density, sometimes also known
as the data distribution and denoted as pdata (·).
The empirical density. For a training dataset consisting
of n independent and identically distributed (IID) data
points {x1 , · · · , xN }, we have
pemp (x) ≜

pθ (·), p(·; θ)
qϕ (·), q(·; ϕ)
Uni[a, b]

A.2

N
1 X
δ(x − xi )
N i=1

The (target) model density, parameterized by θ.
The auxiliary density introduced in training, parameterized by ϕ.
Uniform distribution for a continuous variable over
interval [a, b], or for a discrete variable over integers
from a to b.

Definitions

Term

Description

σ(v)

The sigmoid function, σ(v) ≜ 1+e1−v , also called
the logistic sigmoid function. It is also known as a
squashing function, since it maps the whole real line
to [0, 1], which is necessary for the output to be
interpreted as a probability.


A.2. DEFINITIONS

logit(σ)

softmax(z1:K )

177

σ
The logit function, logit(σ) ≜ log( 1−σ
) for 0 < σ <
1, also known as the inverse of the sigmoid function.
It represents the log of the ratio of probabilities for
two classes, also known as the log odds.
The softmax function, softmax(z1:K )k
≜
exp(zk )
PK
, which realizes normalization from RK
j=1

δ(x = a)
H[q]
KL[p||q]

KL[q||p]

exp(zj )

to ∆K (the K-dimensional probability simplex). It
is also known as the normalized exponential and
can be regarded as a multiclass generalization of the
logistic sigmoid.
An indicator function of x which takes the value 1
when x = a and 0 otherwise.
R
The entropy is defined as H[q] ≜ − qlogq.
The inclusive KL-divergence between two distributions
and q(·) is defined as KL[p||q] ≜
 p(·)

R
p
plog q , which by default is called the KLdivergence, and is sometimes also referred to as the
forward KL-divergence, relative entropy.
The exclusive
KL-divergence is defined as KL[q||p] ≜
 
R
q
qlog p , which is also referred to as the reverse
KL-divergence.


B
Background material

B.1

Maximum entropy models

Theorem B.1. When confronted by a probability distribution p(x)
about which only a few facts are known, the maximum entropy principle
(maxent) offers a rule for choosing a distribution that satisfies those
constraints (Cover, 1999; MacKay, 2003). According to maxent, one
should select the p(x) that maximizes the entropy
H(p) = −

X

p(x) log p(x)

(B.1)

x

subject to the constraints. When there is a reference distribution q(x),
one should select the p(x) that minimizes the relative entropy or
Kullback-Leibler divergence1
KL(p||q) =

X

p(x) log

x

p(x)
q(x)

(B.2)

Assuming the constraints assert that the averages of certain functions
fk (x) are known, i.e.,
Ep(x) [fk (x)] = Fk , k = 1, 2, · · ·

(B.3)

Then, it can be shown that by introducing Lagrange multipliers (one
for each constraint, including normalization),
• The distribution that maximizes the entropy has the following
form
!
X
1
p∗ (x) = exp
wk fk (x)
(B.4)
Z
k
1

When q(x) is uniform, this is the same as maximizing the entropy.

178


B.2. FISHER EQUALITY

179

• The distribution that minimizing relative entropy relative to q(x),
has the following form
X
1
wk fk (x)
p (x) = q(x) exp
Z
k

!

∗

(B.5)

where {wk } are set such that the constraints Eq. (B.3) are satisfied, and
Z is the normalizing constant. The two forms in Eq. (B.4) and Eq. (B.5)
are often collectively referred to as maximum entropy distributions.
Theorem B.1 gives the form of maximum entropy distributions
that satisfy certain moment constraints. In an opposite way, when
given that a distribution satisfies the form of Eq. (B.4) or Eq. (B.5),
the following theorem establish the connection between the maximum
entropy distribution and the maximum likelihood distribution.
Theorem B.2. Assume that a variable x comes from a probability
distribution of the form in Eq. (B.4) or Eq. (B.5), where the functions
fk (x) are given, and the parameters {wk } are not known. A dataset
{x(n) } is supplied. Then, it can be shown that by differentiating the log
likelihood, the maximum-likelihood (ML) parameters wML satisfy
Ep(x) [fk (x)] =

1 X
fk (x(n) ), k = 1, 2, · · ·
N n

(B.6)

= Epemp (x) [fk (x)]
where the left-hand is the model average under the fitted model, the
right-hand the empirical average over the training data points, and
pemp (·) denotes the empirical density over the training data points.
Combining the above two theorems, we can easily see that maximum
entropy fitting with Fk ’s being set as the empirical averages is equivalent
to maximum likelihood fitting of a log-linear distribution (MacKay, 2003;
Pietra et al., 1997).
B.2

Fisher equality

Formally, for any density function pθ (x), the partial derivative w.r.t.
∂
θ of the log density function, ∂θ
logpθ (x), is called the “score”. Under


180

Background material

certain regularity conditions, the expectation of the score w.r.t. the
density itself is 0. This formula is often referred in presenting Fisher
information2 , so we call it Fisher equality, which, is frequently used in
this monograph.


∂
Epθ (x)
log pθ (x) = 0.
(B.7)
∂θ
Further, based on the above basic Fisher equality, we have the
following very useful theorem.
Theorem B.3. Consider any latent-variable model pθ (x, h), which consisting of observation x and latent variable h, then we have
∂
∂
log pθ (x) = Epθ (h|x)
log pθ (x, h)
∂θ
∂θ




(B.8)

which means that the gradient of the log marginal likelihood is equal to
the expected log joint likelihood, where the expectation is taken over
the posteriori distribution.
Proof.
∂
∂
log pθ (x) = Epθ (h|x)
log pθ (x)
∂θ
∂θ


∂
∂
= Epθ (h|x)
log pθ (x, h) −
log pθ (h|x)
∂θ
∂θ


∂
log pθ (x, h)
= Epθ (h|x)
∂θ




where in the second line, according to Fisher equality, we have


Epθ (h|x)

∂
log pθ (h|x) = 0,
∂θ


and thus we obtain the final line. For simplicity, Eq. (B.8) is also referred
to as Fisher equality.
■

2

https://en.wikipedia.org/wiki/Fisher_information


C
Open-source toolkits related to EBMs

• Trans-dimensional random field (TRF) LMs: https://github.com/
thu-spmi/SPMILM
• Energy-based cloze models for representation learning over text
(Electric): https://github.com/google-research/electra
• CRF-based ASR Toolkit (CAT): https://github.com/thu-spmi/
CAT
• Neural CRF Transducers for Sequence Labeling: https://github.
com/thu-spmi/SPMISeq
• Controlled text generation from pre-trained language models (mixand-match): https://github.com/mireshghallah/mixmatch
• Learning neural random fields with inclusive auxiliary generators:
https://github.com/thu-spmi/Inclusive-NRF
• JEMs and JRFs for semi-supervised learning: https://github.com/
thu-spmi/semi-EBM

181


References

Amaya, F. and J. M. Benedi. (2001). “Improvement of a whole sentence
maximum entropy language model using grammatical features”. In:
Proc. Ann. Meeting of the Association for Computational Linguistics
(ACL).
An, K., H. Xiang, and Z. Ou. (2020). “CAT: A CTC-CRF based ASR
Toolkit Bridging the Hybrid and the End-to-end Approaches towards
Data Efficiency and Low Latency”. In: INTERSPEECH.
An, K., H. Zheng, Z. Ou, H. Xiang, K. Ding, and G. Wan. (2022).
“Cuside: Chunking, simulating future context and decoding for
streaming ASR”. In: INTERSPEECH.
Andor, D., C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev,
S. Petrov, and M. Collins. (2016). “Globally Normalized TransitionBased Neural Networks”. In: Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long
Papers).
Andrieu, C., É. Moulines, and P. Priouret. (2005). “Stability of stochastic
approximation under verifiable conditions”. SIAM Journal on control
and optimization. 44(1): 283–312.
Andrieu, C. and J. Thoms. (2008). “A tutorial on adaptive MCMC”.
Statistics and computing. 18(4): 343–373.
Argyriou, A., T. Evgeniou, and M. Pontil. (2007). “Multi-task feature
learning”. In: NIPS.
182


REFERENCES

183

Artieres, T. et al. (2010). “Neural conditional random fields”. In: AISTATS.
Bakhtin, A., S. Gross, M. Ott, Y. Deng, M. Ranzato, and A. Szlam.
(2019). “Real or fake? learning to discriminate machine from human
generated text”. arXiv preprint arXiv:1906.03351.
Belanger, D. and A. McCallum. (2016). “Structured Prediction Energy
Networks”. In: ICML.
Bengio, S., O. Vinyals, N. Jaitly, and N. Shazeer. (2015). “Scheduled
sampling for sequence prediction with recurrent neural networks”.
Advances in neural information processing systems.
Benveniste, A., M. Métivier, and P. Priouret. (1990). Adaptive algorithms
and stochastic approximations. New York: Springer.
Besag, J. E. (1994). “Comments on “Representations of knowledge in
complex systems” by U. Grenander and M.I. Miller”. Journal of the
Royal Statistical Society: Series B. 56: 549–581.
Bishop, C. M. (2006). Pattern recognition and machine learning. Springer.
Bouchard, G. (2007). “Bias-variance tradeoff in hybrid generativediscriminative models”. In: International Conference on Machine
Learning and Applications (ICMLA).
Bu, H., J. Du, X. Na, B. Wu, and H. Zheng. (2017). “AISHELL-1:
An open-source Mandarin speech corpus and a speech recognition
baseline”. In: 2017 20th Conference of the Oriental Chapter of the
International Coordinating Committee on Speech Databases and
Speech I/O Systems and Assessment (O-COCOSDA).
Chatzis, S. P. and Y. Demiris. (2013). “The Infinite-Order Conditional
Random Field Model for Sequential Data Modeling”. IEEE Transactions on Pattern Analysis and Machine Intelligence.
Chelba, C., T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and
T. Robinson. (2014). “One Billion Word Benchmark for Measuring
Progress in Statistical Language Modeling”. In: INTERSPEECH.
Chen, H. (2002). Stochastic approximation and its applications. Springer
Science & Business Media.
Chen, S. F. and J. Goodman. (1999). “An empirical study of smoothing
techniques for language modeling”. Computer Speech & Language.
13(4): 359–394.


184

REFERENCES

Chen, T., E. Fox, and C. Guestrin. (2014). “Stochastic gradient Hamiltonian Monte Carlo”. In: ICML.
Chen, T., S. Kornblith, M. Norouzi, and G. Hinton. (2020). “A simple framework for contrastive learning of visual representations”.
arXiv:2002.05709.
Chen, X., X. Liu, Y. Wang, A. Ragni, J. H. Wong, and M. J. Gales.
(2019). “Exploiting Future Word Contexts in Neural Network Language Models for Speech Recognition”. IEEE/ACM Transactions
on Audio, Speech, and Language Processing. 27(9): 1444–1454.
Chiu, C.-C., T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen,
A. Kannan, R. J. Weiss, K. Rao, E. Gonina, et al. (2018). “State-ofthe-art speech recognition with sequence-to-sequence models”. In:
ICASSP.
Chorowski, J., D. Bahdanau, K. Cho, and Y. Bengio. (2014). “End-toend continuous speech recognition using attention-based recurrent
NN: First results”. arXiv preprint arXiv:1412.1602.
Clark, K., M.-T. Luong, Q. V. Le, and C. D. Manning. (2020a). “Electra:
Pre-training text encoders as discriminators rather than generators”.
In: International Conference on Learning Representations (ICLR).
Clark, K., M.-T. Luong, Q. V. Le, and C. D. Manning. (2020b). “PreTraining Transformers as Energy-Based Cloze Models”. Conference
on Empirical Methods in Natural Language Processing (EMNLP).
Clark, K., M.-T. Luong, C. D. Manning, and Q. Le. (2018). “SemiSupervised Sequence Modeling with Cross-View Training”. In: EMNLP.
Collins, M. and B. Roark. (2004). “Incremental Parsing with the Perceptron Algorithm”. In: ACL.
Collobert, R., J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P.
Kuksa. (2011). “Natural language processing (almost) from scratch”.
Journal of machine learning research. 12(Aug): 2493–2537.
Cover, T. M. (1999). Elements of information theory. John Wiley &
Sons.
Cowell, R. G., A. P. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter.
(1999). Probabilistic Networks and Expert Systems. Springer-Verlag.
Cubuk, E. D., B. Zoph, J. Shlens, and Q. V. Le. (2020). “RandAugment: Practical automated data augmentation with a reduced search
space”. In: CVPR.


REFERENCES

185

Cui, X., B. Kingsbury, G. Saon, D. Haws, and Z. Tuske. (2021). “Reducing exposure bias in training recurrent neural network transducers”.
In: INTERSPEECH.
Dahl, G. E., D. Yu, L. Deng, and A. Acero. (2012). “Context-dependent
pre-trained deep neural networks for large-vocabulary speech recognition”. IEEE Transactions on audio, speech, and language processing.
20(1): 30–42.
Dai, Z., Z. Yang, F. Yang, W. W. Cohen, and R. R. Salakhutdinov.
(2017). “Good semi-supervised learning that requires a bad GAN”.
In: NIPS.
Dayan, P., G. E. Hinton, R. M. Neal, and R. S. Zemel. (1995). “The
helmholtz machine”. Neural computation. 7(5): 889–904.
Dempster, A. P., N. M. Laird, and D. B. Rubin. (1977). “Maximum
likelihood from incomplete data via the EM algorithm”. Journal of
the Royal Statistical Society. 39.
Deng, Y., A. Bakhtin, M. Ott, A. Szlam, and M. Ranzato. (2020).
“Residual energy-based models for text generation”. In: ICLR.
Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. (2018). “Bert:
Pre-training of deep bidirectional transformers for language understanding”. arXiv preprint arXiv:1810.04805 : 4171–4186.
Durrett, G. and D. Klein. (2015). “Neural CRF Parsing”. In: ACL.
Frey, B. J. and N. Jojic. (2005). “A comparison of algorithms for
inference and learning in probabilistic graphical models”. IEEE
Trans. Pattern Analysis and Machine Intelligence (PAMI). 27(9):
1392–1416.
Gao, S., Z. Ou, W. Yang, and H. Xu. (2020). “Integrating Discrete
and Neural Features Via Mixed-Feature Trans-Dimensional Random
Field Language Models”. In: ICASSP.
Ghazvininejad, M., O. Levy, Y. Liu, and L. Zettlemoyer. (2019). “Maskpredict: Parallel decoding of conditional masked language models”.
arXiv preprint arXiv:1904.09324.
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
Ozair, A. Courville, and Y. Bengio. (2014). “Generative adversarial
nets”. In: NIPS.
Goodman, J. (2001). “A bit of progress in language modeling”. Computer
Speech & Language. 15: 403–434.


186

REFERENCES

Goyal, K., C. Dyer, and T. Berg-Kirkpatrick. (2022). “Exposing the
Implicit Energy Networks behind Masked Language Models via
Metropolis–Hastings”. In: International conference on learning representations.
Grathwohl, W., K. Swersky, M. Hashemi, D. Duvenaud, and C. Maddison. (2021). “Oops I took a gradient: Scalable sampling for discrete
distributions”. In: International Conference on Machine Learning.
Grathwohl, W., K.-C. Wang, J. Jacobsen, D. Duvenaud, M. Norouzi,
and K. Swersky. (2020). “Your classifier is secretly an energy based
model and you should treat it like one”. In: ICLR.
Graves, A. (2012). “Sequence transduction with recurrent neural networks”. arXiv preprint arXiv:1211.3711.
Graves, A., S. Fernández, F. Gomez, and J. Schmidhuber. (2006). “Connectionist Temporal Classification: Labelling Unsegmented Sequence
Data with Recurrent Neural Networks”. In: ICML.
Gunawardana, A., M. Mahajan, A. Acero, and J. C. Platt. (2005).
“Hidden conditional random fields for phone classification”. In: Ninth
European Conference on Speech Communication and Technology
(EUROSPEECH).
Guo, C. E., S. C. Zhu, and Y. N. Wu. (2003). “Modeling Visual Patterns
by Integrating Descriptive and Generative Methods.” International
Journal of Computer Vision. 53(1): 5–29.
Guo, C., G. Pleiss, Y. Sun, and K. Q. Weinberger. (2017). “On Calibration of Modern Neural Networks”. In: Proceedings of the 34th
International Conference on Machine Learning.
Gutmann, M. and A. Hyvärinen. (2010). “Noise-contrastive estimation:
A new estimation principle for unnormalized statistical models”. In:
AISTATS.
Gutmann, M. U. and A. Hyvärinen. (2012). “Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to
Natural Image Statistics.” Journal of machine learning research.
13(2).
Hadian, H., H. Sameti, D. Povey, and S. Khudanpur. (2018). “FlatStart Single-Stage Discriminatively Trained HMM-Based Models for
ASR”. IEEE/ACM Transactions on Audio, Speech, and Language
Processing. 26(11): 1949–1961.


REFERENCES

187

Han, T., E. Nijkamp, X. Fang, M. Hill, S.-C. Zhu, and Y. N. Wu.
(2019). “Divergence Triangle for Joint Training of Generator Model,
Energy-based Model, and Inferential Model”. In: CVPR.
Hastie, T., R. Tibshirani, J. H. Friedman, and J. H. Friedman. (2009).
The elements of statistical learning: data mining, inference, and
prediction. Vol. 2. Springer.
He, T., B. McCann, C. Xiong, and E. Hosseini-Asl. (2021). “Joint
energy-based model training for better calibrated natural language
understanding models”. preprint arXiv:2101.06829.
Hinton, G. E. (2002). “Training products of experts by minimizing
contrastive divergence”. Neural computation. 14(8): 1771–1800.
Hinton, G. E., P. Dayan, B. J. Frey, and R. M. Neal. (1995). “The
wake-sleep algorithm for unsupervised neural networks.” Science.
268(5214): 1158–1161.
Hinton, G. E., S. Osindero, and Y. W. Teh. (2006). “A fast learning
algorithm for deep belief nets”. Neural Computation. 18(7): 1527–
1554.
Holtzman, A., J. Buys, L. Du, M. Forbes, and Y. Choi. (2019). “The curious case of neural text degeneration”. In: International Conference
on Learning Representations (ICLR).
Hu, K., Z. Ou, M. Hu, and J. Feng. (2019). “Neural CRF transducers
for sequence labeling”. In: ICASSP.
Huang, Z., W. Xu, and K. Yu. (2015). “Bidirectional LSTM-CRF models
for sequence tagging”. arXiv:1508.01991.
Huembeli, P., J. M. Arrazola, N. Killoran, M. Mohseni, and P. Wittek.
(2022). “The physics of energy-based models”. Quantum Machine
Intelligence. 4(1): 1.
Hyvärinen, A. and P. Dayan. (2005). “Estimation of non-normalized
statistical models by score matching”. Journal of Machine Learning
Research. 6(4).
Jelinek, F. (1976). “Continuous speech recognition by statistical methods”. Proceedings of the IEEE. 64(4): 532–556.
Jordan, M. I., Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. (1999).
“An introduction to variational methods for graphical models”. Machine learning. 37: 183–233.


188

REFERENCES

Jordan, M. I. (2004). “Graphical models”. Statistical science. 19(1):
140–155.
Khalifa, M., H. Elsahar, and M. Dymetman. (2021). “A distributional
approach to controlled text generation”. In: International conference
on learning representations.
Kim, K., J. Oh, J. Gardner, A. B. Dieng, and H. Kim. (2022). “Markov
chain score ascent: A unifying framework of variational inference with
Markovian gradients”. Advances in Neural Information Processing
Systems (NeurIPS).
Kim, T. and Y. Bengio. (2016). “Deep directed generative models with
energy-based probability estimation”. In: ICLR Workshop.
Kingma, D. P., M. Welling, et al. (2019). “An introduction to variational autoencoders”. Foundations and Trends® in Machine Learning. 12(4): 307–392.
Kingma, D. P., D. J. Rezende, S. Mohamed, and M. Welling. (2014).
“Semi-Supervised Learning with Deep Generative Models”. In: NIPS.
Koller, D. and N. Friedman. (2009). Probabilistic graphical models:
principles and techniques. MIT press.
Kuleshov, V. and S. Ermon. (2017). “Neural Variational Inference and
Learning in Undirected Graphical Models”. In: NIPS.
Lafferty, J., A. McCallum, and F. C. Pereira. (2001). “Conditional
random fields: Probabilistic models for segmenting and labeling
sequence data”. In: International conference on Machine learning
(ICML).
Laine, S. and T. Aila. (2017). “Temporal Ensembling for Semi-Supervised
Learning”. In: ICLR.
Lample, G., M. Ballesteros, S. Subramanian, K. Kawakami, and C.
Dyer. (2016). “Neural Architectures for Named Entity Recognition”.
In: NAACL-HLT.
Larochelle, H., M. I. Mandel, R. Pascanu, and Y. Bengio. (2012). “Learning algorithms for the classification restricted Boltzmann machine”.
Journal of Machine Learning Research. 13(1): 643–669.
Liang, F., C. Liu, and R. J. Carroll. (2007). “Stochastic approximation
in Monte Carlo computation”. Journal of the American Statistical
Association. 102(477): 305–320.


REFERENCES

189

Liang, P. and M. I. Jordan. (2008). “An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators”. In: International conference on Machine learning (ICML). 584–591.
Ling, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez, S. Amir, L.
Marujo, and T. Luis. (2015). “Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation”.
In: EMNLP.
Liu, H. and Z. Ou. (2023). “Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech
Recognition”. In: INTERSPEECH.
Liu, J. S. (2001). Monte Carlo strategies in scientific computing. Vol. 10.
Springer.
Liu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov. (2019). “RoBERTa: A Robustly
Optimized BERT Pretraining Approach”. ArXiv. abs/1907.11692.
Lu, L., L. Kong, C. Dyer, N. A. Smith, and S. Renals. (2016). “Segmental
Recurrent Neural Networks for End-to-End Speech Recognition”.
In: INTERSPEECH.
Lüscher, C., E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, R. Schlüter,
and H. Ney. (2019). “RWTH ASR Systems for LibriSpeech: Hybrid
vs Attention”. In: INTERSPEECH.
Ma, Y.-A., T. Chen, and E. Fox. (2015). “A complete recipe for stochastic gradient MCMC”. In: NIPS.
Ma, X. and E. Hovy. (2016). “End-to-end Sequence Labeling via Bidirectional LSTM-CNNs-CRF”. In: ACL.
Ma, Z. and M. Collins. (2018). “Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical
efficiency”. EMNLP.
MacKay, D. J. (2003). Information theory, inference and learning algorithms. Cambridge university press.
Marcus, M., B. Santorini, and M. A. Marcinkiewicz. (1993). “Building
a large annotated corpus of English: The Penn Treebank”.
Martin, S., J. Liermann, and H. Ney. (1998). “Algorithms for bigram
and trigram word clustering”. Speech Communication. 24: 19–37.


190

REFERENCES

McCallum, A., D. Freitag, and F. Pereira. (2000). “Maximum entropy
Markov models for information extraction and segmentation.” In:
ICML.
Mesnil, G., Y. Dauphin, K. Yao, Y. Bengio, L. Deng, D. Z. Hakkani-Tür,
X. He, L. P. Heck, G. Tür, D. Yu, and G. Zweig. (2015). “Using
Recurrent Neural Networks for Slot Filling in Spoken Language Understanding”. IEEE/ACM Trans. on Audio, Speech, and Language
Processing. 23: 530–539.
Miao, N., H. Zhou, L. Mou, R. Yan, and L. Li. (2019). “CGMH: Constrained sentence generation by metropolis-hastings sampling”. In:
Proceedings of the AAAI Conference on Artificial Intelligence.
Miao, Y., M. Gowayyed, and F. Metze. (2015). “EESEN: End-to-end
speech recognition using deep RNN models and WFST-based decoding”. In: ASRU.
Mikolov, T., S. Kombrink, L. Burget, J. H. Cernocky, and S. Khudanpur.
(2011). “Extensions of recurrent neural network language model”. In:
International Conference on Acoustics, Speech and Signal Processing
(ICASSP).
Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. (2013).
“Distributed representations of words and phrases and their compositionality”. In: Advances in neural information processing systems.
3111–3119.
Millidge, B., Y. Song, T. Salvatori, T. Lukasiewicz, and R. Bogacz.
(2023). “Backpropagation at the infinitesimal inference limit of
energy-based models: unifying predictive coding, equilibrium propagation, and contrastive hebbian learning”. In: International Conference on Machine Learning.
Minka, T. (2005). “Divergence measures and message passing”. Microsoft
Research Technical Report.
Mireshghallah, F., K. Goyal, and T. Berg-Kirkpatrick. (2022). “Mix and
Match: Learning-free Controllable Text Generation using Energy
Language Models”. In: Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers).


REFERENCES

191

Miyato, T., S.-i. Maeda, M. Koyama, and S. Ishii. (2018). “Virtual
adversarial training: a regularization method for supervised and
semi-supervised learning”. IEEE transactions on pattern analysis
and machine intelligence. 41(8): 1979–1993.
Mohri, M., F. Pereira, and M. Riley. (2008). “Speech recognition with
weighted finite-state transducers”. In: Springer Handbook of Speech
Processing. Springer. 559–584.
Morency, L.-P., A. Quattoni, and T. Darrell. (2007). “Latent-Dynamic
Discriminative Models for Continuous Gesture Recognition”. In:
CVPR.
Mroueh, Y., C.-L. Li, T. Sercu, A. Raj, and Y. Cheng. (2018). “Sobolev
GAN”. In: ICLR.
Murphy, K. P. (2012). Machine learning: a probabilistic perspective. MIT
press.
Naesseth, C., F. Lindsten, and D. Blei. (2020). “Markovian score climbing: Variational inference with KL (p|| q)”. Advances in Neural
Information Processing Systems (NeurIPS).
Neal, R. M. (1993). Probabilistic inference using Markov chain Monte
Carlo methods. Department of Computer Science, University of
Toronto, Canada.
Neal, R. M. (2011). “MCMC using Hamiltonian dynamics”. Handbook
of Markov Chain Monte Carlo.
Neal, R. M. and G. E. Hinton. (1998). “A view of the EM algorithm
that justifies incremental, sparse, and other variants”. In: Learning
in graphical models. Springer. 355–368.
Neal, R. M. (1992). “Connectionist Learning of Belief Networks”. Artificial Intelligence. 56: 71–113.
Ng, A. and M. Jordan. (2001). “On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes”. Advances
in neural information processing systems. 14.
Ngiam, J., Z. Chen, P. W. Koh, and A. Y. Ng. (2011). “Learning deep
energy models”. In: International conference on machine learning
(ICML).
Nowozin, S. (2018). “Debiasing evidence approximations: On importanceweighted autoencoders and jackknife variational inference”. In: International conference on learning representations.


192

REFERENCES

Oliver, A., A. Odena, C. Raffel, E. D. Cubuk, and I. J. Goodfellow.
(2018). “Realistic evaluation of semi-supervised learning algorithms”.
In: ICLR.
Ostendorf, M. (2016). “Continuous-Space Language Processing: Beyond
Word Embeddings”. In: International Conference on Statistical Language and Speech Processing.
Ou, Z. (2018). “A review of learning with deep generative models from
perspective of graphical modeling”. arXiv preprint arXiv:1808.01630.
Ou, Z. and Y. Song. (2020). “Joint stochastic approximation and its application to learning discrete latent variable models”. In: Conference
on Uncertainty in Artificial Intelligence. PMLR. 929–938.
Ou, Z. and J. Xiao. (2010). “A study of large vocabulary speech recognition decoding using finite-state graphs”. In: The 7th International
Symposium on Chinese Spoken Language Processing.
Panayotov, V., G. Chen, D. Povey, and S. Khudanpur. (2015). “Librispeech: an asr corpus based on public domain audio books”. In:
IEEE international conference on acoustics, speech and signal processing (ICASSP).
Parshakova, T., J.-M. Andreoli, and M. Dymetman. (2019). “Global
autoregressive models for data-efficient sequence learning”. arXiv
preprint arXiv:1909.07063.
Pearl, J. (1988). Probabilistic reasoning in intelligent systems: networks
of plausible inference. Morgan kaufmann.
Peng, J., L. Bo, and J. Xu. (2009). “Conditional Neural Fields”. In:
NIPS.
Pennington, J., R. Socher, and C. Manning. (2014). “Glove: Global vectors for word representation”. In: Conference on empirical methods
in natural language processing (EMNLP). 1532–1543.
Pietra, S. D., V. D. Pietra, and J. Lafferty. (1997). “Inducing Features
of Random Fields”. IEEE Trans. Pattern Analysis and Machine
Intelligence (PAMI). 19: 380–393.
Popov, V., I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov. (2021).
“Grad-TTS: A diffusion probabilistic model for text-to-speech”. In:
International Conference on Machine Learning. PMLR. 8599–8608.


REFERENCES

193

Povey, D., V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar, X. Na,
Y. Wang, and S. Khudanpur. (2016). “Purely Sequence-Trained
Neural Networks for ASR Based on Lattice-Free MMI”. In: INTERSPEECH.
Qin, L., S. Welleck, D. Khashabi, and Y. Choi. (2022). “Cold decoding:
Energy-based constrained text generation with langevin dynamics”.
Advances in Neural Information Processing Systems (NeurIPS).
Rabiner, L. R. (1989). “A tutorial on hidden Markov models and selected
applications in speech recognition”. Proceedings of the IEEE. 77(2):
257–286.
Radford, A., K. Narasimhan, T. Salimans, and I. Sutskever. (2018).
“Improving language understanding by generative pre-training”.
Radford, A., J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et
al. (2019). “Language models are unsupervised multitask learners”.
OpenAI blog. 1(8): 9.
Raffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P. J. Liu. (2020). “Exploring the Limits of
Transfer Learning with a Unified Text-to-Text Transformer”. Journal
of Machine Learning Research. 21(140): 1–67.
Rajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. (2016). “Squad:
100,000+ questions for machine comprehension of text”. In: EMNLP.
Ranzato, M., S. Chopra, M. Auli, and W. Zaremba. (2016). “Sequence
level training with recurrent neural networks”. In: International
Conference on Learning Representations (ICLR).
Rasmus, A., H. Valpola, M. Honkala, M. Berglund, and T. Raiko. (2015).
“Semi-supervised learning with Ladder networks”. In: NIPS.
Ren, Y., C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu. (2020).
“Fastspeech 2: Fast and high-quality end-to-end text to speech”.
arXiv preprint arXiv:2006.04558.
Robbins, H. and S. Monro. (1951). “A stochastic approximation method”.
The Annals of Mathematical Statistics: 400–407.
Roberts, G. O. and J. S. Rosenthal. (2009). “Examples of adaptive
MCMC”. Journal of Computational and Graphical Statistics. 18(2):
349–367.


194

REFERENCES

Roberts, G. O. and R. L. Tweedie. (1996). “Exponential convergence of
Langevin distributions and their discrete approximations”. Bernoulli.
2: 341–363.
Rosenfeld, R., S. F. Chen, and X. Zhu. (2001). “Whole-sentence exponential language models: a vehicle for linguistic-statistical integration”.
Computer Speech & Language. 15: 55–73.
Ruokolainen, T., T. Alumae, and M. Dobrinkat. (2010). “Using Dependency Grammar Features in Whole Sentence Maximum Entropy
Language Model for Speech Recognition.” In: Baltic HLT.
Russell, S. and P. Norvig. (2010). Artificial intelligence: a modern
approach (3rd). Upper Saddle River, Prentice-Hall.
Salakhutdinov, R. and G. Hinton. (2009). “Deep Boltzmann Machines”.
Journal of Machine Learning Research. 5(2): 1967–2006.
Salakhutdinov, R. (2009). “Learning Deep Generative Models”. Ph.D.
thesis, University of Toronto.
Salimans, T., I. Goodfellow, W. Zaremba, V. Cheung, A. Radford,
and X. Chen. (2016). “Improved techniques for training GANs”. In:
NIPS.
Sarawagi, S. and W. W. Cohen. (2004). “Semi-Markov Conditional
Random Fields for Information Extraction”. In: NIPS.
Sarikaya, R., S. F. Chen, A. Sethy, and B. Ramabhadran. (2010). “Impact of word classing on shrinkage-based language models”. In:
Eleventh Annual Conference of the International Speech Communication Association.
Sato, I. and H. Nakagawa. (2014). “Approximation Analysis of Stochastic Gradient Langevin Dynamics by using Fokker-Planck Equation
and Ito Process”. In: ICML.
Sato, K. and Y. Sakakibara. (2005). “RNA secondary structural alignment with conditional random fields”. Bioinformatics. 21: 237–42.
Saul, L. K., T. Jaakkola, and M. I. Jordan. (1996). “Mean field theory for
sigmoid belief networks”. Journal of artificial intelligence research.
4(1): 61–76.
Scellier, B. and Y. Bengio. (2017). “Equilibrium propagation: Bridging the gap between energy-based models and backpropagation”.
Frontiers in computational neuroscience. 11: 24.


REFERENCES

195

Schwenk, H. (2007). “Continuous space language models”. Computer
Speech & Language. 21: 492–518.
Scudder, H. (1965). “Probability of error of some adaptive patternrecognition machines”. IEEE Transactions on Information Theory.
11(3): 363–371.
Shazeer, N., J. Pelemans, and C. Chelba. (2015). “Sparse Non-negative
Matrix Language Modeling For Skip-grams”. In: INTERSPEECH.
Søgaard, A. and Y. Goldberg. (2016). “Deep multi-task learning with
low level tasks supervised at lower layers”. In: ACL. 231–235.
Sohn, K., D. Berthelot, C.-L. Li, and et al. (2020). “FixMatch: Simplifying semi-supervised learning with consistency and confidence”.
arXiv:2001.07685.
Song, Q., M. Wu, and F. Liang. (2014). “Weak convergence rates of
population versus single-chain stochastic approximation MCMC
algorithms”. Advances in Applied Probability. 46(4): 1059–1083.
Song, Y. and Z. Ou. (2018). “Learning neural random fields with inclusive auxiliary generators”. arXiv preprint arXiv:1806.00271.
Song, Y., Z. Ou, Z. Liu, and S. Yang. (2020). “Upgrading CRFs to JRFs
and its Benefits to Sequence Modeling and Labeling”. In: ICASSP.
Song, Y., H. Zheng, and Z. Ou. (2021). “An empirical comparison of
joint-training and pre-training for domain-agnostic semi-supervised
learning via energy-based models”. In: IEEE International Workshop
on Machine Learning for Signal Processing (MLSP).
Springenberg, J. T. (2016). “Unsupervised and Semi-supervised Learning
with Categorical Generative Adversarial Networks”. In: ICML.
Srivastava, N., G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. (2014). “Dropout: a simple way to prevent neural networks
from overfitting”. The journal of machine learning research.
Sun, W., Z. Tu, and A. Ragni. (2023). “Energy-Based Models For Speech
Synthesis”. arXiv preprint arXiv:2310.12765.
Sundermeyer, M., R. Schlüter, and H. Ney. (2012). “LSTM Neural
Networks for Language Modeling.” In: INTERSPEECH. 194–197.
Sutskever, I., O. Vinyals, and Q. V. Le. (2014). “Sequence to sequence
learning with neural networks”. Advances in neural information
processing systems. 27.


196

REFERENCES

Sutton, C., A. McCallum, et al. (2012). “An introduction to conditional
random fields”. Foundations and Trends® in Machine Learning. 4(4):
267–373.
Tan, Z. (2017). “Optimally adjusted mixture sampling and locally
weighted histogram analysis”. Journal of Computational and Graphical Statistics. 26: 54–65.
Tarvainen, A. and H. Valpola. (2017). “Mean teachers are better
role models: Weight-averaged consistency targets improve semisupervised deep learning results”. In: NIPS.
Theis, L., A. V. Den Oord, and M. Bethge. (2016). “A note on the
evaluation of generative models”. In: ICLR.
Tieleman, T. (2008). “Training restricted Boltzmann machines using
approximations to the likelihood gradient”. In: ICML.
Toshniwal, S., A. Kannan, and et al. (2018). “A comparison of techniques for language model integration in encoder-decoder speech
recognition”. In: SLT.
Tu, L. and K. Gimpel. (2018). “Learning Approximate Inference Networks for Structured Prediction”. In: ICLR.
Tüske, Z., K. Audhkhasi, and G. Saon. (2019). “Advancing sequenceto-sequence based speech recognition”. In: INTERSPEECH.
Variani, E., K. Wu, M. D. Riley, D. Rybach, M. Shannon, and C.
Allauzen. (2022). “Global normalization for streaming speech recognition in a modular framework”. Advances in Neural Information
Processing Systems. 35: 4257–4269.
Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin. (2017). “Attention is all you need”.
Advances in neural information processing systems.
Wainwright, M. J., M. I. Jordan, et al. (2008). “Graphical models,
exponential families, and variational inference”. Foundations and
Trends® in Machine Learning. 1(1–2): 1–305.
Wang, A. and K. Cho. (2019). “BERT has a Mouth, and It Must Speak:
BERT as a Markov Random Field Language Model”. In: Proceedings
of the Workshop on Methods for Optimizing and Evaluating Neural
Language Generation.


REFERENCES

197

Wang, A., A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman.
(2019). “GLUE: A multi-task benchmark and analysis platform for
natural language understanding”. In: International Conference on
Learning Representations (ICLR).
Wang, B. (2018). “Statistical Language Models Based on Trans-dimensional
Random Fields”. Ph.D. thesis, Tsinghua University.
Wang, B. and Z. Ou. (2017). “Language modeling with neural transdimensional random fields”. In: IEEE Automatic Speech Recognition
and Understanding Workshop (ASRU).
Wang, B. and Z. Ou. (2018a). “Improved training of neural transdimensional random field language models with dynamic noisecontrastive estimation”. In: IEEE Spoken Language Technology
Workshop (SLT).
Wang, B. and Z. Ou. (2018b). “Learning neural trans-dimensional
random field language models with noise-contrastive estimation”.
In: ICASSP.
Wang, B., Z. Ou, Y. He, and A. Kawamura. (2016). “Model interpolation
with trans-dimensional random field language models for speech
recognition”. arXiv preprint arXiv:1603.09170.
Wang, B., Z. Ou, and Z. Tan. (2015). “Trans-dimensional random
fields for language modeling”. In: Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers). 785–794.
Wang, B., Z. Ou, and Z. Tan. (2018). “Learning trans-dimensional
random fields with applications to language modeling”. IEEE transactions on pattern analysis and machine intelligence. 40(4): 876–
890.
Welling, M. and Y. W. Teh. (2011). “Bayesian learning via stochastic
gradient Langevin dynamics”. In: ICML.
Williams, R. J. and D. Zipser. (1989). “A learning algorithm for continually running fully recurrent neural networks”. Neural computation.
1(2): 270–280.
Wiseman, S. and A. M. Rush. (2016). “Sequence-to-sequence learning
as beam-search optimization”. In: EMNLP.


198

REFERENCES

Xiang, H. and Z. Ou. (2019). “CRF-based Single-stage Acoustic Modeling with CTC Topology”. In: ICASSP. 5676–5680.
Xie, J., Y. Lu, R. Gao, S.-C. Zhu, and Y. N. Wu. (2018). “Cooperative
training of descriptor and generator networks”. IEEE transactions
on pattern analysis and machine intelligence. 42(1): 27–45.
Xie, J., Y. Lu, S.-C. Zhu, and Y. Wu. (2016). “A theory of generative
convnet”. In: ICML.
Xu, H. and Z. Ou. (2016). “Joint stochastic approximation learning of
helmholtz machines”. In: ICLR Workshop Track.
Younes, L. (1989). “Parametric inference for imperfectly observed Gibbsian fields”. Probability Theory and Related Fields. 82: 625–645.
Yu, F., Z. Yao, X. Wang, K. An, L. Xie, Z. Ou, B. Liu, X. Li, and G.
Miao. (2021). “The SLT 2021 children speech recognition challenge:
Open datasets, rules and baselines”. In: IEEE Spoken Language
Technology Workshop (SLT).
Zaremba, W., I. Sutskever, and O. Vinyals. (2014). “Recurrent neural
network regularization”. arXiv:1409.2329.
Zeyer, A., E. Beck, R. Schlüter, and H. Ney. (2017). “CTC in the context
of generalized full-sum HMM training”. In: INTERSPEECH.
Zhang, B., H. Lv, P. Guo, Q. Shao, C. Yang, L. Xie, X. Xu, H. Bu, X.
Chen, C. Zeng, D. Wu, and Z. Peng. (2022a). “WENETSPEECH: A
10000+ Hours Multi-Domain Mandarin Corpus for Speech Recognition”. In: International Conference on Acoustics, Speech, and Signal
Processing (ICASSP).
Zhang, L., D. M. Blei, and C. A. Naesseth. (2022b). “Transport score
climbing: Variational inference using forward KL and adaptive neural
transport”. arXiv preprint arXiv:2202.01841.
Zhang, T., V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. (2020a).
“BERTScore: Evaluating Text Generation with BERT”. In: International Conference on Learning Representations.
Zhang, X., Z. Tan, and Z. Ou. (2023). “Persistently Trained, Diffusionassisted Energy-based Models”. Stat.
Zhang, Y., X. Sun, S. Ma, Y. Yang, and X. Ren. (2018). “Does Higher
Order LSTM Have Better Accuracy for Segmenting and Labeling
Sequence Data?” In: COLING.


REFERENCES

199

Zhang, Y., Z. Ou, M. Hu, and J. Feng. (2020b). “A Probabilistic End-ToEnd Task-Oriented Dialog Model with Latent Belief States towards
Semi-Supervised Learning”. In: Proc. of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Zhao, S., J. Jacobsen, and W. Grathwohl. (2020). “Joint Energy-Based
Models for Semi-Supervised Classification”. In: ICML Workshop on
Uncertainty and Robustness in Deep Learning.
Zheng, H., K. An, and Z. Ou. (2021a). “Efficient neural architecture
search for end-to-end speech recognition via straight-through gradients”. In: 2021 IEEE Spoken Language Technology Workshop (SLT).
Zheng, H., K. An, Z. Ou, C. Huang, K. Ding, and G. Wan. (2022).
“An Empirical Study of Language Model Integration for Transducer
based Speech Recognition”. In: INTERSPEECH.
Zheng, H., W. Peng, Z. Ou, and J. Zhang. (2021b). “Advancing CTCCRF based end-to-end speech recognition with wordpieces and
conformers”. arXiv preprint arXiv:2107.03007.
Zhu, C., K. An, H. Zheng, and Z. Ou. (2021). “Multilingual and crosslingual speech recognition using phonological-vector based phone embeddings”. In: IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).
Zhu, X. (2006). “Semi-supervised learning literature survey”. Technical
report, University of Wisconsin-Madison.
Zhu, Y., R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,
and S. Fidler. (2015). “Aligning books and movies: Towards storylike visual explanations by watching movies and reading books”.
In: Proceedings of the IEEE international conference on computer
vision. 19–27.


Index

Acoustic model (AM), 111
Ancestral sampling, 55, 58, 67
Attention based
encoder-decoder
(AED), 121
AugSA plus JSA algorithm, 53
Automatic speech recognition
(ASR), 108
Autoregressive language model
(ALM), 66, 72

Conditional random field
(CRF), 94
Constrained decoding with
Langevin dynamics
(COLD), 136, 140
Contional EBM, 94
Contrastive divergence (CD),
49
CoopNet, 53
CRF transducer, 124
Data efficiency, 116
Data hungry, 116
Deep belief network (DBN), 29
Deep Boltzmann machine
(DBM), 29
Deep generative model (DGM),
7
Deep neural network (DNN),
19, 110, 142
Directed graphical model
(DGM), 9, 15

Bayesian network (BN), 9
Burned in, 35
Calibration, 164
Clique, 19
Conditional generation, 8
Conditional maximum
likelihood (CML), 104
Conditional model, 8
Conditional NCE, 107
200


INDEX
Discrete feature, 79, 96
Discriminative SSL, 143
DNN-HMM hybrid, 108, 110
Dynamic noise-contrastive
estimation (DNCE), 64
EBMs parameterized by neural
networks, 29
Edge potential, 97
Electric model, 91
Energy function, 21
Energy-based language model
(ELM), 73
Energy-based model (EBM), 7,
9, 21
Entropy, 168
Evidence upper bound (EUBO),
51
Exclusive-NRF algorithm, 52
Exclusive-variational approach,
50
Expectation-Maximization
(EM) algorithm, 50
Expected calibration error
(ECE), 167
Exponential tilting, 88
Exposure bias, 97
Feature extractor, 19, 97
Feature function, 22
Fisher equality, 180
Full conditional, 37
Generative adversarial network
(GAN), 7
Generative AI, 66
Generative SSL, 143, 144

201
Generative-discriminative pair,
6
Gibbs sampling, 37
Globally-normalized ELM
(GN-ELM), 73
Globally-normalized model, 9
Globally-normalized sequence
model, 98
Graphical model, 14
Hamiltonian Monte Carlo
(HMC), 39
Helmholtz machine (HM), 19
Hidden Markov model (HMM),
16
Hybrid generativediscriminative, 7
Importance sampling (IS), 42
Importance weight, 37, 42
Inclusive-NRF algorithm, 53,
149
Inclusive-variational approach,
51
Ising model, 24
Joint EBM (JEM), 147
Joint random field (JRF), 150
Joint stochastic approximation
(JSA), 50
Joint-training for generative
SSL, 144
Label bias, 97
Langevin dynamics (LD), 40
Language model (LM), 72


202
Latent-variable model (LVM),
145
Learning, 5
Linear layer, 19
Linear-chain CRF, 95
Locally-normalized model, 9
Locally-normalized sequence
model, 67, 97
Log-linear model, 22
Logits, 18
Markov Chain Monte Carlo
(MCMC), 35
Markov random field (MRF), 9
Markovian score climbing
(MSC), 50
Masked language model
(MLM), 85, 89
Maximum entropy Markov
model (MEMM), 98
Maximum entropy model
(maxent), 23
Maximum likelihood estimation
(MLE), 32
Metropolis algorithm, 37
Metropolis independence
sampler (MIS), 37
Metropolis-Hastings (MH)
algorithm, 35
MH ratio, 36
MH within Gibbs sampling, 39
Minibatching, 34, 46, 48
Mix-and-match language model,
136
Monte Carlo averaging, 33
Multi-class logistic regression,

INDEX
18
Natural language processing
(NLP), 12, 142
Neural CRF (NCRF), 96, 123
Neural random fields (NRFs),
30, 53
Node potential, 96
Noise-contrastive estimation
(NCE), 61
Non-autoregressive generation,
67
Non-normalized probabilistic
model, 169
Part-of-speech (POS), 149
Path in CTC, 113
Perplexity (PPL), 133
Persistent contrastive
divergence (PCD), 49
Pre-trained language model
(PLM), 70, 84, 133,
135, 144
Pre-training for generative SSL,
144
Probabilistic approach, 3
Probabilistic inference, 5
Probabilistic model, 3
Proposal distribution, 35, 42
Pseudo-log-likelihood (PLL), 85
Recurrent neural network
transducer (RNN-T), 8,
120
Residual ELM, 88, 130
Restricted Boltzmann machine
(RBM), 26


INDEX
Scheduled sampling, 104
Score function, 61
Score matching (SM), 60
Self-normalized importance
sampling (SNIS), 43,
133
Semi-supervised learning (SSL),
7, 142
Sequence labeling, 95, 123, 149
Sequence modeling, 149
Sequence-to-sequence model
(seq2seq), 98
Sigmoid belief network (SBN),
19, 28
Softmax, 18
State space, 105
State topology of a Markov
chain, 109
State transition graph of a
Markov chain, 109
Statistical inference, 5
Stochastic approximation (SA),
43
Stochastic gradient descent
(SGD), 46
Stochastic gradient Langevin
dynamics (SGLD), 40

203
Stochastic maximum likelihood
(SML), 48
Streaming speech recognition,
99
Tabular potential, 22
Target variable, 4
Teacher forcing, 103
Trans-dimensional random field
(TRF), 74
Tree-width, 105
Trigram, 80
Undirected graphical model
(UGM), 9, 19
Variational autoencoder (VAE),
7, 19
Variational inference (VI), 50
Variational learning, 50
Variational methods, 50
Weighted finite-state transducer
(WFST), 109
Whole-sentence maximum
entropy (WSME), 73
Word error rate (WER), 74
Word morphology, 22

